{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "slightly_changed_HungarianLoss+BCELoss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1oA2LpSnMvbgC8vkoiyGq92-qrHpvkugZ",
      "authorship_tag": "ABX9TyObJWRX2ERjK7CQWalUL2VK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailuu684/Convolution-net/blob/main/slightly_changed_HungarianLoss%2BBCELoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcIb8zHyOZgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080a15a8-14f3-4265-a048-d8d5b5dcddaa"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Input, Lambda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.19.5\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 24.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3bddNhWAMNT"
      },
      "source": [
        "# Preprocess Hungarian Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOo-EBp8APcz"
      },
      "source": [
        "def replacenan(t):\n",
        "    return tf.where(tf.math.is_nan(t), x = tf.zeros_like(t), y = t)\n",
        "\n",
        "def replace_nan_by_10_square_10(t):\n",
        "    return tf.where(tf.math.is_nan(t), x = 10**10, y = t)\n",
        "\n",
        "def replace_10_square_10_by_zero(t):\n",
        "    return tf.where(tf.math.equal(t, 10**10), x = 0.0 + 1e-10, y = t)\n",
        "\n",
        "def replace_zero_by_two(t):\n",
        "    return tf.where(tf.math.equal(t, 0), x = 2.0, y = t)\n",
        "\n",
        "def replace_two_by_zero(t):\n",
        "    return tf.where(tf.math.equal(t, 2), x = 0.0 + 1e-10, y = t)\n",
        "\n",
        "def remove_zero(t):\n",
        "    intermediate_tensor = t\n",
        "    # intermediate_tensor = tf.cast(intermediate_tensor,tf.double)\n",
        "    batch = t.shape[0]\n",
        "    zero_vector = tf.zeros(shape=t.shape, dtype=tf.double)\n",
        "    bool_mask = tf.not_equal(intermediate_tensor, zero_vector)\n",
        "    omit_zeros = tf.boolean_mask(intermediate_tensor, bool_mask)\n",
        "    # tf.print('omit zero',omit_zeros)\n",
        "\n",
        "    non_zero_dimension = int(len(omit_zeros) / batch)\n",
        "\n",
        "\n",
        "    omit_zeros = tf.reshape(omit_zeros,shape=(batch,non_zero_dimension))\n",
        "    return omit_zeros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jujrp1Hc-pRV"
      },
      "source": [
        "# Test data hungarian loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0wC0xScCa_F"
      },
      "source": [
        "y_true_test_h_l = tf.random.uniform(shape=(16,28,),dtype=tf.double)\n",
        "y_pred_test_h_l = tf.random.uniform(shape=(16,28,),dtype=tf.double)\n",
        "\n",
        "y_true_test_h_l = np.array(y_true_test_h_l)\n",
        "y_pred_test_h_l = np.array(y_pred_test_h_l)\n",
        "\n",
        "y_true_test_h_l[:,8:] = np.nan\n",
        "# y_pred_test_h_l[:,8:] = np.nan\n",
        "\n",
        "y_true_test_h_l[:,11] = 0\n",
        "y_true_test_h_l[:,15] = 0\n",
        "y_true_test_h_l[:,19] = 0\n",
        "y_true_test_h_l[:,23] = 0\n",
        "y_true_test_h_l[:,27] = 0\n",
        "\n",
        "y_pred_test_h_l[:,11] = 0\n",
        "y_pred_test_h_l[:,15] = 0\n",
        "y_pred_test_h_l[:,19] = 0\n",
        "y_pred_test_h_l[:,23] = 0\n",
        "y_pred_test_h_l[:,27] = 0\n",
        "\n",
        "# Test 0 like 0 in relative coordinate\n",
        "# y_true_test_h_l[:,2][0] = 0.5\n",
        "# y_true_test_h_l[:,0][0] = 0\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(y_true_test_h_l[:,3])):\n",
        "    y_true_test_h_l[:,2][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "    y_pred_test_h_l[:,2][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "\n",
        "    y_true_test_h_l[:,6][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "    y_pred_test_h_l[:,6][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "\n",
        "    # confident values\n",
        "    y_true_test_h_l[:,3][i] = 1.0\n",
        "    y_pred_test_h_l[:,3][i] = 1.0\n",
        "    y_true_test_h_l[:,7][i] = 1.0\n",
        "    y_pred_test_h_l[:,7][i] = 1.0\n",
        "\n",
        "y_true_test_h_l[1,8] = 2.342432\n",
        "y_true_test_h_l[1,9] = 1.534534\n",
        "y_true_test_h_l[1,10] = 0\n",
        "y_true_test_h_l[1,11] = 1.0\n",
        "\n",
        "y_pred_test_h_l[1,8] = 1.3432\n",
        "y_pred_test_h_l[1,9] = 1.7453\n",
        "y_pred_test_h_l[1,10] = 0.2\n",
        "y_pred_test_h_l[1,11] = 0.9\n",
        "\n",
        "y_true_test_h_l = tf.convert_to_tensor(y_true_test_h_l)\n",
        "y_pred_test_h_l = tf.convert_to_tensor(y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H7OQF-7g3c3"
      },
      "source": [
        "tf.reshape(y_pred_test_h_l,shape=(16,7,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAneuc_1MayG"
      },
      "source": [
        "y = tf.constant([1,2], dtype = tf.float32)\n",
        "tf.math.reduce_euclidean_norm([[1,2,3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ4QGByPfjVt"
      },
      "source": [
        "y_true_test_h_bug = tf.random.uniform(shape=(64,28,),dtype=tf.double)\n",
        "y_pred_test_h_bug = tf.random.uniform(shape=(64,28,),dtype=tf.double)\n",
        "hungarian_loss(y_true_test_h_bug,y_pred_test_h_bug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_uoAKJqNE5q"
      },
      "source": [
        "hungarian_loss_test(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKNorJdOojo7"
      },
      "source": [
        "# Hungarian loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o6lMb85q1kX"
      },
      "source": [
        "def hungarian_loss(y_true, y_pred): # -----> worked with non-nan values, the trick here is you should not use for loop \n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "\n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "    # Take only 2 first dimension\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "    # print(y_true_)\n",
        "\n",
        "    # tf.print('y_true',y_true_)\n",
        "    # tf.print('y_pred',y_pred_)\n",
        "\n",
        "    # if change nan --> 10^10\n",
        "    y_true_ = replace_nan_by_10_square_10(y_true_) # shape = (16,7,2)\n",
        "    y_pred_ = replace_nan_by_10_square_10(y_pred_)\n",
        "\n",
        "    y_true_ = tf.cast(y_true_,dtype=tf.double)\n",
        "    y_pred_ = tf.cast(y_pred_,dtype=tf.double)\n",
        "\n",
        "    store_element = [tf.math.reduce_euclidean_norm(tf.constant((y_true_[:,i,:2] - y_pred_[:,j,:2]),dtype=tf.double),axis=-1) for i in range(y_true_.shape[1]) for j in range(y_true_.shape[1])]\n",
        "    store_cost = tf.convert_to_tensor(store_element)\n",
        "    store_cost = tf.transpose(store_cost, perm=[1,0])\n",
        "    \n",
        "\n",
        "    cost_batch, cost_H_W = store_cost.shape\n",
        "    cost_reshape = tf.reshape(store_cost,shape=(cost_batch,int(np.sqrt(cost_H_W)), int(np.sqrt(cost_H_W)) ))\n",
        "    # print('cost = \\n',cost_reshape) # shape = (16,7,7)\n",
        "\n",
        "    final_cost = []\n",
        "    pair_row_col = []\n",
        "\n",
        "\n",
        "    for i in range(cost_reshape.shape[0]):\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_reshape[i,:]) # find the row_ind and col_ind for each batch\n",
        "        chosen_elements = zip(row_ind.tolist(), col_ind.tolist())\n",
        "        # print(row_ind,col_ind)\n",
        "        for row,col in zip(row_ind,col_ind): \n",
        "            if cost_reshape[i,row,col] < 10**10:\n",
        "                final_cost.append(cost_reshape[i,row,col])  \n",
        "                # chosen_elements = zip(row_ind.tolist(), col_ind.tolist())  \n",
        "                # pair_row_col.append(chosen_elements)    \n",
        "        pair_row_col.append(chosen_elements)\n",
        "\n",
        "\n",
        "    \n",
        "    final_cost = tf.convert_to_tensor(final_cost) \n",
        "    # print('final cost shape',final_cost.shape) # shape = ( 112,) = (16,7)\n",
        "    \n",
        "\n",
        "    cost = tf.cast(final_cost,dtype=tf.float32)\n",
        "    #-----------------------------\n",
        "    #--------Hungarian loss-------\n",
        "    #-----------------------------\n",
        "\n",
        "    # final_cost = tf.reduce_mean(cost,axis=-1) # shape = (batch,)\n",
        "    final_cost = tf.reduce_mean(final_cost) # shape = ()\n",
        "    # tf.print('\\nDone hungarian loss part')\n",
        "    #----------------\n",
        "    #---class loss--- note that 16 is the batch\n",
        "    #----------------\n",
        "    # print(y_true_reshape)\n",
        "\n",
        "    # Get the class\n",
        "    y_true_cls = y_true_reshape[:,:,2] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_cls = tf.sigmoid(y_pred_reshape[:,:,2])\n",
        "    \n",
        "    # Confident scores\n",
        "    y_true_conf = y_true_reshape[:,:,3] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_conf = tf.sigmoid(y_pred_reshape[:,:,3])\n",
        "\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    \n",
        "    y_true_cls_tf = []\n",
        "    y_pred_cls_tf = []\n",
        "    cls_loss = []\n",
        "    store_confident_loss = []\n",
        "    for batch_pair in pair_row_col:\n",
        "        for row,col in batch_pair:\n",
        "            ## y_true_cls[row,:] = [ 1 0 10e+10 10e+10 10e+10 10e+10 10e+10]\n",
        "            # len_cls_label_true = len(np.array(y_true_cls[row,:]))\n",
        "            # len_cls_label_pred = len(np.array(y_pred_cls[col,:]))\n",
        "\n",
        "            if row < len(y_pred_cls) and col < len(y_true_cls):\n",
        "                # Number of object in one frame\n",
        "                num_object_true = np.count_nonzero(~np.isnan(np.array(y_true_cls[col,:])))\n",
        "                # num_object_pred = np.count_nonzero(~np.isnan(np.array(y_pred_cls[col,:])))\n",
        "\n",
        "                # Calculate loss\n",
        "                # print('y_true',y_true_cls[row,:num_object_true])\n",
        "                # print('y_pred',y_pred_cls[col,:num_object_true])\n",
        "            \n",
        "                if len(y_true_cls[col,:num_object_true]) == len(y_pred_cls[row,:num_object_true]):\n",
        "                    y_true_cls_tf.append(y_true_cls[col,:num_object_true])\n",
        "                    y_pred_cls_tf.append(y_pred_cls[row,:num_object_true])\n",
        "                    cls_bce_loss = bce(y_true_cls[col,:num_object_true], y_pred_cls[row,:num_object_true])\n",
        "                    cls_loss.append(cls_bce_loss)\n",
        "                \n",
        "                # print('y_true',y_true_conf[row,:])\n",
        "                # print('y_pred',y_pred_conf[col,:])\n",
        "                confident_loss = bce(y_true_conf[col,:], y_pred_conf[row,:])\n",
        "                store_confident_loss.append(confident_loss)\n",
        "            else:\n",
        "                tf.print('indices from hungarian algorithm is larger than the object')\n",
        "                tf.print('col = \\n', col)\n",
        "                tf.print('row = \\n', row)\n",
        "                tf.print('y true = \\n', y_true_cls)\n",
        "                tf.print('y pred = \\n',y_pred_cls)\n",
        "\n",
        "    reduce_mean_cls_loss = tf.reduce_mean(cls_loss)\n",
        "    # tf.print('\\nDone class loss part')\n",
        "    #------------------------\n",
        "    #-----Confident Loss-----\n",
        "    #------------------------\n",
        "    reduce_confident_loss = tf.reduce_mean(store_confident_loss)\n",
        "    # print('confident loss', store_confident_loss)\n",
        "\n",
        "    #----------------------------\n",
        "    #------ SUM OF LOSSES--------\n",
        "    #----------------------------\n",
        "    regression_loss = final_cost\n",
        "\n",
        "    # Training\n",
        "    regression_loss = tf.cast(regression_loss,dtype=tf.float32)\n",
        "\n",
        "    # Testing\n",
        "    # regression_loss = tf.cast(regression_loss,dtype=tf.double)\n",
        "\n",
        "    tf.print('\\nreduce_confident_loss = ', reduce_confident_loss)\n",
        "    tf.print('\\nreduce_mean_cls_loss = ', reduce_mean_cls_loss)\n",
        "    tf.print('\\nregression loss = ', regression_loss)\n",
        "    sum_loss = reduce_mean_cls_loss + regression_loss + reduce_confident_loss\n",
        "    return sum_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R3o5-nntsbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e281f91-1282-426b-fed3-64016844b4a0"
      },
      "source": [
        "hungarian_loss(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true [[[0.9594437655188186 0.18418625623256712]\n",
            "  [0.60027210290549649 0.67695129731242254]\n",
            "  [nan nan]\n",
            "  ...\n",
            "  [nan nan]\n",
            "  [nan nan]\n",
            "  [nan nan]]\n",
            "\n",
            " [[0.14659053565153779 0.23023923300875704]\n",
            "  [0.28673074621247019 0.080505630743321177]\n",
            "  [2.342432 1.534534]\n",
            "  ...\n",
            "  [nan nan]\n",
            "  [nan nan]\n",
            "  [nan nan]]\n",
            "\n",
            " [[0.73578588625084151 0.45678510045648579]\n",
            "  [0.89984797721977161 0.30570071429675982]\n",
            "  [nan nan]\n",
            "  ...\n",
            "  [nan nan]\n",
            "  [nan nan]\n",
            "  [nan nan]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.39633733433213214 0.74309534071096728]\n",
            "  [0.73802075602145867 0.6307451293673505]\n",
            "  [nan nan]\n",
            "  ...\n",
            "  [nan nan]\n",
            "  [nan nan]\n",
            "  [nan nan]]\n",
            "\n",
            " [[0.56442038784854054 0.73943245169425231]\n",
            "  [0.827929785824987 0.42285523635337974]\n",
            "  [nan nan]\n",
            "  ...\n",
            "  [nan nan]\n",
            "  [nan nan]\n",
            "  [nan nan]]\n",
            "\n",
            " [[0.347224107257849 0.19775412965213168]\n",
            "  [0.44146679885271212 0.82631294618117779]\n",
            "  [nan nan]\n",
            "  ...\n",
            "  [nan nan]\n",
            "  [nan nan]\n",
            "  [nan nan]]]\n",
            "y_pred [[[0.10841243929717836 0.45805752304129888]\n",
            "  [0.45480098276143677 0.044750702604267056]\n",
            "  [0.96086120286961907 0.33239772635076714]\n",
            "  ...\n",
            "  [0.68486935351274014 0.48101709244736823]\n",
            "  [0.698579026298237 0.69455839087779814]\n",
            "  [0.37836710224079839 0.99102844685246971]]\n",
            "\n",
            " [[0.029345594504127881 0.70077706747935475]\n",
            "  [0.86846026287024092 0.00906651747769538]\n",
            "  [1.3432 1.7453]\n",
            "  ...\n",
            "  [0.0097102682968459586 0.077964731847079527]\n",
            "  [0.51874305012225608 0.424541020446793]\n",
            "  [0.68109389628195727 0.1384876831593902]]\n",
            "\n",
            " [[0.33699302839249823 0.79646579344612478]\n",
            "  [0.46891655195731952 0.33876822693875019]\n",
            "  [0.26745463116422918 0.926353419458154]\n",
            "  ...\n",
            "  [0.36891132363605283 0.68823593095423585]\n",
            "  [0.019663537858303215 0.94139551206768624]\n",
            "  [0.6501702670186198 0.16001468201472213]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.96727568999554348 0.78904027684484945]\n",
            "  [0.93302941436812592 0.769127973208851]\n",
            "  [0.23654210171324386 0.93573159851000143]\n",
            "  ...\n",
            "  [0.62733519034127538 0.37876388426187257]\n",
            "  [0.31937201390326475 0.54882300409864282]\n",
            "  [0.41277257269373191 0.041548294014196241]]\n",
            "\n",
            " [[0.23384906382374004 0.35894567812441136]\n",
            "  [0.42067313700493258 0.43678374701222222]\n",
            "  [0.18996005279432993 0.48151544372326827]\n",
            "  ...\n",
            "  [0.696580113404498 0.43322857454731478]\n",
            "  [0.040867482396346411 0.18375931653047517]\n",
            "  [0.18931750495122279 0.90385488856853247]]\n",
            "\n",
            " [[0.52821784747286471 0.97592743629959511]\n",
            "  [0.23974242938817847 0.71135691285630664]\n",
            "  [0.94850877838742331 0.4955265896353056]\n",
            "  ...\n",
            "  [0.52606935785554576 0.684589783908977]\n",
            "  [0.79168781230120722 0.28333876792199986]\n",
            "  [0.86050994199844655 0.19547902219491009]]]\n",
            "cost = \n",
            " tf.Tensor(\n",
            "[[[8.94013305e-01 5.23551919e-01 1.48218248e-01 9.56750600e-01\n",
            "   4.04350903e-01 5.73175477e-01 9.94305994e-01]\n",
            "  [5.38368288e-01 6.48721388e-01 4.98740075e-01 7.86402553e-01\n",
            "   2.13417215e-01 9.98712217e-02 3.84559859e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[4.84924973e-01 7.54992234e-01 1.93061735e+00 6.54121757e-01\n",
            "   2.04752854e-01 4.19822199e-01 5.42321113e-01]\n",
            "  [6.71553253e-01 5.86099631e-01 1.97171691e+00 5.96589157e-01\n",
            "   2.77032131e-01 4.14957900e-01 3.98602826e-01]\n",
            "  [2.45876378e+00 2.12123637e+00 1.02121834e+00 1.92144654e+00\n",
            "   2.75012449e+00 2.13492993e+00 2.17002065e+00]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[5.23849899e-01 2.91799973e-01 6.63195726e-01 1.82595655e-01\n",
            "   4.33781548e-01 8.64684028e-01 3.08873300e-01]\n",
            "  [7.46763722e-01 4.32198281e-01 8.86076252e-01 4.00642797e-01\n",
            "   6.54390497e-01 1.08574054e+00 2.89073311e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[3.46559745e-01 4.69705184e-01 1.59780367e-01 6.02082420e-01\n",
            "   5.04184775e-01 6.57594753e-01 7.22355050e-01]\n",
            "  [4.47539752e-01 5.71103591e-01 3.86794901e-01 1.07768506e-01\n",
            "   1.56269486e-01 6.74854292e-01 2.03728121e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[3.53841946e-01 5.76722937e-01 1.73187446e-01 9.31671464e-02\n",
            "   6.41475521e-01 2.96207543e-01 5.35243445e-01]\n",
            "  [6.67166314e-01 3.76102802e-01 4.21329576e-01 3.94850634e-01\n",
            "   4.23470425e-01 2.33426153e-01 8.53190910e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[9.20807381e-01 8.03841177e-01 7.78054370e-01 5.91097826e-01\n",
            "   6.85418502e-01 2.51141598e-02 4.20235980e-01]\n",
            "  [6.03505826e-01 5.10742531e-01 4.70684048e-01 6.23558526e-01\n",
            "   3.77191922e-01 3.42438377e-01 1.35222437e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[6.02520807e-01 6.15976153e-01 6.86321059e-01 6.65360785e-01\n",
            "   6.58611050e-01 5.63492525e-02 8.16980711e-02]\n",
            "  [5.91034116e-01 6.01049016e-01 4.49397831e-01 3.75646082e-01\n",
            "   4.49392167e-01 6.65914889e-01 7.16603019e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[9.16642999e-01 6.48440771e-01 5.85010346e-01 2.36541736e-01\n",
            "   7.85403099e-01 7.61788571e-01 7.85703899e-01]\n",
            "  [7.92759193e-01 4.94459477e-01 5.95912080e-01 9.35475738e-02\n",
            "   6.24719205e-01 5.78533250e-01 6.69334355e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[2.82547014e-01 7.21172219e-01 6.22305411e-01 8.22179364e-01\n",
            "   4.99218192e-01 3.60514179e-01 6.57346320e-01]\n",
            "  [8.42640203e-01 3.99490615e-01 1.47041057e-01 1.35618067e-01\n",
            "   5.36295092e-01 7.29063146e-01 2.75734974e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[6.12659489e-01 2.25250145e-01 6.57349540e-01 3.81399815e-01\n",
            "   3.31417330e-01 3.43256668e-01 7.93153332e-01]\n",
            "  [7.95493139e-01 1.26335534e-01 7.92236891e-01 2.13241879e-01\n",
            "   2.32859417e-01 4.76670043e-01 8.37902780e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[3.12627347e-01 1.55029667e-01 4.20928355e-01 5.61374495e-01\n",
            "   5.81019860e-01 5.17774748e-01 2.30210174e-01]\n",
            "  [8.20646033e-01 6.76429964e-01 4.03279669e-01 1.72926092e-01\n",
            "   1.39673480e-01 9.90502687e-01 8.07012065e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[8.87409529e-01 5.00980523e-01 3.87358482e-01 2.69396080e-01\n",
            "   3.75583501e-01 9.24458292e-01 5.79469834e-01]\n",
            "  [4.23531062e-01 5.76616584e-01 8.18377628e-01 5.31531360e-01\n",
            "   4.00026388e-01 1.95374570e-01 5.59221644e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[9.34788049e-01 1.13716314e+00 7.33422285e-01 1.44971509e-01\n",
            "   4.60912957e-01 7.32330142e-01 5.97690815e-01]\n",
            "  [2.73536625e-01 8.53175937e-01 7.08665276e-01 6.64234194e-01\n",
            "   4.71356589e-01 7.24010928e-01 6.27568974e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[5.72784028e-01 5.37323075e-01 2.50286324e-01 2.86372941e-01\n",
            "   4.31390101e-01 2.08962679e-01 7.01739536e-01]\n",
            "  [2.78595008e-01 2.39119611e-01 5.86939168e-01 5.85869290e-01\n",
            "   2.75219626e-01 4.26588800e-01 6.73007646e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[5.04031333e-01 3.35051504e-01 4.54688603e-01 3.81284204e-01\n",
            "   3.33507133e-01 7.63465964e-01 4.09556969e-01]\n",
            "  [5.97508440e-01 4.07494762e-01 6.40660909e-01 6.43819036e-01\n",
            "   1.31758653e-01 8.22577612e-01 7.99491282e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]\n",
            "\n",
            " [[7.98944572e-01 5.24728625e-01 6.70978162e-01 2.12802447e-01\n",
            "   5.18646872e-01 4.52628673e-01 5.13290877e-01]\n",
            "  [1.72945772e-01 2.32180126e-01 6.05401671e-01 7.85983863e-01\n",
            "   1.65054681e-01 6.46123608e-01 7.57329912e-01]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]\n",
            "  [1.41421356e+10 1.41421356e+10 1.41421356e+10 1.41421356e+10\n",
            "   1.41421356e+10 1.41421356e+10 1.41421356e+10]]], shape=(16, 7, 7), dtype=float64)\n",
            "\n",
            "reduce_confident_loss =  0.59349615339721951\n",
            "\n",
            "reduce_mean_cls_loss =  0.81855659878679687\n",
            "\n",
            "regression loss =  0.41020703520486307\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=1.8222597873888793>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cirheutih4Kv"
      },
      "source": [
        "# slightly change hungarian loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGmt87nqOiJp"
      },
      "source": [
        "def hungarian_loss_test(y_true, y_pred): # -----> worked with non-nan values, the trick here is you should not use for loop \n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "    # Take only 2 first dimensions\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "\n",
        "    # tf.print('y_true \\n',y_true_)\n",
        "    # tf.print('y_pred \\n',y_pred)\n",
        "\n",
        "    y_true_ = tf.cast(y_true_,dtype=tf.float32)\n",
        "    y_pred_ = tf.cast(y_pred_,dtype=tf.float32)\n",
        "\n",
        "    store_element = [tf.math.reduce_euclidean_norm(tf.constant((y_true_[:,i,:2] - y_pred_[:,j,:2]),dtype=tf.float32),axis=-1) for i in range(y_true_.shape[1]) for j in range(y_true_.shape[1])]\n",
        "    store_cost = tf.convert_to_tensor(store_element)\n",
        "    store_cost = tf.transpose(store_cost, perm=[1,0])\n",
        "    \n",
        "\n",
        "    cost_batch, cost_H_W = store_cost.shape\n",
        "    cost_reshape = tf.reshape(store_cost,shape=(cost_batch,int(np.sqrt(cost_H_W)), int(np.sqrt(cost_H_W)) )) # ( batch, 7,7)\n",
        "     # shape = (16,7,7)\n",
        "    cost_reshape = replace_nan_by_10_square_10(cost_reshape)\n",
        "    # tf.print('cost = \\n',cost_reshape)\n",
        "    # tf.print('cost shape = ', cost_reshape.shape)\n",
        "    final_cost = []\n",
        "    pair_row_col = []\n",
        "\n",
        "\n",
        "    for i in range(cost_reshape.shape[0]):\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_reshape[i,:]) # find the row_ind and col_ind for each batch\n",
        "        chosen_elements = zip(row_ind.tolist(), col_ind.tolist())\n",
        "        # print(row_ind,col_ind)\n",
        "        for row,col in zip(row_ind,col_ind): \n",
        "            if cost_reshape[i,row,col] < 10**10:\n",
        "                final_cost.append(cost_reshape[i,row,col])  \n",
        "                # chosen_elements = zip(row_ind.tolist(), col_ind.tolist())  \n",
        "                # pair_row_col.append(chosen_elements)    \n",
        "        pair_row_col.append(chosen_elements)\n",
        "\n",
        "\n",
        "    \n",
        "    final_cost = tf.convert_to_tensor(final_cost) \n",
        "    # print('final cost shape',final_cost.shape) # shape = ( 112,) = (16,7)\n",
        "    \n",
        "\n",
        "    cost = tf.cast(final_cost,dtype=tf.float32)\n",
        "    #-----------------------------\n",
        "    #--------Hungarian loss-------\n",
        "    #-----------------------------\n",
        "\n",
        "    # final_cost = tf.reduce_mean(cost,axis=-1) # shape = (batch,)\n",
        "    final_cost = tf.reduce_mean(final_cost) # shape = ()\n",
        "    # tf.print('\\nDone hungarian loss part')\n",
        "    #----------------\n",
        "    #---class loss--- note that 16 is the batch\n",
        "    #----------------\n",
        "    # print(y_true_reshape)\n",
        "\n",
        "    y_true_cls = y_true_reshape[:,:,2] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_cls = tf.sigmoid(y_pred_reshape[:,:,2])\n",
        "    \n",
        "    # Confident scores\n",
        "    y_true_conf = y_true_reshape[:,:,3] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_conf = tf.sigmoid(y_pred_reshape[:,:,3])\n",
        "\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    \n",
        "    y_true_cls_tf = []\n",
        "    y_pred_cls_tf = []\n",
        "    cls_loss = []\n",
        "    store_confident_loss = []\n",
        "    for batch_pair in pair_row_col:\n",
        "        for row,col in batch_pair:\n",
        "\n",
        "            if row < len(y_pred_cls) and col < len(y_true_cls):\n",
        "                # Number of object in one frame\n",
        "                num_object_true = np.count_nonzero(~np.isnan(np.array(y_true_cls[col,:])))\n",
        "                # num_object_pred = np.count_nonzero(~np.isnan(np.array(y_pred_cls[col,:])))\n",
        "\n",
        "            \n",
        "                if len(y_true_cls[col,:num_object_true]) == len(y_pred_cls[row,:num_object_true]):\n",
        "                    y_true_cls_tf.append(y_true_cls[col,:num_object_true])\n",
        "                    y_pred_cls_tf.append(y_pred_cls[row,:num_object_true])\n",
        "                    cls_bce_loss = bce(y_true_cls[col,:num_object_true], y_pred_cls[row,:num_object_true])\n",
        "                    cls_loss.append(cls_bce_loss)\n",
        "\n",
        "                    confident_loss = bce(y_true_conf[col,:], y_pred_conf[row,:])\n",
        "                    store_confident_loss.append(confident_loss)\n",
        "            else:\n",
        "                tf.print('indices from hungarian algorithm is larger than the object')\n",
        "                tf.print('col = \\n', col)\n",
        "                tf.print('row = \\n', row)\n",
        "                tf.print('y true = \\n', y_true_cls)\n",
        "                tf.print('y pred = \\n',y_pred_cls)\n",
        "\n",
        "    reduce_mean_cls_loss = tf.reduce_mean(cls_loss)\n",
        "    # tf.print('\\nDone class loss part')\n",
        "    #------------------------\n",
        "    #-----Confident Loss-----\n",
        "    #------------------------\n",
        "    reduce_confident_loss = tf.reduce_mean(store_confident_loss)\n",
        "    # print('confident loss', store_confident_loss)\n",
        "\n",
        "    #----------------------------\n",
        "    #------ SUM OF LOSSES--------\n",
        "    #----------------------------\n",
        "    regression_loss = final_cost\n",
        "\n",
        "    # Training\n",
        "    regression_loss = tf.cast(regression_loss,dtype=tf.float32)\n",
        "\n",
        "    # Testing\n",
        "    # regression_loss = tf.cast(regression_loss,dtype=tf.double)\n",
        "\n",
        "    # tf.print('\\nreduce_confident_loss = ', reduce_confident_loss)\n",
        "    # tf.print('\\nreduce_mean_cls_loss = ', reduce_mean_cls_loss)\n",
        "    # tf.print('\\nregression loss = ', regression_loss)\n",
        "    sum_loss = reduce_mean_cls_loss + regression_loss + reduce_confident_loss\n",
        "    return sum_loss\n",
        "\n",
        "# Use with model.fit() \n",
        "def hungarian_loss_fit_test(y_true, y_pred):\n",
        "    loss = tf.py_function(func=hungarian_loss_test, inp=[y_true,y_pred], Tout=tf.float32,)\n",
        "    loss_tensor = tf.convert_to_tensor(loss)\n",
        "    # print('hungarian loss shape',cost_tensor.shape)\n",
        "\n",
        "    return loss_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrhtWCIvtgYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6e250a-1a8d-4f25-affe-3cb68954da99"
      },
      "source": [
        "hungarian_loss_test(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = \n",
            " [[[0.193659514 0.148534104 0.440788865 ... 0.882218242 0.201700673 0.849517]\n",
            "  [0.476395965 0.487233788 0.380835712 ... 0.277627259 0.528800964 0.230658546]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  ...\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]]\n",
            "\n",
            " [[0.0326961689 0.371385038 1.04262865 ... 0.577944219 0.201805219 0.70638907]\n",
            "  [0.667315185 0.743829489 1.42213762 ... 0.27326411 0.498248219 0.106565461]\n",
            "  [1.58089912 1.82260823 1.02121842 ... 2.18821263 1.70456493 2.21871328]\n",
            "  ...\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]]\n",
            "\n",
            " [[0.359569937 0.36499396 0.748291433 ... 0.552292347 0.5146662 0.639129341]\n",
            "  [0.284199983 0.420850813 0.817118049 ... 0.710601091 0.157643139 0.00395428482]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  ...\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.110713765 0.217543826 0.252665877 ... 0.579328954 0.877056479 0.615305722]\n",
            "  [0.135067925 0.228547305 0.264791816 ... 0.583549678 0.886329889 0.617561221]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  ...\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]]\n",
            "\n",
            " [[1.02219808 0.416799426 0.505354106 ... 0.0748257563 0.234668836 0.207489759]\n",
            "  [0.982139826 0.257349551 0.497814536 ... 0.267575353 0.372557 0.102648795]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  ...\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]]\n",
            "\n",
            " [[0.576120734 0.304587781 0.273951948 ... 0.422971815 0.354177177 0.386004865]\n",
            "  [0.768311322 0.724441886 0.196210563 ... 0.550720453 0.761566639 0.780487359]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  ...\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]\n",
            "  [1e+10 1e+10 1e+10 ... 1e+10 1e+10 1e+10]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=1.6660322360694408>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apji0x1ZRbAi"
      },
      "source": [
        "tf.reshape(y_true_test_h_l,shape=(16,7,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC4yO7JEcZxR"
      },
      "source": [
        "cost_test = np.array([[4, 1, np.inf], [np.inf, 2, 5], [0.5, 2, 0.25]])\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "row_ind, col_ind = linear_sum_assignment(cost_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpeGQ8zcvOrH"
      },
      "source": [
        "row_ind, col_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45gQxfUcvS9x"
      },
      "source": [
        "cost_test[row_ind, col_ind].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHDaokbocjBS"
      },
      "source": [
        "test_cost_matrix_true = tf.random.uniform(shape=(2,2))\n",
        "test_cost_matrix_pred = tf.random.uniform(shape=(2,2))\n",
        "store_element = [tf.square(test_cost_matrix_true[:,i] - test_cost_matrix_pred[:,j]) for i in range(test_cost_matrix_true.shape[1]) for j in range(test_cost_matrix_pred.shape[1])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmU_L3ypC3xH"
      },
      "source": [
        "hungarian_loss(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vesBPR4eYHj3"
      },
      "source": [
        "# Test loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icXB6kCaYK2A"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "def rmsle_K(y_true, y_pred):\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "\n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "    # Take only 2 first dimension\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "\n",
        "    return K.sqrt(tf.experimental.numpy.nanmean(K.square(tf.math.log1p(y_true_) - tf.math.log1p(y_pred_))))\n",
        "\n",
        "def rmsle_K_fit_test(y_true, y_pred):\n",
        "    loss = tf.py_function(func=rmsle_K, inp=[y_true,y_pred], Tout=tf.float32,)\n",
        "    loss_tensor = tf.convert_to_tensor(loss)\n",
        "    return loss_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jdQ1USwZ5UJ",
        "outputId": "235ae634-5ca3-401b-86e6-8384489047fc"
      },
      "source": [
        "rmsle_K(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=0.25896492599772303>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvx-kGWfH-uQ"
      },
      "source": [
        "# Custom metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmsMDXxQIAlJ"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def custom_mse(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true,dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred,dtype=tf.float32)\n",
        "    loss = K.square(y_pred - y_true)  # (batch_size, 2)\n",
        "    loss = tf.experimental.numpy.nanmean(loss,axis=-1)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIuEafvMIvyC"
      },
      "source": [
        "class custom_MSE(tf.keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self, name='custom_mse', **kwargs):\n",
        "    super(custom_MSE, self).__init__(name=name, **kwargs)\n",
        "    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.convert_to_tensor(y_true)\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    \n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "    \n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "    \n",
        "    y_true_ = tf.cast(y_true_, tf.float32)\n",
        "    y_pred_ = tf.cast(y_pred_, tf.float32)\n",
        "\n",
        "    # y_true_reg = y_true[:,:2]\n",
        "    # y_pred_reg = y_pred[:,:2]\n",
        "\n",
        "    loss = K.square(y_true_ - y_pred_)  \n",
        "    \n",
        "    loss = tf.experimental.numpy.nanmean(loss,axis=1)\n",
        "    # loss = tf.experimental.numpy.nanmean(loss,axis=0)\n",
        "    # tf.print(loss)\n",
        "    if sample_weight is not None:\n",
        "        sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "        values = tf.multiply(values, sample_weight)\n",
        "    \n",
        "    # self.true_positives.assign_add(tf.reduce_mean(loss))\n",
        "    # tf.print(loss)\n",
        "    self.true_positives.assign(tf.reduce_mean(loss))\n",
        "\n",
        "  def result(self):\n",
        "    return self.true_positives\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.true_positives.assign(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj3GbHvUtM_O"
      },
      "source": [
        "test_custom_mse = custom_MSE()\n",
        "test_custom_mse(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjyvjb0etWXM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61D_cJqN-r3k"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxlu1leK9D0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f2ec30-6ed2-4b87-fa6d-d2f55265cb99"
      },
      "source": [
        "def add_confident_value(label):\n",
        "    reshape = tf.reshape(label,shape = (7,3))\n",
        "    store_new_values = np.empty(shape = (7,4))\n",
        "\n",
        "    for i in range(reshape.shape[0]):\n",
        "        for j in range(reshape.shape[1]):\n",
        "            store_new_values[i,j] = reshape[i,j]\n",
        "        \n",
        "        if np.isnan(reshape[i,2]):\n",
        "            # print('in if statement',reshape_test_[i,2])\n",
        "            store_new_values[i,3] = 0\n",
        "        else:\n",
        "            # print('in elif statement',reshape_test_[i,2])\n",
        "            store_new_values[i,3] = 1\n",
        "    store_new_values = tf.reshape(store_new_values,shape=(28,))\n",
        "    return store_new_values\n",
        "\n",
        "def plot_images(images):\n",
        "    assert isinstance(images, (list, tuple, np.ndarray))    \n",
        "    cols = min(6, len(images))\n",
        "    rows = 1 + (len(images)-1)//cols\n",
        "    plt.figure(figsize=(3.2*cols, 1.2*rows))\n",
        "    for n, image in enumerate(images):\n",
        "        plt.subplot(rows, cols, n+1)\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "        # plt.xlabel(f'{pred_titles[n]}', size=14, c='blue')\n",
        "        plt.imshow(image)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_dir_name(filename):\n",
        "    pos1 = filename.rfind('_')\n",
        "    pos2 = filename.find('.')\n",
        "    return filename[pos1+1:pos2], filename[0:pos1]\n",
        "\n",
        "TRAIN_SET_PATH = '/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test'\n",
        "town_listdir = os.listdir(TRAIN_SET_PATH)\n",
        "print(town_listdir)\n",
        "\n",
        "def read_rgb_image(rgb_path):\n",
        "    image = cv2.imread(rgb_path)\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image,(200,200))\n",
        "\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = image[0:,:,:]\n",
        "    image = image/255.0\n",
        "    return image\n",
        "\n",
        "def read_depth_image(depth_path):\n",
        "    depth_img = cv2.imread(depth_path)\n",
        "    depth_img = cv2.cvtColor(depth_img,cv2.COLOR_BGR2RGB)\n",
        "    depth_img = cv2.resize(depth_img,(200,200))\n",
        "\n",
        "    R = depth_img[:, :, 0].astype('float32')\n",
        "    G = depth_img[:, :, 1].astype('float32')\n",
        "    B = depth_img[:, :, 2].astype('float32')\n",
        "    normalized = (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1)\n",
        "    in_meters = 1000 * normalized\n",
        "    in_meters[in_meters == 1000] = 0\n",
        "    in_meters = tf.expand_dims(in_meters,axis=-1)\n",
        "    return in_meters\n",
        "\n",
        "def normalize_on_rgb_depth(concat_img):\n",
        "    # mean = [103.939, 116.779, 123.68, 1]\n",
        "    channel_avg = np.array([0.485, 0.456, 0.406])\n",
        "    channel_std = np.array([0.229, 0.224, 0.225])\n",
        "    # concat_img[:,:,0:3] = (concat_img[:,:,0:3] - channel_avg) / channel_std\n",
        "    \n",
        "    \n",
        "    concat_img_list = concat_img.numpy()\n",
        "    # print(concat_img_list.shape)\n",
        "    concat_img_list[:,:,0:3] = (concat_img_list[:,:,0:3] - channel_avg) / channel_std\n",
        "\n",
        "    concat_img_list[:,:,3][concat_img_list[:,:,3] == 0.0] = 1 # log10(1) = 0 thats what we want, not -inf\n",
        "\n",
        "    concat_img_list[:,:,3] = np.log10(concat_img_list[:,:,3])\n",
        "    concat_img_tensor = tf.convert_to_tensor(concat_img_list)\n",
        "    # concat_img_tensor.astype(np.float32)\n",
        "    # print(concat_img_tensor)\n",
        "    concat_img_tensor = tf.cast(concat_img_tensor,tf.float32)\n",
        "    return concat_img_tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Town04', 'Town01']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JckoguKi-uS8"
      },
      "source": [
        "# Load data train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiF8k3YqOuGN"
      },
      "source": [
        "def load_4_side_data_train():\n",
        "    def fetch_data_pair():\n",
        "        for i,(town) in enumerate(town_listdir): # use enumerate for debugging to check the loops\n",
        "            town_path = os.path.join(TRAIN_SET_PATH,town)\n",
        "\n",
        "            csv_ego_path = os.path.join(town_path,'location.csv')\n",
        "\n",
        "            csv_agent_path = os.path.join(town_path,'agent_vehicle_information.csv')\n",
        "            csv_walker_path = os.path.join(town_path,'walkers_information.csv')\n",
        "            # print(csv_ego_path,csv_agent_path)\n",
        "            rgb_path = os.path.join(town_path,'rgb_image')\n",
        "\n",
        "            front_rgb_path = os.path.join(rgb_path,'front_image')\n",
        "            \n",
        "            round_1_4 = os.listdir(front_rgb_path)\n",
        "            round_1_4.sort()\n",
        "\n",
        "            SET_TRAIN_PATH = [os.path.join(front_rgb_path,round_1_4[i]) for i in range(0,4)]\n",
        "            \n",
        "            df = pd.read_csv(csv_ego_path)\n",
        "            df_agent = pd.read_csv(csv_agent_path)\n",
        "            df_walker = pd.read_csv(csv_walker_path)\n",
        "\n",
        "            #------------------------------------------------\n",
        "            #--------FILL THE MISSING VALUES-----------------\n",
        "            #------------------------------------------------\n",
        "            df['relative_x'] = df['relative_x'].fillna(0)\n",
        "            df['relative_y'] = df['relative_y'].fillna(0)\n",
        "            df['throttle'] = df['throttle'].fillna(0)\n",
        "            df['steer'] = df['steer'].fillna(0)\n",
        "\n",
        "            df_walker['relative_x'] = df_walker['relative_x'].fillna(0)\n",
        "            df_walker['relative_y'] = df_walker['relative_y'].fillna(0)\n",
        "            # df_walker['throttle'] = df_walker['throttle'].fillna(0)\n",
        "            # df_walker['steer'] = df_walker['steer'].fillna(0)\n",
        "            #-------------------------------------------------\n",
        "\n",
        "            \n",
        "            # print('in train',os.path.isfile(csv_agent_path))\n",
        "            for round in SET_TRAIN_PATH:  \n",
        "                for subdir,_,name_images in os.walk(round):\n",
        "                    name_images.sort()\n",
        "                    # print(subdir)\n",
        "                    '''\n",
        "                    subdir: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image/round1\n",
        "                    '''\n",
        "\n",
        "                    # split the directory to get the depth directory\n",
        "                    '''\n",
        "                    head1: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image\n",
        "                    split_round: round1 -> round 4\n",
        "                    '''\n",
        "                    head1, split_round = os.path.split(subdir)\n",
        "                    \n",
        "                    '''\n",
        "                    rgb_img_path_split: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image\n",
        "                    '''\n",
        "                    rgb_img_path_split, front_img_split = os.path.split(head1)\n",
        "                    path_to_town = head1[:97]\n",
        "\n",
        "                    # print(path_to_town)\n",
        "                    left_path = 'left_image'\n",
        "                    right_path = 'right_image'\n",
        "                    back_path = 'back_image'\n",
        "                    front_path = 'front_image'\n",
        "\n",
        "\n",
        "                    path_to_depth = os.path.join(path_to_town,'depth_image')\n",
        "                    subdir_depth_front = os.path.join(path_to_depth, front_img_split)\n",
        "                    subdir_depth_round = os.path.join(subdir_depth_front, split_round)\n",
        "                    # print(subdir_depth_front)\n",
        "\n",
        "                    for name in name_images:\n",
        "                        # print(name)\n",
        "                        path_to_rgb_front_img = os.path.join(subdir,name)\n",
        "                        # print(subdir)\n",
        "\n",
        "                        # path to rgb left image\n",
        "                        path_to_rgb_left = os.path.join(rgb_img_path_split,left_path)\n",
        "                        path_to_rgb_left_img = os.path.join(path_to_rgb_left,split_round)\n",
        "                        path_to_rgb_left_img = os.path.join(path_to_rgb_left_img,name)\n",
        "                        # print(path_to_rgb_left_img)\n",
        "                        # print(os.path.isfile(path_to_rgb_left_img))\n",
        "                        # print(path_to_rgb_left_img)\n",
        "\n",
        "                        # path to rgb right image\n",
        "                        path_to_rgb_right = os.path.join(rgb_img_path_split,right_path)\n",
        "                        path_to_rgb_right_img = os.path.join(path_to_rgb_right,split_round)\n",
        "                        path_to_rgb_right_img = os.path.join(path_to_rgb_right_img,name)\n",
        "                        # print(os.path.isfile(path_to_rgb_right_img))\n",
        "\n",
        "                        # path to rgb back image\n",
        "                        path_to_rgb_back = os.path.join(rgb_img_path_split,back_path)\n",
        "                        path_to_rgb_back_img = os.path.join(path_to_rgb_back,split_round)\n",
        "                        path_to_rgb_back_img = os.path.join(path_to_rgb_back_img,name)\n",
        "                        # print(os.path.isfile(path_to_rgb_back_img))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                        # --------------------Split the path to get the path depth---------------------------\n",
        "\n",
        "                       \n",
        "                        path_to_depth_front = os.path.join(subdir_depth_round,name)\n",
        "                        # path_to_depth_front_img = os.path.join(subdir_depth_round,name)\n",
        "                        # print(os.path.isfile(path_to_depth_front))\n",
        "\n",
        "                        # split the depth path \n",
        "                        path_to_depth_split = path_to_depth_front[:109]\n",
        "                        path_from_depth_round_to_img = path_to_depth_front[122:] # round4/round4_024192.png\n",
        "                        # ----> works when rgb image has .png extension\n",
        "                        \n",
        "                        test = path_to_depth_front[122:142]\n",
        "                        path_from_depth_round_to_img = test + '.png'\n",
        "                        # print(path_from_depth_round_to_img)\n",
        "\n",
        "                        # path to depth front image\n",
        "                        path_to_depth_front = os.path.join(path_to_depth_split,front_path)\n",
        "                        path_to_depth_front_img = os.path.join(path_to_depth_front,path_from_depth_round_to_img)\n",
        "\n",
        "                        # path to depth right image\n",
        "                        path_to_depth_right = os.path.join(path_to_depth_split,right_path)\n",
        "                        path_to_depth_right_img = os.path.join(path_to_depth_right,path_from_depth_round_to_img)\n",
        "                        # print(os.path.isfile(path_to_depth_right))\n",
        "\n",
        "                        # path to depth left image\n",
        "                        path_to_depth_left = os.path.join(path_to_depth_split,left_path)\n",
        "                        path_to_depth_left_img = os.path.join(path_to_depth_left,path_from_depth_round_to_img)\n",
        "                        # print(os.path.isfile(path_to_depth_left))\n",
        "                        # print(path_to_depth_left_img)\n",
        "\n",
        "                        # path to depth back image\n",
        "                        path_to_depth_back = os.path.join(path_to_depth_split,back_path)\n",
        "                        path_to_depth_back_img = os.path.join(path_to_depth_back,path_from_depth_round_to_img)\n",
        "                        # print(os.path.isfile(path_to_depth_back))\n",
        "\n",
        "                        frame,round = get_dir_name(path_to_rgb_front_img)\n",
        "                        try:\n",
        "                            frame = int(frame)\n",
        "                        except:\n",
        "                            print('Can not convert', str ,\"to int\")\n",
        "                            print(frame, \"   \", path_to_rgb_front_img)\n",
        "\n",
        "                        # label_steer = 0\n",
        "                        # label_throttle = 0\n",
        "                        # label_velocity = 0\n",
        "\n",
        "                        #------------------------------\n",
        "                        #------create nan array--------\n",
        "                        #------------------------------\n",
        "                        label_array = np.empty((21,))\n",
        "                        label_array[:] = np.NaN\n",
        "                        label_array = np.array(label_array)\n",
        "                        #-------------------------------\n",
        "\n",
        "                        label = []\n",
        "\n",
        "                        for index,row in df.iterrows():\n",
        "                            if row['frame'] == frame:\n",
        "                                cls = 1\n",
        "                                relative_x = row['relative_x']\n",
        "                                relative_y = row['relative_y']\n",
        "                                label.append([float(relative_x), float(relative_y),int(cls)])\n",
        "                                # print('im okay')\n",
        "\n",
        "                        label_walker = []\n",
        "                    \n",
        "                        for index, row in df_walker.iterrows():\n",
        "                            if row['frame'] == frame:\n",
        "                                cls = 0\n",
        "                                relative_x = row['relative_x']\n",
        "                                relative_y = row['relative_y']\n",
        "                                label_walker.append([float(relative_x),float(relative_y),int(cls)])\n",
        "                            else:\n",
        "                                # print('searching for the correct frame of walker')\n",
        "                                # print('frame of walker',row['frame'])\n",
        "                                # print('frame of images',frame)\n",
        "                                pass\n",
        "                                # label_walker.append([float(np.nan),float(np.nan),int(np.nan)])\n",
        "\n",
        "                        #-------------------------------------------\n",
        "                        #----------TODO---------------------------\n",
        "                        #---------------------------------------\n",
        "                        # Generate again with the relative to the walkers, cannot do it in here because we do not calculate the\n",
        "                        # relative in the same loop\n",
        "\n",
        "                        # After having it, it needs simply add to the let say label_walker, the concatenate label and label_walker\n",
        "                        # then using for loop as below to add value to the NaN arrays\n",
        "                        \n",
        "\n",
        "                        label = np.array(label)\n",
        "                        label_walker = np.array(label_walker)\n",
        "                        \n",
        "                        # print(label.shape)\n",
        "                        \n",
        "\n",
        "                        # if label.shape[0] == 1 and label_agent.shape[0] == 1: # label shape from the df is (1,3). so I need to squeeze it to (3,)\n",
        "                            # print('OKE') \n",
        "                        if label.shape[0] != 0 or label_walker.shape[0] != 0:\n",
        "                            # label = tf.squeeze(label,axis=0)\n",
        "                            # label_agent = tf.squeeze(label_agent,axis=0)\n",
        "                            if label.shape[0] != 1 and len(label) != 0:\n",
        "                                # print(label)\n",
        "                                label = tf.reshape(label,shape=(1,label.shape[0]*label.shape[1]))\n",
        "                            if label_walker.shape[0] != 1 and len(label_walker) != 0:\n",
        "                                # print('label walker',label_walker)\n",
        "                                label_walker = tf.reshape(label_walker,shape=(1,label_walker.shape[0]*label_walker.shape[1]))\n",
        "                            \n",
        "\n",
        "                            # if walker and agent vehicles are seen, then it is concatenated\n",
        "                            if label.shape[0] == 1 and label_walker.shape[0] == 1:\n",
        "                                final_label = tf.concat([label, label_walker],axis=1)\n",
        "                                # print(final_label)\n",
        "                            # if only agent vehicle, then final label is the agent label\n",
        "                            elif label.shape[0] == 1 and label_walker.shape[0] == 0:\n",
        "                                final_label = label\n",
        "                            # if only walker, then final label is the walker\n",
        "                            elif label.shape[0] == 0 and label_walker.shape[0] == 1:\n",
        "                                final_label = label_walker\n",
        "\n",
        "                            label_h, label_w = final_label.shape\n",
        "                            # label_walker_h, label_walker_w = label_walker.shape\n",
        "                            # print(label_h,label_w)\n",
        "                            # final_label = np.array([final_label])\n",
        "\n",
        "                            # if the final label has more than 2 objects, then it needs to reshape to (label_h * label_w, )\n",
        "                            if label_h != 1:\n",
        "                                final_label = tf.reshape(final_label,shape=(label_h * label_w))\n",
        "                                # label = label.numpy()\n",
        "                                final_label = np.array([final_label])\n",
        "\n",
        "                   \n",
        "                            # print(label)\n",
        "\n",
        "                            # Finally, it is filled in the label_array\n",
        "                            for i in range(len(final_label[0])):\n",
        "                                label_array[i] = final_label[0][i]\n",
        "\n",
        "                            \n",
        "                            # concat_label = tf.concat([label, label_agent],axis=-1)\n",
        "\n",
        "                            if os.path.isfile(path_to_rgb_front_img) and os.path.isfile(path_to_rgb_left_img) and os.path.isfile(path_to_rgb_right_img) and os.path.isfile(path_to_rgb_back_img) and os.path.isfile(path_to_depth_front_img) and os.path.isfile(path_to_depth_left_img) and os.path.isfile(path_to_depth_right_img) and os.path.isfile(path_to_depth_back_img):\n",
        "\n",
        "                                # read rgb front images\n",
        "                                front_rgb_image = read_rgb_image(path_to_rgb_front_img)\n",
        "\n",
        "                                # read rgb left images\n",
        "                                left_rgb_image = read_rgb_image(path_to_rgb_left_img)\n",
        "\n",
        "                                # read rgb right images\n",
        "                                right_rgb_image = read_rgb_image(path_to_rgb_right_img)\n",
        "\n",
        "                                # read rgb back images\n",
        "                                back_rgb_image = read_rgb_image(path_to_rgb_back_img)\n",
        "                           \n",
        "                                # read depth front images\n",
        "                                front_depth_image = read_depth_image(path_to_depth_front_img)\n",
        "\n",
        "                                # read depth |left images\n",
        "                                left_depth_image = read_depth_image(path_to_depth_left_img)\n",
        "\n",
        "                                # read depth right images\n",
        "                                right_depth_image = read_depth_image(path_to_depth_right_img)\n",
        "\n",
        "                                # read depth back images\n",
        "                                back_depth_image = read_depth_image(path_to_depth_back_img)\n",
        "\n",
        "                                # Concatenate rgb and depth\n",
        "                                concatennate_rgb_depth_front = tf.concat([front_rgb_image,front_depth_image],axis=-1)\n",
        "                                concatennate_rgb_depth_right = tf.concat([right_rgb_image,right_depth_image],axis=-1)\n",
        "                                concatennate_rgb_depth_left = tf.concat([left_rgb_image,left_depth_image],axis=-1)\n",
        "                                concatennate_rgb_depth_back = tf.concat([back_rgb_image,back_depth_image],axis=-1)\n",
        "\n",
        "                                normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "                                normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "                                normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "                                normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)\n",
        "\n",
        "                                label_array_included_conf_val = add_confident_value(label_array)\n",
        "\n",
        "                                # tf.print('Is nan in normalized_front?', tf.math.is_nan(normalized_front))\n",
        "                                # tf.print('Is nan in normalized_left?', tf.math.is_nan(normalized_left))\n",
        "                                # tf.print('Is nan in normalized_right?', tf.math.is_nan(normalized_right))\n",
        "                                # tf.print('Is nan in normalized_back?', tf.math.is_nan(normalized_back))\n",
        "\n",
        "                                yield [normalized_front,normalized_left,normalized_right,normalized_back], label_array_included_conf_val \n",
        "                            else:\n",
        "                                # print('frame of depth image is not same to rgb image')\n",
        "                                pass                   \n",
        "                        else:\n",
        "                            pass\n",
        "                            \n",
        "    return fetch_data_pair       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MRzErCD_DZr"
      },
      "source": [
        "# Load data validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8HQK2N0_BAk"
      },
      "source": [
        "def load_4_side_data_validation():\n",
        "    def fetch_data_pair():\n",
        "        for i,(town) in enumerate(town_listdir): # use enumerate for debugging to check the loops\n",
        "            town_path = os.path.join(TRAIN_SET_PATH,town)\n",
        "\n",
        "            csv_ego_path = os.path.join(town_path,'location.csv')\n",
        "\n",
        "            csv_agent_path = os.path.join(town_path,'agent_vehicle_information.csv')\n",
        "            # print(csv_ego_path,csv_agent_path)\n",
        "            csv_walker_path = os.path.join(town_path,'walkers_information.csv')\n",
        "            rgb_path = os.path.join(town_path,'rgb_image')\n",
        "\n",
        "            front_rgb_path = os.path.join(rgb_path,'front_image')\n",
        "            \n",
        "            round_1_4 = os.listdir(front_rgb_path)\n",
        "            round_1_4.sort()\n",
        "\n",
        "            # SET_TRAIN_PATH = [os.path.join(front_rgb_path,round_1_4[i]) for i in range(0,4)]\n",
        "            SET_DEV_PATH = os.path.join(front_rgb_path,round_1_4[4])\n",
        "\n",
        "            # print(os.path.isdir(SET_DEV_PATH))\n",
        "\n",
        "            df = pd.read_csv(csv_ego_path)\n",
        "            df_walker = pd.read_csv(csv_walker_path)\n",
        "            #------------------------------------------------\n",
        "            #--------FILL THE MISSING VALUES-----------------\n",
        "            #------------------------------------------------\n",
        "            df['relative_x'] = df['relative_x'].fillna(0)\n",
        "            df['relative_y'] = df['relative_y'].fillna(0)\n",
        "            df['throttle'] = df['throttle'].fillna(0)\n",
        "            df['steer'] = df['steer'].fillna(0)\n",
        "\n",
        "            df_walker['relative_x'] = df_walker['relative_x'].fillna(0)\n",
        "            df_walker['relative_y'] = df_walker['relative_y'].fillna(0)\n",
        "            #-------------------------------------------------\n",
        "            \n",
        "            # df_agent = pd.read_csv(csv_agent_path)\n",
        "            # print('in validation',os.path.isfile(csv_agent_path))\n",
        "            \n",
        "            for subdir,_,name_images in os.walk(SET_DEV_PATH):\n",
        "                name_images.sort()\n",
        "                # print(subdir)\n",
        "                '''\n",
        "                subdir: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image/round1\n",
        "                '''\n",
        "\n",
        "                # split the directory to get the depth directory\n",
        "                '''\n",
        "                head1: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image\n",
        "                split_round: round1 -> round 4\n",
        "                '''\n",
        "                head1, split_round = os.path.split(subdir)\n",
        "                \n",
        "                '''\n",
        "                rgb_img_path_split: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image\n",
        "                '''\n",
        "                rgb_img_path_split, front_img_split = os.path.split(head1)\n",
        "                path_to_town = head1[:97]\n",
        "\n",
        "                # print(path_to_town)\n",
        "                left_path = 'left_image'\n",
        "                right_path = 'right_image'\n",
        "                back_path = 'back_image'\n",
        "                front_path = 'front_image'\n",
        "\n",
        "                path_to_depth = os.path.join(path_to_town,'depth_image')\n",
        "                subdir_depth_front = os.path.join(path_to_depth, front_img_split)\n",
        "                subdir_depth_round = os.path.join(subdir_depth_front, split_round)\n",
        "                # print(subdir_depth_front)\n",
        "\n",
        "                for name in name_images:\n",
        "                    # print(name)\n",
        "                    path_to_rgb_front_img = os.path.join(subdir,name)\n",
        "                    # print(subdir)\n",
        "\n",
        "                    # path to rgb left image\n",
        "                    path_to_rgb_left = os.path.join(rgb_img_path_split,left_path)\n",
        "                    path_to_rgb_left_img = os.path.join(path_to_rgb_left,split_round)\n",
        "                    path_to_rgb_left_img = os.path.join(path_to_rgb_left_img,name)\n",
        "                    # print(os.path.isfile(path_to_rgb_left_img))\n",
        "                    # print(path_to_rgb_left_img)\n",
        "\n",
        "                    # path to rgb right image\n",
        "                    path_to_rgb_right = os.path.join(rgb_img_path_split,right_path)\n",
        "                    path_to_rgb_right_img = os.path.join(path_to_rgb_right,split_round)\n",
        "                    path_to_rgb_right_img = os.path.join(path_to_rgb_right_img,name)\n",
        "                    # print(os.path.isfile(path_to_rgb_right_img))\n",
        "\n",
        "                    # path to rgb back image\n",
        "                    path_to_rgb_back = os.path.join(rgb_img_path_split,back_path)\n",
        "                    path_to_rgb_back_img = os.path.join(path_to_rgb_back,split_round)\n",
        "                    path_to_rgb_back_img = os.path.join(path_to_rgb_back_img,name)\n",
        "                    # print(os.path.isfile(path_to_rgb_back_img))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    # path to depth front image\n",
        "                    path_to_depth_front = os.path.join(subdir_depth_round,name)\n",
        "                    \n",
        "\n",
        "                    #-----------------------------------------------------------------------------\n",
        "                    #--------------------------- split the depth path ----------------------------\n",
        "                    #-----------------------------------------------------------------------------\n",
        "                    path_to_depth_split = path_to_depth_front[:109]\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    path_from_depth_round_to_img = path_to_depth_front[122:] # round4/round4_024192.png\n",
        "                    test = path_to_depth_front[122:142]\n",
        "                    path_from_depth_round_to_img = test + '.png'\n",
        "                    # print(path_from_depth_round_to_img)\n",
        "\n",
        "                    # path to depth front image\n",
        "                    path_to_depth_front = os.path.join(path_to_depth_split,front_path)\n",
        "                    path_to_depth_front_img = os.path.join(path_to_depth_front,path_from_depth_round_to_img)\n",
        "                    # print(os.path.isfile(path_to_depth_front_img))\n",
        "                    # print(path_to_depth_front_img)\n",
        "\n",
        "                    # path to depth right image\n",
        "                    path_to_depth_right = os.path.join(path_to_depth_split,right_path)\n",
        "                    path_to_depth_right_img = os.path.join(path_to_depth_right,path_from_depth_round_to_img)\n",
        "                    # print(path_to_depth_right_img)\n",
        "                    # print('path to depth right is ',os.path.isfile(path_to_depth_right_img))\n",
        "\n",
        "                    # path to depth left image\n",
        "                    path_to_depth_left = os.path.join(path_to_depth_split,left_path)\n",
        "                    path_to_depth_left_img = os.path.join(path_to_depth_left,path_from_depth_round_to_img)\n",
        "                    # print(os.path.isfile(path_to_depth_left_img))\n",
        "                    # print(path_to_depth_left_img)\n",
        "\n",
        "                    # path to depth back image\n",
        "                    path_to_depth_back = os.path.join(path_to_depth_split,back_path)\n",
        "                    path_to_depth_back_img = os.path.join(path_to_depth_back,path_from_depth_round_to_img)\n",
        "                    # print(os.path.isfile(path_to_depth_back_img))\n",
        "\n",
        "                    # print(f'path to rgb front is {os.path.isfile(path_to_rgb_front_img)}\\n\n",
        "                    #         path to rgb left is {os.path.isfile(path_to_rgb_left_img)}\\n\n",
        "                    #         path to rgb right is {os.path.isfile(path_to_rgb_right_img)}\\n\n",
        "                    #         path to rgb back is {os.path.isfile(path_to_rgb_back_img)}\\n\n",
        "                    #         path to depth front is ')\n",
        "\n",
        "                    frame,round = get_dir_name(path_to_rgb_front_img)\n",
        "                    try:\n",
        "                        frame = int(frame)\n",
        "                    except:\n",
        "                        print('Can not convert', str ,\"to int\")\n",
        "\n",
        "                    \n",
        "                    #------------------------------\n",
        "                    #------create nan array--------\n",
        "                    #------------------------------\n",
        "                    label_array = np.empty((21,))\n",
        "                    label_array[:] = np.NaN\n",
        "                    label_array = np.array(label_array)\n",
        "                    #-------------------------------\n",
        "\n",
        "                    label = []\n",
        "\n",
        "                    for index,row in df.iterrows():\n",
        "                        if row['frame'] == frame: \n",
        "                            cls = 1                  \n",
        "                            relative_x = row['relative_x']\n",
        "                            relative_y = row['relative_y']\n",
        "                            label.append([float(relative_x), float(relative_y),int(cls)])\n",
        "                    \n",
        "\n",
        "                    label_walker = []\n",
        "                    for index, row in df_walker.iterrows():\n",
        "                        if row['frame'] == frame:\n",
        "                            cls = 0\n",
        "                            relative_x = row['relative_x']\n",
        "                            relative_y = row['relative_y']\n",
        "                            label_walker.append([float(relative_x),float(relative_y),int(cls)])\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "                    label = np.array(label)\n",
        "                    label_walker = np.array(label_walker)\n",
        "\n",
        "                    # if label.shape[0] == 1 and label_agent.shape[0] == 1:\n",
        "                    if label.shape[0] != 0 or label_walker.shape[0] != 0:\n",
        "\n",
        "                        if label.shape[0] != 1 and len(label) != 0:\n",
        "                                # print(label)\n",
        "                                label = tf.reshape(label,shape=(1,label.shape[0]*label.shape[1]))\n",
        "                        if label_walker.shape[0] != 1 and len(label_walker) != 0:\n",
        "                            # print('label walker',label_walker)\n",
        "                            label_walker = tf.reshape(label_walker,shape=(1,label_walker.shape[0]*label_walker.shape[1]))\n",
        "\n",
        "                        if label.shape[0] == 1 and label_walker.shape[0] == 1:\n",
        "                            final_label = tf.concat([label, label_walker],axis=1)\n",
        "                            # print(final_label)\n",
        "                        elif label.shape[0] == 1 and label_walker.shape[0] == 0:\n",
        "                            final_label = label\n",
        "                        elif label.shape[0] == 0 and label_walker.shape[0] == 1:\n",
        "                            final_label = label_walker\n",
        "\n",
        "                        label_h, label_w = final_label.shape\n",
        "                        # label_walker_h, label_walker_w = label_walker.shape\n",
        "                        # print(label_h,label_w)\n",
        "                        # final_label = np.array([final_label])\n",
        "                        if label_h != 1:\n",
        "                            final_label = tf.reshape(final_label,shape=(label_h * label_w))\n",
        "                            # label = label.numpy()\n",
        "                            final_label = np.array([final_label])\n",
        "\n",
        "                \n",
        "                        # print(label)\n",
        "                        for i in range(len(final_label[0])):\n",
        "                            label_array[i] = final_label[0][i]\n",
        "\n",
        "                        if os.path.isfile(path_to_rgb_front_img) and os.path.isfile(path_to_rgb_left_img) and os.path.isfile(path_to_rgb_right_img) and os.path.isfile(path_to_rgb_back_img) and os.path.isfile(path_to_depth_front_img) and os.path.isfile(path_to_depth_left_img) and os.path.isfile(path_to_depth_right_img) and os.path.isfile(path_to_depth_back_img):\n",
        "\n",
        "                            # read rgb front images\n",
        "                            front_rgb_image = read_rgb_image(path_to_rgb_front_img)\n",
        "\n",
        "                            # read rgb left images\n",
        "                            left_rgb_image = read_rgb_image(path_to_rgb_left_img)\n",
        "                            # print(path_to_rgb_left_img)\n",
        "\n",
        "                            # read rgb right images\n",
        "                            right_rgb_image = read_rgb_image(path_to_rgb_right_img)\n",
        "\n",
        "                            # read rgb back images\n",
        "                            back_rgb_image = read_rgb_image(path_to_rgb_back_img)\n",
        "                         \n",
        "                            # read depth front images\n",
        "                            front_depth_image = read_depth_image(path_to_depth_front_img)\n",
        "\n",
        "                            # read depth left images\n",
        "                            left_depth_image = read_depth_image(path_to_depth_left_img)\n",
        "\n",
        "                            # read depth right images\n",
        "                            right_depth_image = read_depth_image(path_to_depth_right_img)\n",
        "\n",
        "                            # read depth back images\n",
        "                            back_depth_image = read_depth_image(path_to_depth_back_img)\n",
        "\n",
        "                            # Concatenate rgb and depth\n",
        "                            concatennate_rgb_depth_front = tf.concat([front_rgb_image,front_depth_image],axis=-1)\n",
        "                            concatennate_rgb_depth_right = tf.concat([right_rgb_image,right_depth_image],axis=-1)\n",
        "                            concatennate_rgb_depth_left = tf.concat([left_rgb_image,left_depth_image],axis=-1)\n",
        "                            concatennate_rgb_depth_back = tf.concat([back_rgb_image,back_depth_image],axis=-1)\n",
        "\n",
        "                            normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "                            normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "                            normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "                            normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)\n",
        "\n",
        "                            label_array_included_conf_val = add_confident_value(label_array)\n",
        "                            yield [normalized_front,normalized_left,normalized_right,normalized_back], label_array_included_conf_val\n",
        "                        else:\n",
        "                            pass\n",
        "                            # print('frame of depth image is not same to rgb image') \n",
        "                            # print(path_to_rgb_front_img)                        \n",
        "                    else:\n",
        "                        pass\n",
        "                        # print('validation label is Null or has shape (2,2)')\n",
        "    return fetch_data_pair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPJjsNtm-6JT"
      },
      "source": [
        "# Load data from generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50xX2Zg19asf"
      },
      "source": [
        "IMG_SHAPE = (4,200, 200, 4)\n",
        "LABEL_SHAPE = (28,)\n",
        "OUTPUT = 28 # same as label shape\n",
        "train_set = tf.data.Dataset.from_generator(load_4_side_data_train(), \n",
        "                                           output_types=(tf.float32, tf.float64),\n",
        "                                           output_shapes=(tf.TensorShape(IMG_SHAPE), tf.TensorShape(LABEL_SHAPE)\n",
        "                                            ) )\n",
        "\n",
        "validation_set = tf.data.Dataset.from_generator(load_4_side_data_validation(), output_types=(tf.float32, tf.float64),\n",
        "                                         output_shapes=(tf.TensorShape(IMG_SHAPE), tf.TensorShape(LABEL_SHAPE)\n",
        "                                         ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kaz80cfO1dQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6a227b-8dd8-42b6-9fd3-d5fb2ba43065"
      },
      "source": [
        "for i, (feature,labels) in enumerate(train_set.take(60)):\n",
        "    # print(i)\n",
        "    if i % 10 ==0:\n",
        "        # print(i)\n",
        "        print(feature[0])\n",
        "        # plot_images(feature[0])\n",
        "        print(labels)\n",
        "        # break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.25130573 -0.12745096  0.1127669   0.        ]\n",
            "  [-0.21705624 -0.10994396  0.14762528  0.        ]\n",
            "  [-0.19993149 -0.10994396  0.16505449  0.        ]\n",
            "  ...\n",
            "  [-0.31980476 -0.17997196  0.1127669   0.        ]\n",
            "  [-0.31980476 -0.17997196  0.1301961   0.        ]\n",
            "  [-0.3369295  -0.19747896  0.1127669   0.        ]]\n",
            "\n",
            " [[-0.23418099 -0.10994396  0.1127669   0.        ]\n",
            "  [-0.21705624 -0.10994396  0.1301961   0.        ]\n",
            "  [-0.18280673 -0.09243695  0.18248367  0.        ]\n",
            "  ...\n",
            "  [-0.25130573 -0.14495796  0.16505449  0.        ]\n",
            "  [-0.2684305  -0.14495796  0.14762528  0.        ]\n",
            "  [-0.31980476 -0.17997196  0.1301961   0.        ]]\n",
            "\n",
            " [[-0.25130573 -0.14495796  0.1127669   0.        ]\n",
            "  [-0.19993149 -0.10994396  0.14762528  0.        ]\n",
            "  [-0.18280673 -0.07492995  0.18248367  0.        ]\n",
            "  ...\n",
            "  [-0.23418099 -0.12745096  0.18248367  0.        ]\n",
            "  [-0.28555524 -0.14495796  0.14762528  0.        ]\n",
            "  [-0.35405427 -0.19747896  0.1127669   0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.67343104  0.64285725  0.7053596   0.49784002]\n",
            "  [ 0.7590548   0.6778712   0.75764716  0.49784002]\n",
            "  [ 0.74193007  0.7303923   0.79250556  0.49784002]\n",
            "  ...\n",
            "  [ 0.63918155  0.6603643   0.6879304   0.49784824]\n",
            "  [ 0.74193007  0.74789923  0.80993474  0.49784824]\n",
            "  [ 0.77617955  0.7303923   0.80993474  0.49784824]]\n",
            "\n",
            " [[ 0.74193007  0.6603643   0.740218    0.49364895]\n",
            "  [ 0.74193007  0.64285725  0.80993474  0.49364895]\n",
            "  [ 0.74193007  0.71288526  0.7750764   0.49364895]\n",
            "  ...\n",
            "  [ 0.7933043   0.76540625  0.7750764   0.49365723]\n",
            "  [ 0.77617955  0.74789923  0.80993474  0.49365723]\n",
            "  [ 0.6905558   0.64285725  0.7750764   0.49365723]]\n",
            "\n",
            " [[ 0.74193007  0.7303923   0.7750764   0.48950088]\n",
            "  [ 0.65630627  0.64285725  0.6879304   0.48950088]\n",
            "  [ 0.7076805   0.6778712   0.6879304   0.48950088]\n",
            "  ...\n",
            "  [ 0.8275538   0.8004203   0.87965155  0.48950088]\n",
            "  [ 0.77617955  0.74789923  0.8622223   0.48950088]\n",
            "  [ 0.6905558   0.69537824  0.80993474  0.48950088]]], shape=(200, 200, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 22.25017 -19.08486   0.        1.            nan       nan       nan\n",
            "   0.            nan       nan       nan   0.            nan       nan\n",
            "       nan   0.            nan       nan       nan   0.            nan\n",
            "       nan       nan   0.            nan       nan       nan   0.     ], shape=(28,), dtype=float64)\n",
            "tf.Tensor(\n",
            "[[[-0.18280673 -0.09243695  0.14762528  0.        ]\n",
            "  [-0.16568197 -0.09243695  0.14762528  0.        ]\n",
            "  [-0.16568197 -0.07492995  0.16505449  0.        ]\n",
            "  ...\n",
            "  [-0.19993149 -0.10994396  0.16505449  0.        ]\n",
            "  [-0.25130573 -0.12745096  0.1301961   0.        ]\n",
            "  [-0.30268002 -0.17997196  0.1301961   0.        ]]\n",
            "\n",
            " [[-0.16568197 -0.09243695  0.16505449  0.        ]\n",
            "  [-0.14855723 -0.05742295  0.18248367  0.        ]\n",
            "  [-0.16568197 -0.07492995  0.16505449  0.        ]\n",
            "  ...\n",
            "  [-0.25130573 -0.12745096  0.14762528  0.        ]\n",
            "  [-0.28555524 -0.17997196  0.14762528  0.        ]\n",
            "  [-0.30268002 -0.17997196  0.1301961   0.        ]]\n",
            "\n",
            " [[-0.16568197 -0.10994396  0.16505449  0.        ]\n",
            "  [-0.16568197 -0.07492995  0.16505449  0.        ]\n",
            "  [-0.13143247 -0.05742295  0.18248367  0.        ]\n",
            "  ...\n",
            "  [-0.25130573 -0.14495796  0.16505449  0.        ]\n",
            "  [-0.2684305  -0.16246496  0.14762528  0.        ]\n",
            "  [-0.31980476 -0.17997196  0.1127669   0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.7933043   0.81792724  0.82736397  0.4976014 ]\n",
            "  [ 0.9816766   0.95798326  1.019085    0.4976014 ]\n",
            "  [ 0.8275538   0.81792724  0.8970807   0.4976014 ]\n",
            "  ...\n",
            "  [ 0.7590548   0.74789923  0.82736397  0.4976014 ]\n",
            "  [ 0.7933043   0.78291327  0.9145099   0.4976014 ]\n",
            "  [ 0.8275538   0.83543426  0.87965155  0.4976014 ]]\n",
            "\n",
            " [[ 0.74193007  0.74789923  0.79250556  0.4934246 ]\n",
            "  [ 0.81042904  0.78291327  0.84479314  0.4934246 ]\n",
            "  [ 0.604932    0.62535024  0.7227888   0.4934246 ]\n",
            "  ...\n",
            "  [ 0.8446786   0.83543426  0.8970807   0.4934329 ]\n",
            "  [ 0.8275538   0.81792724  0.9493683   0.4934329 ]\n",
            "  [ 0.74193007  0.7303923   0.80993474  0.4934329 ]]\n",
            "\n",
            " [[ 0.77617955  0.78291327  0.9319391   0.4892912 ]\n",
            "  [ 0.6220568   0.6603643   0.75764716  0.4892912 ]\n",
            "  [ 0.22818747  0.29271722  0.4439217   0.4892912 ]\n",
            "  ...\n",
            "  [ 0.8446786   0.8529413   0.9667975   0.4892912 ]\n",
            "  [ 0.8275538   0.8004203   0.9842267   0.4892912 ]\n",
            "  [ 0.8275538   0.8004203   0.9667975   0.4892912 ]]], shape=(200, 200, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[  5.65193 -26.35073   1.        1.       18.39594  16.79524   1.\n",
            "   1.       18.34284  -9.58091   0.        1.            nan       nan\n",
            "       nan   0.            nan       nan       nan   0.            nan\n",
            "       nan       nan   0.            nan       nan       nan   0.     ], shape=(28,), dtype=float64)\n",
            "tf.Tensor(\n",
            "[[[-0.28555524 -0.17997196  0.02562094  0.        ]\n",
            "  [-0.25130573 -0.16246496  0.07790852  0.        ]\n",
            "  [-0.23418099 -0.17997196  0.02562094  0.        ]\n",
            "  ...\n",
            "  [-0.2684305  -0.17997196  0.07790852  0.        ]\n",
            "  [-0.28555524 -0.17997196  0.06047932  0.        ]\n",
            "  [-0.31980476 -0.19747896  0.04305013  0.        ]]\n",
            "\n",
            " [[-0.2684305  -0.19747896  0.04305013  0.        ]\n",
            "  [-0.2684305  -0.17997196  0.06047932  0.        ]\n",
            "  [-0.25130573 -0.17997196  0.06047932  0.        ]\n",
            "  ...\n",
            "  [-0.2684305  -0.17997196  0.07790852  0.        ]\n",
            "  [-0.28555524 -0.19747896  0.06047932  0.        ]\n",
            "  [-0.3369295  -0.21498597  0.06047932  0.        ]]\n",
            "\n",
            " [[-0.25130573 -0.17997196  0.06047932  0.        ]\n",
            "  [-0.25130573 -0.17997196  0.06047932  0.        ]\n",
            "  [-0.2684305  -0.17997196  0.06047932  0.        ]\n",
            "  ...\n",
            "  [-0.2684305  -0.17997196  0.07790852  0.        ]\n",
            "  [-0.31980476 -0.21498597  0.07790852  0.        ]\n",
            "  [-0.38830376 -0.267507    0.00819175  0.94358605]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.65630627  0.6603643   0.75764716  0.4998427 ]\n",
            "  [ 0.77617955  0.78291327  0.9493683   0.4998345 ]\n",
            "  [ 0.7248053   0.76540625  0.84479314  0.4998345 ]\n",
            "  ...\n",
            "  [ 0.7076805   0.76540625  0.80993474  0.5016079 ]\n",
            "  [ 0.65630627  0.7303923   0.84479314  0.5016079 ]\n",
            "  [ 0.7590548   0.8004203   0.9319391   0.5015997 ]]\n",
            "\n",
            " [[ 0.74193007  0.74789923  0.9145099   0.49562138]\n",
            "  [ 0.6220568   0.6603643   0.7750764   0.49561313]\n",
            "  [ 0.7076805   0.76540625  0.87965155  0.49561313]\n",
            "  ...\n",
            "  [ 0.77617955  0.8004203   0.84479314  0.49732965]\n",
            "  [ 0.87892807  0.9229693   1.0365143   0.49732965]\n",
            "  [ 0.7590548   0.78291327  0.8970807   0.49732965]]\n",
            "\n",
            " [[ 0.77617955  0.78291327  0.84479314  0.49144214]\n",
            "  [ 0.65630627  0.6603643   0.7053596   0.49143377]\n",
            "  [ 0.7248053   0.74789923  0.75764716  0.49143377]\n",
            "  ...\n",
            "  [ 0.7933043   0.78291327  0.9319391   0.4931087 ]\n",
            "  [ 0.77617955  0.78291327  0.9145099   0.4931087 ]\n",
            "  [ 0.7933043   0.81792724  0.9319391   0.49310037]]], shape=(200, 200, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ -4.19244 -23.70473   1.        1.       18.51088  13.66825   1.\n",
            "   1.       14.78433  -6.6691    0.        1.            nan       nan\n",
            "       nan   0.            nan       nan       nan   0.            nan\n",
            "       nan       nan   0.            nan       nan       nan   0.     ], shape=(28,), dtype=float64)\n",
            "tf.Tensor(\n",
            "[[[0.1425637  0.29271722 0.6007844  0.        ]\n",
            "  [0.15968846 0.29271722 0.6007844  0.        ]\n",
            "  [0.22818747 0.3452382  0.653072   0.        ]\n",
            "  ...\n",
            "  [0.41655976 0.48529422 0.6879304  0.        ]\n",
            "  [0.41655976 0.48529422 0.7053596  0.        ]\n",
            "  [0.39943498 0.46778724 0.7053596  0.        ]]\n",
            "\n",
            " [[0.1425637  0.29271722 0.6007844  0.        ]\n",
            "  [0.15968846 0.3102242  0.61821365 0.        ]\n",
            "  [0.15968846 0.3102242  0.6356428  0.        ]\n",
            "  ...\n",
            "  [0.41655976 0.48529422 0.7227888  0.        ]\n",
            "  [0.41655976 0.48529422 0.740218   0.        ]\n",
            "  [0.41655976 0.48529422 0.7227888  0.        ]]\n",
            "\n",
            " [[0.1425637  0.29271722 0.6007844  0.        ]\n",
            "  [0.1425637  0.29271722 0.6007844  0.        ]\n",
            "  [0.1768132  0.32773122 0.6356428  0.        ]\n",
            "  ...\n",
            "  [0.4336845  0.50280124 0.740218   0.        ]\n",
            "  [0.41655976 0.48529422 0.7227888  0.        ]\n",
            "  [0.41655976 0.48529422 0.6879304  0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.1871736  1.1505603  1.1410894  0.5147417 ]\n",
            "  [1.1529241  1.1505603  1.1759478  0.5147417 ]\n",
            "  [1.2042984  1.1680673  1.1759478  0.5147417 ]\n",
            "  ...\n",
            "  [1.2556727  1.2030813  1.1759478  0.5147417 ]\n",
            "  [1.2556727  1.1855743  1.1236602  0.5147417 ]\n",
            "  [1.3241717  1.2731093  1.2456646  0.5147417 ]]\n",
            "\n",
            " [[1.1357994  1.0805323  1.0713726  0.51031184]\n",
            "  [1.2214231  1.1330533  1.1236602  0.51031184]\n",
            "  [1.1186746  1.0455183  1.0016558  0.51031184]\n",
            "  ...\n",
            "  [1.2385479  1.1855743  1.1585186  0.51031184]\n",
            "  [1.2727973  1.2205883  1.1236602  0.51031184]\n",
            "  [1.2385479  1.2205883  1.1585186  0.51031184]]\n",
            "\n",
            " [[1.2385479  1.2205883  1.2456646  0.5059252 ]\n",
            "  [1.2042984  1.1330533  1.0713726  0.5059252 ]\n",
            "  [1.1015499  1.0980393  1.019085   0.5059252 ]\n",
            "  ...\n",
            "  [1.2042984  1.2030813  1.1410894  0.5059252 ]\n",
            "  [1.2042984  1.0980393  1.0539434  0.5059252 ]\n",
            "  [1.2042984  1.1505603  1.0539434  0.5059252 ]]], shape=(200, 200, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 23.64683 -15.33075   0.        1.            nan       nan       nan\n",
            "   0.            nan       nan       nan   0.            nan       nan\n",
            "       nan   0.            nan       nan       nan   0.            nan\n",
            "       nan       nan   0.            nan       nan       nan   0.     ], shape=(28,), dtype=float64)\n",
            "tf.Tensor(\n",
            "[[[ 0.05693981  0.18767507  0.49620926  0.        ]\n",
            "  [ 0.10831419  0.20518221  0.5136385   0.        ]\n",
            "  [ 0.1425637   0.24019621  0.54849684  0.        ]\n",
            "  ...\n",
            "  [ 0.29668647  0.36274523  0.5659261   0.        ]\n",
            "  [ 0.29668647  0.36274523  0.58335525  0.        ]\n",
            "  [ 0.26243696  0.32773122  0.54849684  0.        ]]\n",
            "\n",
            " [[ 0.05693981  0.17016807  0.49620926  0.        ]\n",
            "  [ 0.07406469  0.18767507  0.49620926  0.        ]\n",
            "  [ 0.07406469  0.18767507  0.5136385   0.        ]\n",
            "  ...\n",
            "  [ 0.31381124  0.3802522   0.58335525  0.        ]\n",
            "  [ 0.29668647  0.36274523  0.6007844   0.        ]\n",
            "  [ 0.27956173  0.3452382   0.5659261   0.        ]]\n",
            "\n",
            " [[-0.23418099 -0.10994396  0.09533771  1.0530337 ]\n",
            "  [-0.2684305  -0.14495796  0.09533771  1.052903  ]\n",
            "  [-0.40542853 -0.302521   -0.07895422  1.0530658 ]\n",
            "  ...\n",
            "  [ 0.31381124  0.3802522   0.6007844   0.        ]\n",
            "  [ 0.27956173  0.3452382   0.5659261   0.        ]\n",
            "  [ 0.27956173  0.3452382   0.54849684  0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.0844251   1.0455183   1.106231    0.51560336]\n",
            "  [ 1.1700488   1.1505603   1.1585186   0.51560336]\n",
            "  [ 1.1357994   1.0805323   1.0888019   0.51560336]\n",
            "  ...\n",
            "  [ 0.81042904  0.74789923  0.75764716  0.51559544]\n",
            "  [ 0.7933043   0.76540625  0.6879304   0.51559544]\n",
            "  [ 0.8275538   0.76540625  0.7227888   0.51559544]]\n",
            "\n",
            " [[ 1.0844251   1.0455183   1.0539434   0.51115835]\n",
            "  [ 1.0501755   1.0280113   1.0539434   0.51115835]\n",
            "  [ 1.1015499   1.0805323   1.106231    0.51115835]\n",
            "  ...\n",
            "  [ 0.8446786   0.76540625  0.7227888   0.51115835]\n",
            "  [ 0.8618033   0.76540625  0.740218    0.51115835]\n",
            "  [ 0.8618033   0.78291327  0.7053596   0.51115835]]\n",
            "\n",
            " [[ 1.0330509   1.0105042   1.0713726   0.5067641 ]\n",
            "  [ 1.1015499   1.0805323   1.1236602   0.5067641 ]\n",
            "  [ 1.1357994   1.0980393   1.106231    0.5067641 ]\n",
            "  ...\n",
            "  [ 0.81042904  0.7303923   0.67050123  0.5067641 ]\n",
            "  [ 0.9474271   0.83543426  0.79250556  0.5067641 ]\n",
            "  [ 0.89605284  0.81792724  0.7053596   0.5067641 ]]], shape=(200, 200, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 22.87116 -15.45111   0.        1.            nan       nan       nan\n",
            "   0.            nan       nan       nan   0.            nan       nan\n",
            "       nan   0.            nan       nan       nan   0.            nan\n",
            "       nan       nan   0.            nan       nan       nan   0.     ], shape=(28,), dtype=float64)\n",
            "tf.Tensor(\n",
            "[[[0.07406469 0.18767507 0.49620926 0.        ]\n",
            "  [0.12543894 0.24019621 0.5136385  0.        ]\n",
            "  [0.1425637  0.24019621 0.5136385  0.        ]\n",
            "  ...\n",
            "  [0.27956173 0.3452382  0.54849684 0.        ]\n",
            "  [0.26243696 0.32773122 0.54849684 0.        ]\n",
            "  [0.24531221 0.3102242  0.53106767 0.        ]]\n",
            "\n",
            " [[0.03981505 0.15266107 0.4613509  0.        ]\n",
            "  [0.09118944 0.20518221 0.5136385  0.        ]\n",
            "  [0.10831419 0.22268921 0.54849684 0.        ]\n",
            "  ...\n",
            "  [0.26243696 0.32773122 0.54849684 0.        ]\n",
            "  [0.26243696 0.32773122 0.5659261  0.        ]\n",
            "  [0.24531221 0.3102242  0.53106767 0.        ]]\n",
            "\n",
            " [[0.03981505 0.15266107 0.4613509  0.        ]\n",
            "  [0.07406469 0.18767507 0.49620926 0.        ]\n",
            "  [0.07406469 0.18767507 0.49620926 0.        ]\n",
            "  ...\n",
            "  [0.27956173 0.3452382  0.58335525 0.        ]\n",
            "  [0.26243696 0.32773122 0.5659261  0.        ]\n",
            "  [0.24531221 0.3102242  0.5136385  0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1.1700488  1.1155463  1.019085   0.51482874]\n",
            "  [1.1529241  1.1155463  1.019085   0.51482874]\n",
            "  [1.1015499  1.0280113  0.9667975  0.51482874]\n",
            "  ...\n",
            "  [1.1015499  1.0105042  1.0016558  0.51482874]\n",
            "  [1.1529241  1.1330533  1.1585186  0.51482874]\n",
            "  [1.0844251  1.0455183  1.0365143  0.51482874]]\n",
            "\n",
            " [[1.1529241  1.0805323  1.0539434  0.5103918 ]\n",
            "  [1.0844251  1.0280113  0.9667975  0.5103918 ]\n",
            "  [1.0844251  0.9754903  0.9493683  0.5103918 ]\n",
            "  ...\n",
            "  [1.1186746  1.0455183  1.019085   0.5103918 ]\n",
            "  [1.1186746  1.0805323  1.0888019  0.5103918 ]\n",
            "  [1.1529241  1.0980393  1.0888019  0.5103918 ]]\n",
            "\n",
            " [[1.0844251  0.9929973  0.9667975  0.5060059 ]\n",
            "  [1.0673003  0.9754903  0.9319391  0.5060059 ]\n",
            "  [1.0844251  0.9929973  0.9842267  0.5060059 ]\n",
            "  ...\n",
            "  [1.1186746  1.0630252  1.019085   0.5060059 ]\n",
            "  [1.1700488  1.0980393  1.0888019  0.5060059 ]\n",
            "  [1.1186746  1.0280113  1.019085   0.5060059 ]]], shape=(200, 200, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[-13.76199   9.82101   1.        1.       14.13008 -18.36108   0.\n",
            "   1.            nan       nan       nan   0.            nan       nan\n",
            "       nan   0.            nan       nan       nan   0.            nan\n",
            "       nan       nan   0.            nan       nan       nan   0.     ], shape=(28,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_dm7vkJiZ8F"
      },
      "source": [
        "# Load resnet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPpJ3ttdiDIB"
      },
      "source": [
        "#identity_block\n",
        "\n",
        "def identity_block(X, f, filters, stage, block, time):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 3\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch_' + f'{time}'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch_' + f'{time}'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "#convolutional_block\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s),padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(F2, (f, f), strides = (1,1), padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(F3, (1, 1), strides = (1,1), padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "    ##### SHORTCUT PATH ####\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def ResNet50(time, input_shape = (32, 32, 3), classes = 14):\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    # X_input = Input(input_shape)\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(input_shape)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(32, (7, 7), strides = (1, 1), name = f'conv_{time}', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = f'bn_conv_{time}')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [32, 32, 128], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='b', time = time)\n",
        "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='c', time = time)\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='b', time = time)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='c', time = time)\n",
        "    # X = identity_block(X, 3, [64, 64, 256], stage=3, block='d', time = time)\n",
        "\n",
        "    # Stage 4 \n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='b', time = time)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='c', time = time)\n",
        "    # X = identity_block(X, 3, [128, 128, 512], stage=4, block='d', time = time)\n",
        "    # X = identity_block(X, 3, [128, 128, 512], stage=4, block='e', time = time)\n",
        "    # X = identity_block(X, 3, [128, 128, 512], stage=4, block='f', time = time)\n",
        "\n",
        "    # Stage 5 \n",
        "    X = convolutional_block(X, f = 3, filters = [256,256, 1024], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='b', time = time)\n",
        "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='c', time = time)\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D(pool_size=(2,2), name=f'avg_pool_{time}')(X)\n",
        "    \n",
        "\n",
        "    # output layer\n",
        "    X = Dense(512)(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "    X = Dense(128)(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "    X = Dense(64)(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "    X = Dense(32)(X) # shape = (none, 12, 17, 32)\n",
        "    # X = Flatten()(X)\n",
        "    # tf.print('dense layer from resnet50 = \\n',X)\n",
        "    # X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # # Create model\n",
        "    # model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy-pQa45ifUu"
      },
      "source": [
        "# Load ViT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdhw6dFDicsF"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, ZeroPadding2D, AveragePooling2D, Flatten\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "# TODO: add the config layer to each class, so they can be saved\n",
        "# --> done\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "# Problem when loading the model: __init__() missing 1 required positional argument projection_dim\n",
        "#--------------------------------------------------------\n",
        "#---------UNCOMMENT THIS FOR TRAINING--------------------\n",
        "#--------------------------------------------------------\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        # a = self.prj_dim\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_patches': self.num_patches,\n",
        "            'projection': self.projection,\n",
        "            'position_embedding': self.position_embedding,\n",
        "            'projection_dim': self.projection_dim   \n",
        "            })\n",
        "        return config\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "#---------------USE THIS FOR LOAD THE MODEL---------------------\n",
        "#---------------------------------------------------------------\n",
        "# class PatchEncoder(layers.Layer):\n",
        "#     def __init__(self, num_patches, **kwargs):\n",
        "#         super(PatchEncoder, self).__init__()\n",
        "#         self.num_patches = num_patches\n",
        "#         self.projection = layers.Dense(units=projection_dim)\n",
        "#         self.position_embedding = layers.Embedding(\n",
        "#             input_dim=num_patches, output_dim=projection_dim\n",
        "#         )\n",
        "\n",
        "#     def call(self, patch):\n",
        "#         positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "#         encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "#         return encoded\n",
        "\n",
        "#     def get_config(self):\n",
        "#         config = super().get_config().copy()\n",
        "#         config.update({\n",
        "#             'num_patches': self.num_patches,\n",
        "#             'projection': self.projection,\n",
        "#             'position_embedding': self.position_embedding \n",
        "#             })\n",
        "#         return config\n",
        "\n",
        "\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size,**kwargs):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        images = tf.image.resize(images,size=(image_size,image_size))\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patch_size': self.patch_size,              \n",
        "            })\n",
        "        return config\n",
        "\n",
        "def ViT_feature(input, i):\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(input)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    \n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        # append_inputs.append(input)\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.2\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.2)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6,name=f'feature_layer_{i}')(encoded_patches) \n",
        "    # tf.print('representation = \\n', representation)\n",
        "    # print(representation.shape) \n",
        "    return representation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvkrr7GiijLl"
      },
      "source": [
        "# Set ViT hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1hasBN-ihO8"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 12  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813iImfUiq5u"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9zHr9eil3o"
      },
      "source": [
        "def create_resnet_on_4_sides():\n",
        "    # shape = (None, 4, 600, 800, 4)\n",
        "    inputs = layers.Input(shape=IMG_SHAPE)\n",
        "\n",
        "    branch_outputs = []\n",
        "    \n",
        "    for i in range(0,4):\n",
        "        \n",
        "\n",
        "        # Set the clear session, keras starts a blank state at every iteration and memory consumption is constant over time\n",
        "        # tf.keras.backend.clear_session()\n",
        "\n",
        "        resnet_feature = ResNet50(input_shape=inputs[:,i,:,:,:],time=i)\n",
        "        to_vit = ViT_feature(resnet_feature, i) \n",
        "        branch_outputs.append(to_vit)\n",
        "    \n",
        "\n",
        "    # expand the dim to (none, 6336,1) just for testing\n",
        "    # branch_out_expand = []\n",
        "    # for branch_out in branch_outputs:\n",
        "    #     branch_out = tf.expand_dims(branch_out,axis=-1)\n",
        "    #     # branch_out = tf.transpose(branch_out,perm=[0,2,1])\n",
        "    #     branch_out_expand.append(branch_out)\n",
        "\n",
        "    stack_output = Concatenate()(branch_outputs) # shape = (None, 4, 256) which is 4 patches and 256 features\n",
        "    \n",
        "    # stack_output = Add()(branch_outputs)\n",
        "    # stack_input = Concatenate()(branch_input)\n",
        "    # # print(stack_output.shape)\n",
        "    # dropout_output = layers.Dropout(0.5)(stack_output)\n",
        "    stack_output = layers.Flatten()(stack_output)\n",
        "    # K.print_tensor(stack_output, message='stack_output = ')\n",
        "    stack_output = layers.Dense(512)(stack_output)\n",
        "    stack_output = layers.Dropout(0.2)(stack_output)\n",
        "    stack_output = layers.Dense(128)(stack_output)\n",
        "    stack_output = layers.Dense(64)(stack_output)\n",
        "    stack_output = layers.Dense(OUTPUT)(stack_output)\n",
        "    # stack_output = layers.Reshape(target_shape=(7,4))(stack_output)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=stack_output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i_FXRU5jKEH"
      },
      "source": [
        "vit_resnet_backbone_model = create_resnet_on_4_sides()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GyJhADMcIGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4448d875-25cc-46cd-8f50-ef99dad5e767"
      },
      "source": [
        "vit_resnet_backbone_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 4, 200, 200, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_4 (Sli (None, 200, 200, 4)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_5 (Sli (None, 200, 200, 4)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_6 (Sli (None, 200, 200, 4)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_7 (Sli (None, 200, 200, 4)  0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_4 (ZeroPadding2D (None, 206, 206, 4)  0           tf.__operators__.getitem_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 206, 206, 4)  0           tf.__operators__.getitem_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_6 (ZeroPadding2D (None, 206, 206, 4)  0           tf.__operators__.getitem_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPadding2D (None, 206, 206, 4)  0           tf.__operators__.getitem_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv_0 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_0 (BatchNormalization)  (None, 200, 200, 32) 128         conv_0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_1 (BatchNormalization)  (None, 200, 200, 32) 128         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_2 (BatchNormalization)  (None, 200, 200, 32) 128         conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_3 (BatchNormalization)  (None, 200, 200, 32) 128         conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 200, 200, 32) 0           bn_conv_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 200, 200, 32) 0           bn_conv_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_222 (Activation)     (None, 200, 200, 32) 0           bn_conv_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_259 (Activation)     (None, 200, 200, 32) 0           bn_conv_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 66, 66, 32)   0           activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 66, 66, 32)   0           activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 66, 66, 32)   0           activation_222[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 66, 66, 32)   0           activation_259[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 66, 66, 32)   1056        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 66, 66, 32)   1056        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 66, 66, 32)   1056        max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 66, 66, 32)   1056        max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 66, 66, 32)   128         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 66, 66, 32)   128         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 66, 66, 32)   128         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 66, 66, 32)   128         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 66, 66, 32)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 66, 66, 32)   0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_223 (Activation)     (None, 66, 66, 32)   0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_260 (Activation)     (None, 66, 66, 32)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 66, 66, 32)   9248        activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 66, 66, 32)   9248        activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 66, 66, 32)   9248        activation_223[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 66, 66, 32)   9248        activation_260[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 66, 66, 32)   128         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 66, 66, 32)   128         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 66, 66, 32)   128         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 66, 66, 32)   128         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 66, 66, 32)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 66, 66, 32)   0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_224 (Activation)     (None, 66, 66, 32)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_261 (Activation)     (None, 66, 66, 32)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 66, 66, 128)  4224        activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 66, 66, 128)  4224        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 66, 66, 128)  4224        activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 66, 66, 128)  4224        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 66, 66, 128)  4224        activation_224[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 66, 66, 128)  4224        max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 66, 66, 128)  4224        activation_261[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 66, 66, 128)  4224        max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 66, 66, 128)  512         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 66, 66, 128)  512         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 66, 66, 128)  512         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 66, 66, 128)  512         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 66, 66, 128)  512         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 66, 66, 128)  512         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 66, 66, 128)  512         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 66, 66, 128)  512         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_112 (Add)                   (None, 66, 66, 128)  0           batch_normalization_66[0][0]     \n",
            "                                                                 batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_140 (Add)                   (None, 66, 66, 128)  0           batch_normalization_82[0][0]     \n",
            "                                                                 batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_168 (Add)                   (None, 66, 66, 128)  0           batch_normalization_98[0][0]     \n",
            "                                                                 batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_196 (Add)                   (None, 66, 66, 128)  0           batch_normalization_114[0][0]    \n",
            "                                                                 batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 66, 66, 128)  0           add_112[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 66, 66, 128)  0           add_140[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_225 (Activation)     (None, 66, 66, 128)  0           add_168[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_262 (Activation)     (None, 66, 66, 128)  0           add_196[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_02a (Conv2D)       (None, 66, 66, 32)   4128        activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_12a (Conv2D)       (None, 66, 66, 32)   4128        activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_22a (Conv2D)       (None, 66, 66, 32)   4128        activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_32a (Conv2D)       (None, 66, 66, 32)   4128        activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_02a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_12a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_22a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_32a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_226 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_263 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_02b (Conv2D)       (None, 66, 66, 32)   9248        activation_152[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_12b (Conv2D)       (None, 66, 66, 32)   9248        activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_22b (Conv2D)       (None, 66, 66, 32)   9248        activation_226[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_32b (Conv2D)       (None, 66, 66, 32)   9248        activation_263[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_02b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_12b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_22b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_32b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_190 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_227 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_264 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_02c (Conv2D)       (None, 66, 66, 128)  4224        activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_12c (Conv2D)       (None, 66, 66, 128)  4224        activation_190[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_22c (Conv2D)       (None, 66, 66, 128)  4224        activation_227[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_32c (Conv2D)       (None, 66, 66, 128)  4224        activation_264[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_02c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_12c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_22c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_32c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_113 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_02c[0][0]            \n",
            "                                                                 activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_141 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_12c[0][0]            \n",
            "                                                                 activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_169 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_22c[0][0]            \n",
            "                                                                 activation_225[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_197 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_32c[0][0]            \n",
            "                                                                 activation_262[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 66, 66, 128)  0           add_113[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_191 (Activation)     (None, 66, 66, 128)  0           add_141[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_228 (Activation)     (None, 66, 66, 128)  0           add_169[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_265 (Activation)     (None, 66, 66, 128)  0           add_197[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_02a (Conv2D)       (None, 66, 66, 32)   4128        activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_12a (Conv2D)       (None, 66, 66, 32)   4128        activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_22a (Conv2D)       (None, 66, 66, 32)   4128        activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_32a (Conv2D)       (None, 66, 66, 32)   4128        activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_02a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_12a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_22a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_32a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_192 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_229 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_266 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_02b (Conv2D)       (None, 66, 66, 32)   9248        activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_12b (Conv2D)       (None, 66, 66, 32)   9248        activation_192[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_22b (Conv2D)       (None, 66, 66, 32)   9248        activation_229[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_32b (Conv2D)       (None, 66, 66, 32)   9248        activation_266[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_02b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_12b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_22b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_32b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_193 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_230 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_267 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_02c (Conv2D)       (None, 66, 66, 128)  4224        activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_12c (Conv2D)       (None, 66, 66, 128)  4224        activation_193[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_22c (Conv2D)       (None, 66, 66, 128)  4224        activation_230[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_32c (Conv2D)       (None, 66, 66, 128)  4224        activation_267[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_02c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_12c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_22c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_32c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_114 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_02c[0][0]            \n",
            "                                                                 activation_154[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_142 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_12c[0][0]            \n",
            "                                                                 activation_191[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_170 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_22c[0][0]            \n",
            "                                                                 activation_228[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_198 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_32c[0][0]            \n",
            "                                                                 activation_265[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 66, 66, 128)  0           add_114[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_194 (Activation)     (None, 66, 66, 128)  0           add_142[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_231 (Activation)     (None, 66, 66, 128)  0           add_170[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_268 (Activation)     (None, 66, 66, 128)  0           add_198[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 33, 33, 64)   8256        activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 33, 33, 64)   8256        activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 33, 33, 64)   8256        activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 33, 33, 64)   8256        activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 33, 33, 64)   256         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 33, 33, 64)   256         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 33, 33, 64)   256         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 33, 33, 64)   256         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 33, 33, 64)   0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_195 (Activation)     (None, 33, 33, 64)   0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_232 (Activation)     (None, 33, 33, 64)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_269 (Activation)     (None, 33, 33, 64)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 33, 33, 64)   36928       activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 33, 33, 64)   36928       activation_195[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 33, 33, 64)   36928       activation_232[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 33, 33, 64)   36928       activation_269[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 33, 33, 64)   256         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 33, 33, 64)   256         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 33, 33, 64)   256         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 33, 33, 64)   256         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 33, 33, 64)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_196 (Activation)     (None, 33, 33, 64)   0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_233 (Activation)     (None, 33, 33, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_270 (Activation)     (None, 33, 33, 64)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 33, 33, 256)  16640       activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 33, 33, 256)  33024       activation_157[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 33, 33, 256)  16640       activation_196[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 33, 33, 256)  33024       activation_194[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 33, 33, 256)  16640       activation_233[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 33, 33, 256)  33024       activation_231[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 33, 33, 256)  16640       activation_270[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 33, 33, 256)  33024       activation_268[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 33, 33, 256)  1024        conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 33, 33, 256)  1024        conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 33, 33, 256)  1024        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 33, 33, 256)  1024        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 33, 33, 256)  1024        conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 33, 33, 256)  1024        conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 33, 33, 256)  1024        conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 33, 33, 256)  1024        conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_115 (Add)                   (None, 33, 33, 256)  0           batch_normalization_70[0][0]     \n",
            "                                                                 batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_143 (Add)                   (None, 33, 33, 256)  0           batch_normalization_86[0][0]     \n",
            "                                                                 batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_171 (Add)                   (None, 33, 33, 256)  0           batch_normalization_102[0][0]    \n",
            "                                                                 batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_199 (Add)                   (None, 33, 33, 256)  0           batch_normalization_118[0][0]    \n",
            "                                                                 batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 33, 33, 256)  0           add_115[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_197 (Activation)     (None, 33, 33, 256)  0           add_143[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_234 (Activation)     (None, 33, 33, 256)  0           add_171[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_271 (Activation)     (None, 33, 33, 256)  0           add_199[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_02a (Conv2D)       (None, 33, 33, 64)   16448       activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_12a (Conv2D)       (None, 33, 33, 64)   16448       activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_22a (Conv2D)       (None, 33, 33, 64)   16448       activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_32a (Conv2D)       (None, 33, 33, 64)   16448       activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_02a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_12a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_22a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_32a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_198 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_235 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_272 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_02b (Conv2D)       (None, 33, 33, 64)   36928       activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_12b (Conv2D)       (None, 33, 33, 64)   36928       activation_198[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_22b (Conv2D)       (None, 33, 33, 64)   36928       activation_235[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_32b (Conv2D)       (None, 33, 33, 64)   36928       activation_272[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_02b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_12b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_22b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_32b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_199 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_236 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_273 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_02c (Conv2D)       (None, 33, 33, 256)  16640       activation_162[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_12c (Conv2D)       (None, 33, 33, 256)  16640       activation_199[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_22c (Conv2D)       (None, 33, 33, 256)  16640       activation_236[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_32c (Conv2D)       (None, 33, 33, 256)  16640       activation_273[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_02c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_12c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_22c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_32c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_116 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_02c[0][0]            \n",
            "                                                                 activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_144 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_12c[0][0]            \n",
            "                                                                 activation_197[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_172 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_22c[0][0]            \n",
            "                                                                 activation_234[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_200 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_32c[0][0]            \n",
            "                                                                 activation_271[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 33, 33, 256)  0           add_116[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_200 (Activation)     (None, 33, 33, 256)  0           add_144[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_237 (Activation)     (None, 33, 33, 256)  0           add_172[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_274 (Activation)     (None, 33, 33, 256)  0           add_200[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_02a (Conv2D)       (None, 33, 33, 64)   16448       activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_12a (Conv2D)       (None, 33, 33, 64)   16448       activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_22a (Conv2D)       (None, 33, 33, 64)   16448       activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_32a (Conv2D)       (None, 33, 33, 64)   16448       activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_02a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_12a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_22a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_32a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_201 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_238 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_275 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_02b (Conv2D)       (None, 33, 33, 64)   36928       activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_12b (Conv2D)       (None, 33, 33, 64)   36928       activation_201[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_22b (Conv2D)       (None, 33, 33, 64)   36928       activation_238[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_32b (Conv2D)       (None, 33, 33, 64)   36928       activation_275[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_02b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_12b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_22b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_32b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_202 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_239 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_276 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_02c (Conv2D)       (None, 33, 33, 256)  16640       activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_12c (Conv2D)       (None, 33, 33, 256)  16640       activation_202[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_22c (Conv2D)       (None, 33, 33, 256)  16640       activation_239[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_32c (Conv2D)       (None, 33, 33, 256)  16640       activation_276[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_02c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_12c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_22c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_32c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_117 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_02c[0][0]            \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_145 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_12c[0][0]            \n",
            "                                                                 activation_200[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_173 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_22c[0][0]            \n",
            "                                                                 activation_237[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_201 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_32c[0][0]            \n",
            "                                                                 activation_274[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 33, 33, 256)  0           add_117[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_203 (Activation)     (None, 33, 33, 256)  0           add_145[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_240 (Activation)     (None, 33, 33, 256)  0           add_173[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_277 (Activation)     (None, 33, 33, 256)  0           add_201[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 17, 17, 128)  32896       activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 17, 17, 128)  32896       activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 17, 17, 128)  32896       activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 17, 17, 128)  32896       activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 17, 17, 128)  512         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 17, 17, 128)  512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 17, 17, 128)  512         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 17, 17, 128)  512         conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 17, 17, 128)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_204 (Activation)     (None, 17, 17, 128)  0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_241 (Activation)     (None, 17, 17, 128)  0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_278 (Activation)     (None, 17, 17, 128)  0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 17, 17, 128)  147584      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 17, 17, 128)  147584      activation_204[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 17, 17, 128)  147584      activation_241[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 17, 17, 128)  147584      activation_278[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 17, 17, 128)  512         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 17, 17, 128)  512         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 17, 17, 128)  512         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 17, 17, 128)  512         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 17, 17, 128)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_205 (Activation)     (None, 17, 17, 128)  0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_242 (Activation)     (None, 17, 17, 128)  0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_279 (Activation)     (None, 17, 17, 128)  0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 17, 17, 512)  66048       activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 17, 17, 512)  131584      activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 17, 17, 512)  66048       activation_205[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 17, 17, 512)  131584      activation_203[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 17, 17, 512)  66048       activation_242[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 17, 17, 512)  131584      activation_240[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 17, 17, 512)  66048       activation_279[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 17, 17, 512)  131584      activation_277[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 17, 17, 512)  2048        conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 17, 17, 512)  2048        conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 17, 17, 512)  2048        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 17, 17, 512)  2048        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 17, 17, 512)  2048        conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 17, 17, 512)  2048        conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 17, 17, 512)  2048        conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 17, 17, 512)  2048        conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_118 (Add)                   (None, 17, 17, 512)  0           batch_normalization_74[0][0]     \n",
            "                                                                 batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_146 (Add)                   (None, 17, 17, 512)  0           batch_normalization_90[0][0]     \n",
            "                                                                 batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_174 (Add)                   (None, 17, 17, 512)  0           batch_normalization_106[0][0]    \n",
            "                                                                 batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_202 (Add)                   (None, 17, 17, 512)  0           batch_normalization_122[0][0]    \n",
            "                                                                 batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 17, 17, 512)  0           add_118[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_206 (Activation)     (None, 17, 17, 512)  0           add_146[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_243 (Activation)     (None, 17, 17, 512)  0           add_174[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_280 (Activation)     (None, 17, 17, 512)  0           add_202[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_02a (Conv2D)       (None, 17, 17, 128)  65664       activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_12a (Conv2D)       (None, 17, 17, 128)  65664       activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_22a (Conv2D)       (None, 17, 17, 128)  65664       activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_32a (Conv2D)       (None, 17, 17, 128)  65664       activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_02a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_12a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_22a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_32a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_207 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_244 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_281 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_02b (Conv2D)       (None, 17, 17, 128)  147584      activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_12b (Conv2D)       (None, 17, 17, 128)  147584      activation_207[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_22b (Conv2D)       (None, 17, 17, 128)  147584      activation_244[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_32b (Conv2D)       (None, 17, 17, 128)  147584      activation_281[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_02b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_12b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_22b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_32b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_208 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_245 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_282 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_02c (Conv2D)       (None, 17, 17, 512)  66048       activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_12c (Conv2D)       (None, 17, 17, 512)  66048       activation_208[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_22c (Conv2D)       (None, 17, 17, 512)  66048       activation_245[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_32c (Conv2D)       (None, 17, 17, 512)  66048       activation_282[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_02c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_12c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_22c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_32c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_119 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_02c[0][0]            \n",
            "                                                                 activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_147 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_12c[0][0]            \n",
            "                                                                 activation_206[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_175 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_22c[0][0]            \n",
            "                                                                 activation_243[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_203 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_32c[0][0]            \n",
            "                                                                 activation_280[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 17, 17, 512)  0           add_119[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_209 (Activation)     (None, 17, 17, 512)  0           add_147[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_246 (Activation)     (None, 17, 17, 512)  0           add_175[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_283 (Activation)     (None, 17, 17, 512)  0           add_203[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_02a (Conv2D)       (None, 17, 17, 128)  65664       activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_12a (Conv2D)       (None, 17, 17, 128)  65664       activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_22a (Conv2D)       (None, 17, 17, 128)  65664       activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_32a (Conv2D)       (None, 17, 17, 128)  65664       activation_283[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_02a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_12a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_22a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_32a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_210 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_247 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_284 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_02b (Conv2D)       (None, 17, 17, 128)  147584      activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_12b (Conv2D)       (None, 17, 17, 128)  147584      activation_210[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_22b (Conv2D)       (None, 17, 17, 128)  147584      activation_247[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_32b (Conv2D)       (None, 17, 17, 128)  147584      activation_284[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_02b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_12b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_22b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_32b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_211 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_248 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_285 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_02c (Conv2D)       (None, 17, 17, 512)  66048       activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_12c (Conv2D)       (None, 17, 17, 512)  66048       activation_211[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_22c (Conv2D)       (None, 17, 17, 512)  66048       activation_248[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_32c (Conv2D)       (None, 17, 17, 512)  66048       activation_285[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_02c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_12c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_22c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_32c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_120 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_02c[0][0]            \n",
            "                                                                 activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_148 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_12c[0][0]            \n",
            "                                                                 activation_209[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_176 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_22c[0][0]            \n",
            "                                                                 activation_246[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_204 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_32c[0][0]            \n",
            "                                                                 activation_283[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 17, 17, 512)  0           add_120[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_212 (Activation)     (None, 17, 17, 512)  0           add_148[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_249 (Activation)     (None, 17, 17, 512)  0           add_176[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_286 (Activation)     (None, 17, 17, 512)  0           add_204[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 9, 9, 256)    131328      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 9, 9, 256)    131328      activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 9, 9, 256)    131328      activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 9, 9, 256)    131328      activation_286[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 9, 9, 256)    1024        conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 9, 9, 256)    1024        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 9, 9, 256)    1024        conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 9, 9, 256)    1024        conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 9, 9, 256)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_213 (Activation)     (None, 9, 9, 256)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 9, 9, 256)    0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_287 (Activation)     (None, 9, 9, 256)    0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 9, 9, 256)    590080      activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 9, 9, 256)    590080      activation_213[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 9, 9, 256)    590080      activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 9, 9, 256)    590080      activation_287[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 9, 9, 256)    1024        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 9, 9, 256)    1024        conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 9, 9, 256)    1024        conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 9, 9, 256)    1024        conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 9, 9, 256)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_214 (Activation)     (None, 9, 9, 256)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 9, 9, 256)    0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_288 (Activation)     (None, 9, 9, 256)    0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 9, 9, 1024)   263168      activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 9, 9, 1024)   525312      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 9, 9, 1024)   263168      activation_214[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 9, 9, 1024)   525312      activation_212[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 9, 9, 1024)   263168      activation_251[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 9, 9, 1024)   525312      activation_249[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 9, 9, 1024)   263168      activation_288[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 9, 9, 1024)   525312      activation_286[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 9, 9, 1024)   4096        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 9, 9, 1024)   4096        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 9, 9, 1024)   4096        conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 9, 9, 1024)   4096        conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 9, 9, 1024)   4096        conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 9, 9, 1024)   4096        conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 9, 9, 1024)   4096        conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 9, 9, 1024)   4096        conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_121 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_78[0][0]     \n",
            "                                                                 batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_149 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_94[0][0]     \n",
            "                                                                 batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_177 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_110[0][0]    \n",
            "                                                                 batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_205 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_126[0][0]    \n",
            "                                                                 batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 9, 9, 1024)   0           add_121[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_215 (Activation)     (None, 9, 9, 1024)   0           add_149[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 9, 9, 1024)   0           add_177[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_289 (Activation)     (None, 9, 9, 1024)   0           add_205[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_02a (Conv2D)       (None, 9, 9, 256)    262400      activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_12a (Conv2D)       (None, 9, 9, 256)    262400      activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_22a (Conv2D)       (None, 9, 9, 256)    262400      activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_32a (Conv2D)       (None, 9, 9, 256)    262400      activation_289[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_02a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_12a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_22a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_32a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_216 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_290 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_02b (Conv2D)       (None, 9, 9, 256)    590080      activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_12b (Conv2D)       (None, 9, 9, 256)    590080      activation_216[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_22b (Conv2D)       (None, 9, 9, 256)    590080      activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_32b (Conv2D)       (None, 9, 9, 256)    590080      activation_290[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_02b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_12b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_22b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_32b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_217 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_291 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_02c (Conv2D)       (None, 9, 9, 1024)   263168      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_12c (Conv2D)       (None, 9, 9, 1024)   263168      activation_217[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_22c (Conv2D)       (None, 9, 9, 1024)   263168      activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_32c (Conv2D)       (None, 9, 9, 1024)   263168      activation_291[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_02c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_12c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_22c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_32c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_122 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_02c[0][0]            \n",
            "                                                                 activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_150 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_12c[0][0]            \n",
            "                                                                 activation_215[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_178 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_22c[0][0]            \n",
            "                                                                 activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_206 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_32c[0][0]            \n",
            "                                                                 activation_289[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 9, 9, 1024)   0           add_122[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_218 (Activation)     (None, 9, 9, 1024)   0           add_150[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 9, 9, 1024)   0           add_178[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_292 (Activation)     (None, 9, 9, 1024)   0           add_206[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_02a (Conv2D)       (None, 9, 9, 256)    262400      activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_12a (Conv2D)       (None, 9, 9, 256)    262400      activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_22a (Conv2D)       (None, 9, 9, 256)    262400      activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_32a (Conv2D)       (None, 9, 9, 256)    262400      activation_292[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_02a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_12a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_22a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_32a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_219 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_293 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_02b (Conv2D)       (None, 9, 9, 256)    590080      activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_12b (Conv2D)       (None, 9, 9, 256)    590080      activation_219[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_22b (Conv2D)       (None, 9, 9, 256)    590080      activation_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_32b (Conv2D)       (None, 9, 9, 256)    590080      activation_293[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_02b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_12b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_22b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_32b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_220 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_294 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_02c (Conv2D)       (None, 9, 9, 1024)   263168      activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_12c (Conv2D)       (None, 9, 9, 1024)   263168      activation_220[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_22c (Conv2D)       (None, 9, 9, 1024)   263168      activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_32c (Conv2D)       (None, 9, 9, 1024)   263168      activation_294[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_02c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_12c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_22c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_32c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_123 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_02c[0][0]            \n",
            "                                                                 activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_151 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_12c[0][0]            \n",
            "                                                                 activation_218[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_179 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_22c[0][0]            \n",
            "                                                                 activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_207 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_32c[0][0]            \n",
            "                                                                 activation_292[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 9, 9, 1024)   0           add_123[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_221 (Activation)     (None, 9, 9, 1024)   0           add_151[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_258 (Activation)     (None, 9, 9, 1024)   0           add_179[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_295 (Activation)     (None, 9, 9, 1024)   0           add_207[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_0 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_1 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_221[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_2 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_258[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_3 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_295[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_88 (Dense)                (None, 4, 4, 512)    524800      avg_pool_0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_109 (Dense)               (None, 4, 4, 512)    524800      avg_pool_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_130 (Dense)               (None, 4, 4, 512)    524800      avg_pool_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_151 (Dense)               (None, 4, 4, 512)    524800      avg_pool_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_77 (Dropout)            (None, 4, 4, 512)    0           dense_88[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_96 (Dropout)            (None, 4, 4, 512)    0           dense_109[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_115 (Dropout)           (None, 4, 4, 512)    0           dense_130[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 4, 4, 512)    0           dense_151[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_89 (Dense)                (None, 4, 4, 128)    65664       dropout_77[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_110 (Dense)               (None, 4, 4, 128)    65664       dropout_96[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_131 (Dense)               (None, 4, 4, 128)    65664       dropout_115[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_152 (Dense)               (None, 4, 4, 128)    65664       dropout_134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 4, 4, 128)    0           dense_89[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_97 (Dropout)            (None, 4, 4, 128)    0           dense_110[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_116 (Dropout)           (None, 4, 4, 128)    0           dense_131[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (None, 4, 4, 128)    0           dense_152[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_90 (Dense)                (None, 4, 4, 64)     8256        dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_111 (Dense)               (None, 4, 4, 64)     8256        dropout_97[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_132 (Dense)               (None, 4, 4, 64)     8256        dropout_116[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_153 (Dense)               (None, 4, 4, 64)     8256        dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 4, 4, 64)     0           dense_90[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_98 (Dropout)            (None, 4, 4, 64)     0           dense_111[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_117 (Dropout)           (None, 4, 4, 64)     0           dense_132[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (None, 4, 4, 64)     0           dense_153[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_91 (Dense)                (None, 4, 4, 32)     2080        dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_112 (Dense)               (None, 4, 4, 32)     2080        dropout_98[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_133 (Dense)               (None, 4, 4, 32)     2080        dropout_117[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_154 (Dense)               (None, 4, 4, 32)     2080        dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "patches_4 (Patches)             (None, None, 1152)   0           dense_91[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "patches_5 (Patches)             (None, None, 1152)   0           dense_112[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patches_6 (Patches)             (None, None, 1152)   0           dense_133[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patches_7 (Patches)             (None, None, 1152)   0           dense_154[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_4 (PatchEncoder)  (None, 4, 64)        74048       patches_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_5 (PatchEncoder)  (None, 4, 64)        74048       patches_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_6 (PatchEncoder)  (None, 4, 64)        74048       patches_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_7 (PatchEncoder)  (None, 4, 64)        74048       patches_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_64 (LayerNo (None, 4, 64)        128         patch_encoder_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_80 (LayerNo (None, 4, 64)        128         patch_encoder_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_96 (LayerNo (None, 4, 64)        128         patch_encoder_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_112 (LayerN (None, 4, 64)        128         patch_encoder_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_32 (MultiH (None, 4, 64)        66368       layer_normalization_64[0][0]     \n",
            "                                                                 layer_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_40 (MultiH (None, 4, 64)        66368       layer_normalization_80[0][0]     \n",
            "                                                                 layer_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_48 (MultiH (None, 4, 64)        66368       layer_normalization_96[0][0]     \n",
            "                                                                 layer_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_56 (MultiH (None, 4, 64)        66368       layer_normalization_112[0][0]    \n",
            "                                                                 layer_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_124 (Add)                   (None, 4, 64)        0           multi_head_attention_32[0][0]    \n",
            "                                                                 patch_encoder_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_152 (Add)                   (None, 4, 64)        0           multi_head_attention_40[0][0]    \n",
            "                                                                 patch_encoder_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_180 (Add)                   (None, 4, 64)        0           multi_head_attention_48[0][0]    \n",
            "                                                                 patch_encoder_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_208 (Add)                   (None, 4, 64)        0           multi_head_attention_56[0][0]    \n",
            "                                                                 patch_encoder_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_65 (LayerNo (None, 4, 64)        128         add_124[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_81 (LayerNo (None, 4, 64)        128         add_152[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_97 (LayerNo (None, 4, 64)        128         add_180[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_113 (LayerN (None, 4, 64)        128         add_208[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_93 (Dense)                (None, 4, 128)       8320        layer_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_114 (Dense)               (None, 4, 128)       8320        layer_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_135 (Dense)               (None, 4, 128)       8320        layer_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_156 (Dense)               (None, 4, 128)       8320        layer_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_80 (Dropout)            (None, 4, 128)       0           dense_93[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_99 (Dropout)            (None, 4, 128)       0           dense_114[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_118 (Dropout)           (None, 4, 128)       0           dense_135[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (None, 4, 128)       0           dense_156[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_94 (Dense)                (None, 4, 64)        8256        dropout_80[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_115 (Dense)               (None, 4, 64)        8256        dropout_99[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_136 (Dense)               (None, 4, 64)        8256        dropout_118[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_157 (Dense)               (None, 4, 64)        8256        dropout_137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_81 (Dropout)            (None, 4, 64)        0           dense_94[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_100 (Dropout)           (None, 4, 64)        0           dense_115[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_119 (Dropout)           (None, 4, 64)        0           dense_136[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (None, 4, 64)        0           dense_157[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_125 (Add)                   (None, 4, 64)        0           dropout_81[0][0]                 \n",
            "                                                                 add_124[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_153 (Add)                   (None, 4, 64)        0           dropout_100[0][0]                \n",
            "                                                                 add_152[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_181 (Add)                   (None, 4, 64)        0           dropout_119[0][0]                \n",
            "                                                                 add_180[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_209 (Add)                   (None, 4, 64)        0           dropout_138[0][0]                \n",
            "                                                                 add_208[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_66 (LayerNo (None, 4, 64)        128         add_125[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_82 (LayerNo (None, 4, 64)        128         add_153[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_98 (LayerNo (None, 4, 64)        128         add_181[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_114 (LayerN (None, 4, 64)        128         add_209[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_33 (MultiH (None, 4, 64)        66368       layer_normalization_66[0][0]     \n",
            "                                                                 layer_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_41 (MultiH (None, 4, 64)        66368       layer_normalization_82[0][0]     \n",
            "                                                                 layer_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_49 (MultiH (None, 4, 64)        66368       layer_normalization_98[0][0]     \n",
            "                                                                 layer_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_57 (MultiH (None, 4, 64)        66368       layer_normalization_114[0][0]    \n",
            "                                                                 layer_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_126 (Add)                   (None, 4, 64)        0           multi_head_attention_33[0][0]    \n",
            "                                                                 add_125[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_154 (Add)                   (None, 4, 64)        0           multi_head_attention_41[0][0]    \n",
            "                                                                 add_153[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_182 (Add)                   (None, 4, 64)        0           multi_head_attention_49[0][0]    \n",
            "                                                                 add_181[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_210 (Add)                   (None, 4, 64)        0           multi_head_attention_57[0][0]    \n",
            "                                                                 add_209[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_67 (LayerNo (None, 4, 64)        128         add_126[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_83 (LayerNo (None, 4, 64)        128         add_154[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_99 (LayerNo (None, 4, 64)        128         add_182[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_115 (LayerN (None, 4, 64)        128         add_210[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_95 (Dense)                (None, 4, 128)       8320        layer_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_116 (Dense)               (None, 4, 128)       8320        layer_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_137 (Dense)               (None, 4, 128)       8320        layer_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_158 (Dense)               (None, 4, 128)       8320        layer_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_82 (Dropout)            (None, 4, 128)       0           dense_95[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_101 (Dropout)           (None, 4, 128)       0           dense_116[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_120 (Dropout)           (None, 4, 128)       0           dense_137[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (None, 4, 128)       0           dense_158[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_96 (Dense)                (None, 4, 64)        8256        dropout_82[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_117 (Dense)               (None, 4, 64)        8256        dropout_101[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_138 (Dense)               (None, 4, 64)        8256        dropout_120[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_159 (Dense)               (None, 4, 64)        8256        dropout_139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_83 (Dropout)            (None, 4, 64)        0           dense_96[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_102 (Dropout)           (None, 4, 64)        0           dense_117[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_121 (Dropout)           (None, 4, 64)        0           dense_138[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_140 (Dropout)           (None, 4, 64)        0           dense_159[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_127 (Add)                   (None, 4, 64)        0           dropout_83[0][0]                 \n",
            "                                                                 add_126[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_155 (Add)                   (None, 4, 64)        0           dropout_102[0][0]                \n",
            "                                                                 add_154[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_183 (Add)                   (None, 4, 64)        0           dropout_121[0][0]                \n",
            "                                                                 add_182[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_211 (Add)                   (None, 4, 64)        0           dropout_140[0][0]                \n",
            "                                                                 add_210[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_68 (LayerNo (None, 4, 64)        128         add_127[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_84 (LayerNo (None, 4, 64)        128         add_155[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_100 (LayerN (None, 4, 64)        128         add_183[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_116 (LayerN (None, 4, 64)        128         add_211[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_34 (MultiH (None, 4, 64)        66368       layer_normalization_68[0][0]     \n",
            "                                                                 layer_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_42 (MultiH (None, 4, 64)        66368       layer_normalization_84[0][0]     \n",
            "                                                                 layer_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_50 (MultiH (None, 4, 64)        66368       layer_normalization_100[0][0]    \n",
            "                                                                 layer_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_58 (MultiH (None, 4, 64)        66368       layer_normalization_116[0][0]    \n",
            "                                                                 layer_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_128 (Add)                   (None, 4, 64)        0           multi_head_attention_34[0][0]    \n",
            "                                                                 add_127[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_156 (Add)                   (None, 4, 64)        0           multi_head_attention_42[0][0]    \n",
            "                                                                 add_155[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_184 (Add)                   (None, 4, 64)        0           multi_head_attention_50[0][0]    \n",
            "                                                                 add_183[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_212 (Add)                   (None, 4, 64)        0           multi_head_attention_58[0][0]    \n",
            "                                                                 add_211[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_69 (LayerNo (None, 4, 64)        128         add_128[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_85 (LayerNo (None, 4, 64)        128         add_156[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_101 (LayerN (None, 4, 64)        128         add_184[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_117 (LayerN (None, 4, 64)        128         add_212[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_97 (Dense)                (None, 4, 128)       8320        layer_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_118 (Dense)               (None, 4, 128)       8320        layer_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_139 (Dense)               (None, 4, 128)       8320        layer_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_160 (Dense)               (None, 4, 128)       8320        layer_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_84 (Dropout)            (None, 4, 128)       0           dense_97[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_103 (Dropout)           (None, 4, 128)       0           dense_118[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_122 (Dropout)           (None, 4, 128)       0           dense_139[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_141 (Dropout)           (None, 4, 128)       0           dense_160[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_98 (Dense)                (None, 4, 64)        8256        dropout_84[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_119 (Dense)               (None, 4, 64)        8256        dropout_103[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_140 (Dense)               (None, 4, 64)        8256        dropout_122[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_161 (Dense)               (None, 4, 64)        8256        dropout_141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_85 (Dropout)            (None, 4, 64)        0           dense_98[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_104 (Dropout)           (None, 4, 64)        0           dense_119[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_123 (Dropout)           (None, 4, 64)        0           dense_140[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_142 (Dropout)           (None, 4, 64)        0           dense_161[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_129 (Add)                   (None, 4, 64)        0           dropout_85[0][0]                 \n",
            "                                                                 add_128[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_157 (Add)                   (None, 4, 64)        0           dropout_104[0][0]                \n",
            "                                                                 add_156[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_185 (Add)                   (None, 4, 64)        0           dropout_123[0][0]                \n",
            "                                                                 add_184[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_213 (Add)                   (None, 4, 64)        0           dropout_142[0][0]                \n",
            "                                                                 add_212[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_70 (LayerNo (None, 4, 64)        128         add_129[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_86 (LayerNo (None, 4, 64)        128         add_157[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_102 (LayerN (None, 4, 64)        128         add_185[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_118 (LayerN (None, 4, 64)        128         add_213[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_35 (MultiH (None, 4, 64)        66368       layer_normalization_70[0][0]     \n",
            "                                                                 layer_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_43 (MultiH (None, 4, 64)        66368       layer_normalization_86[0][0]     \n",
            "                                                                 layer_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_51 (MultiH (None, 4, 64)        66368       layer_normalization_102[0][0]    \n",
            "                                                                 layer_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_59 (MultiH (None, 4, 64)        66368       layer_normalization_118[0][0]    \n",
            "                                                                 layer_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_130 (Add)                   (None, 4, 64)        0           multi_head_attention_35[0][0]    \n",
            "                                                                 add_129[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_158 (Add)                   (None, 4, 64)        0           multi_head_attention_43[0][0]    \n",
            "                                                                 add_157[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_186 (Add)                   (None, 4, 64)        0           multi_head_attention_51[0][0]    \n",
            "                                                                 add_185[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_214 (Add)                   (None, 4, 64)        0           multi_head_attention_59[0][0]    \n",
            "                                                                 add_213[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_71 (LayerNo (None, 4, 64)        128         add_130[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_87 (LayerNo (None, 4, 64)        128         add_158[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_103 (LayerN (None, 4, 64)        128         add_186[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_119 (LayerN (None, 4, 64)        128         add_214[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_99 (Dense)                (None, 4, 128)       8320        layer_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_120 (Dense)               (None, 4, 128)       8320        layer_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_141 (Dense)               (None, 4, 128)       8320        layer_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_162 (Dense)               (None, 4, 128)       8320        layer_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_86 (Dropout)            (None, 4, 128)       0           dense_99[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_105 (Dropout)           (None, 4, 128)       0           dense_120[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_124 (Dropout)           (None, 4, 128)       0           dense_141[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_143 (Dropout)           (None, 4, 128)       0           dense_162[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_100 (Dense)               (None, 4, 64)        8256        dropout_86[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_121 (Dense)               (None, 4, 64)        8256        dropout_105[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_142 (Dense)               (None, 4, 64)        8256        dropout_124[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_163 (Dense)               (None, 4, 64)        8256        dropout_143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_87 (Dropout)            (None, 4, 64)        0           dense_100[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_106 (Dropout)           (None, 4, 64)        0           dense_121[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_125 (Dropout)           (None, 4, 64)        0           dense_142[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_144 (Dropout)           (None, 4, 64)        0           dense_163[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_131 (Add)                   (None, 4, 64)        0           dropout_87[0][0]                 \n",
            "                                                                 add_130[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_159 (Add)                   (None, 4, 64)        0           dropout_106[0][0]                \n",
            "                                                                 add_158[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_187 (Add)                   (None, 4, 64)        0           dropout_125[0][0]                \n",
            "                                                                 add_186[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_215 (Add)                   (None, 4, 64)        0           dropout_144[0][0]                \n",
            "                                                                 add_214[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_72 (LayerNo (None, 4, 64)        128         add_131[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_88 (LayerNo (None, 4, 64)        128         add_159[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_104 (LayerN (None, 4, 64)        128         add_187[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_120 (LayerN (None, 4, 64)        128         add_215[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_36 (MultiH (None, 4, 64)        66368       layer_normalization_72[0][0]     \n",
            "                                                                 layer_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_44 (MultiH (None, 4, 64)        66368       layer_normalization_88[0][0]     \n",
            "                                                                 layer_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_52 (MultiH (None, 4, 64)        66368       layer_normalization_104[0][0]    \n",
            "                                                                 layer_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_60 (MultiH (None, 4, 64)        66368       layer_normalization_120[0][0]    \n",
            "                                                                 layer_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_132 (Add)                   (None, 4, 64)        0           multi_head_attention_36[0][0]    \n",
            "                                                                 add_131[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_160 (Add)                   (None, 4, 64)        0           multi_head_attention_44[0][0]    \n",
            "                                                                 add_159[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_188 (Add)                   (None, 4, 64)        0           multi_head_attention_52[0][0]    \n",
            "                                                                 add_187[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_216 (Add)                   (None, 4, 64)        0           multi_head_attention_60[0][0]    \n",
            "                                                                 add_215[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_73 (LayerNo (None, 4, 64)        128         add_132[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_89 (LayerNo (None, 4, 64)        128         add_160[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_105 (LayerN (None, 4, 64)        128         add_188[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_121 (LayerN (None, 4, 64)        128         add_216[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_101 (Dense)               (None, 4, 128)       8320        layer_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_122 (Dense)               (None, 4, 128)       8320        layer_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_143 (Dense)               (None, 4, 128)       8320        layer_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_164 (Dense)               (None, 4, 128)       8320        layer_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_88 (Dropout)            (None, 4, 128)       0           dense_101[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_107 (Dropout)           (None, 4, 128)       0           dense_122[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_126 (Dropout)           (None, 4, 128)       0           dense_143[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_145 (Dropout)           (None, 4, 128)       0           dense_164[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_102 (Dense)               (None, 4, 64)        8256        dropout_88[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_123 (Dense)               (None, 4, 64)        8256        dropout_107[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_144 (Dense)               (None, 4, 64)        8256        dropout_126[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_165 (Dense)               (None, 4, 64)        8256        dropout_145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_89 (Dropout)            (None, 4, 64)        0           dense_102[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_108 (Dropout)           (None, 4, 64)        0           dense_123[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_127 (Dropout)           (None, 4, 64)        0           dense_144[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_146 (Dropout)           (None, 4, 64)        0           dense_165[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_133 (Add)                   (None, 4, 64)        0           dropout_89[0][0]                 \n",
            "                                                                 add_132[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_161 (Add)                   (None, 4, 64)        0           dropout_108[0][0]                \n",
            "                                                                 add_160[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_189 (Add)                   (None, 4, 64)        0           dropout_127[0][0]                \n",
            "                                                                 add_188[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_217 (Add)                   (None, 4, 64)        0           dropout_146[0][0]                \n",
            "                                                                 add_216[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_74 (LayerNo (None, 4, 64)        128         add_133[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_90 (LayerNo (None, 4, 64)        128         add_161[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_106 (LayerN (None, 4, 64)        128         add_189[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_122 (LayerN (None, 4, 64)        128         add_217[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_37 (MultiH (None, 4, 64)        66368       layer_normalization_74[0][0]     \n",
            "                                                                 layer_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_45 (MultiH (None, 4, 64)        66368       layer_normalization_90[0][0]     \n",
            "                                                                 layer_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_53 (MultiH (None, 4, 64)        66368       layer_normalization_106[0][0]    \n",
            "                                                                 layer_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_61 (MultiH (None, 4, 64)        66368       layer_normalization_122[0][0]    \n",
            "                                                                 layer_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_134 (Add)                   (None, 4, 64)        0           multi_head_attention_37[0][0]    \n",
            "                                                                 add_133[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_162 (Add)                   (None, 4, 64)        0           multi_head_attention_45[0][0]    \n",
            "                                                                 add_161[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_190 (Add)                   (None, 4, 64)        0           multi_head_attention_53[0][0]    \n",
            "                                                                 add_189[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_218 (Add)                   (None, 4, 64)        0           multi_head_attention_61[0][0]    \n",
            "                                                                 add_217[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_75 (LayerNo (None, 4, 64)        128         add_134[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_91 (LayerNo (None, 4, 64)        128         add_162[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_107 (LayerN (None, 4, 64)        128         add_190[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_123 (LayerN (None, 4, 64)        128         add_218[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_103 (Dense)               (None, 4, 128)       8320        layer_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_124 (Dense)               (None, 4, 128)       8320        layer_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_145 (Dense)               (None, 4, 128)       8320        layer_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_166 (Dense)               (None, 4, 128)       8320        layer_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_90 (Dropout)            (None, 4, 128)       0           dense_103[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_109 (Dropout)           (None, 4, 128)       0           dense_124[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 4, 128)       0           dense_145[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_147 (Dropout)           (None, 4, 128)       0           dense_166[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_104 (Dense)               (None, 4, 64)        8256        dropout_90[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_125 (Dense)               (None, 4, 64)        8256        dropout_109[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_146 (Dense)               (None, 4, 64)        8256        dropout_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_167 (Dense)               (None, 4, 64)        8256        dropout_147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_91 (Dropout)            (None, 4, 64)        0           dense_104[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_110 (Dropout)           (None, 4, 64)        0           dense_125[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 4, 64)        0           dense_146[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_148 (Dropout)           (None, 4, 64)        0           dense_167[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_135 (Add)                   (None, 4, 64)        0           dropout_91[0][0]                 \n",
            "                                                                 add_134[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_163 (Add)                   (None, 4, 64)        0           dropout_110[0][0]                \n",
            "                                                                 add_162[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_191 (Add)                   (None, 4, 64)        0           dropout_129[0][0]                \n",
            "                                                                 add_190[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_219 (Add)                   (None, 4, 64)        0           dropout_148[0][0]                \n",
            "                                                                 add_218[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_76 (LayerNo (None, 4, 64)        128         add_135[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_92 (LayerNo (None, 4, 64)        128         add_163[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_108 (LayerN (None, 4, 64)        128         add_191[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_124 (LayerN (None, 4, 64)        128         add_219[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_38 (MultiH (None, 4, 64)        66368       layer_normalization_76[0][0]     \n",
            "                                                                 layer_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_46 (MultiH (None, 4, 64)        66368       layer_normalization_92[0][0]     \n",
            "                                                                 layer_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_54 (MultiH (None, 4, 64)        66368       layer_normalization_108[0][0]    \n",
            "                                                                 layer_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_62 (MultiH (None, 4, 64)        66368       layer_normalization_124[0][0]    \n",
            "                                                                 layer_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_136 (Add)                   (None, 4, 64)        0           multi_head_attention_38[0][0]    \n",
            "                                                                 add_135[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_164 (Add)                   (None, 4, 64)        0           multi_head_attention_46[0][0]    \n",
            "                                                                 add_163[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_192 (Add)                   (None, 4, 64)        0           multi_head_attention_54[0][0]    \n",
            "                                                                 add_191[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_220 (Add)                   (None, 4, 64)        0           multi_head_attention_62[0][0]    \n",
            "                                                                 add_219[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_77 (LayerNo (None, 4, 64)        128         add_136[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_93 (LayerNo (None, 4, 64)        128         add_164[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_109 (LayerN (None, 4, 64)        128         add_192[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_125 (LayerN (None, 4, 64)        128         add_220[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_105 (Dense)               (None, 4, 128)       8320        layer_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_126 (Dense)               (None, 4, 128)       8320        layer_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_147 (Dense)               (None, 4, 128)       8320        layer_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_168 (Dense)               (None, 4, 128)       8320        layer_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_92 (Dropout)            (None, 4, 128)       0           dense_105[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_111 (Dropout)           (None, 4, 128)       0           dense_126[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 4, 128)       0           dense_147[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_149 (Dropout)           (None, 4, 128)       0           dense_168[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_106 (Dense)               (None, 4, 64)        8256        dropout_92[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_127 (Dense)               (None, 4, 64)        8256        dropout_111[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_148 (Dense)               (None, 4, 64)        8256        dropout_130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_169 (Dense)               (None, 4, 64)        8256        dropout_149[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_93 (Dropout)            (None, 4, 64)        0           dense_106[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 4, 64)        0           dense_127[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 4, 64)        0           dense_148[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_150 (Dropout)           (None, 4, 64)        0           dense_169[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_137 (Add)                   (None, 4, 64)        0           dropout_93[0][0]                 \n",
            "                                                                 add_136[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_165 (Add)                   (None, 4, 64)        0           dropout_112[0][0]                \n",
            "                                                                 add_164[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_193 (Add)                   (None, 4, 64)        0           dropout_131[0][0]                \n",
            "                                                                 add_192[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_221 (Add)                   (None, 4, 64)        0           dropout_150[0][0]                \n",
            "                                                                 add_220[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_78 (LayerNo (None, 4, 64)        128         add_137[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_94 (LayerNo (None, 4, 64)        128         add_165[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_110 (LayerN (None, 4, 64)        128         add_193[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_126 (LayerN (None, 4, 64)        128         add_221[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_39 (MultiH (None, 4, 64)        66368       layer_normalization_78[0][0]     \n",
            "                                                                 layer_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_47 (MultiH (None, 4, 64)        66368       layer_normalization_94[0][0]     \n",
            "                                                                 layer_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_55 (MultiH (None, 4, 64)        66368       layer_normalization_110[0][0]    \n",
            "                                                                 layer_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_63 (MultiH (None, 4, 64)        66368       layer_normalization_126[0][0]    \n",
            "                                                                 layer_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_138 (Add)                   (None, 4, 64)        0           multi_head_attention_39[0][0]    \n",
            "                                                                 add_137[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_166 (Add)                   (None, 4, 64)        0           multi_head_attention_47[0][0]    \n",
            "                                                                 add_165[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_194 (Add)                   (None, 4, 64)        0           multi_head_attention_55[0][0]    \n",
            "                                                                 add_193[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_222 (Add)                   (None, 4, 64)        0           multi_head_attention_63[0][0]    \n",
            "                                                                 add_221[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_79 (LayerNo (None, 4, 64)        128         add_138[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_95 (LayerNo (None, 4, 64)        128         add_166[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_111 (LayerN (None, 4, 64)        128         add_194[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_127 (LayerN (None, 4, 64)        128         add_222[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_107 (Dense)               (None, 4, 128)       8320        layer_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_128 (Dense)               (None, 4, 128)       8320        layer_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_149 (Dense)               (None, 4, 128)       8320        layer_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_170 (Dense)               (None, 4, 128)       8320        layer_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_94 (Dropout)            (None, 4, 128)       0           dense_107[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 4, 128)       0           dense_128[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 4, 128)       0           dense_149[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_151 (Dropout)           (None, 4, 128)       0           dense_170[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_108 (Dense)               (None, 4, 64)        8256        dropout_94[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_129 (Dense)               (None, 4, 64)        8256        dropout_113[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_150 (Dense)               (None, 4, 64)        8256        dropout_132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_171 (Dense)               (None, 4, 64)        8256        dropout_151[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 4, 64)        0           dense_108[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_114 (Dropout)           (None, 4, 64)        0           dense_129[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 4, 64)        0           dense_150[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_152 (Dropout)           (None, 4, 64)        0           dense_171[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_139 (Add)                   (None, 4, 64)        0           dropout_95[0][0]                 \n",
            "                                                                 add_138[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_167 (Add)                   (None, 4, 64)        0           dropout_114[0][0]                \n",
            "                                                                 add_166[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_195 (Add)                   (None, 4, 64)        0           dropout_133[0][0]                \n",
            "                                                                 add_194[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_223 (Add)                   (None, 4, 64)        0           dropout_152[0][0]                \n",
            "                                                                 add_222[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_0 (LayerNormaliza (None, 4, 64)        128         add_139[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_1 (LayerNormaliza (None, 4, 64)        128         add_167[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_2 (LayerNormaliza (None, 4, 64)        128         add_195[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_3 (LayerNormaliza (None, 4, 64)        128         add_223[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4, 256)       0           feature_layer_0[0][0]            \n",
            "                                                                 feature_layer_1[0][0]            \n",
            "                                                                 feature_layer_2[0][0]            \n",
            "                                                                 feature_layer_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1024)         0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_172 (Dense)               (None, 512)          524800      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_153 (Dropout)           (None, 512)          0           dense_172[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_173 (Dense)               (None, 128)          65664       dropout_153[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_174 (Dense)               (None, 64)           8256        dense_173[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_175 (Dense)               (None, 28)           1820        dense_174[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 26,024,412\n",
            "Trainable params: 25,939,676\n",
            "Non-trainable params: 84,736\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVv1duthjCV2"
      },
      "source": [
        "# Custom Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LmXz-_iIOQx"
      },
      "source": [
        "def get_activations(model, layer, X_batch): \n",
        "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.get_layer[layer].output])\n",
        "    activations = get_activations([X_batch,0]) \n",
        "    return activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zb8HM6sJeDf"
      },
      "source": [
        "# Simple model for testing\n",
        "inputs_test_why_nan = tf.keras.Input(shape=(4,200,200,4))\n",
        "flatten = layers.Flatten()(inputs_test_why_nan)\n",
        "outputs_test_why_nan = tf.keras.layers.Dense(28)(flatten)\n",
        "model_test_why_nan = tf.keras.Model(inputs_test_why_nan, outputs_test_why_nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmXuVdvLBhiH"
      },
      "source": [
        "# test data training\n",
        "x_train = tf.random.uniform(shape=(1000,4,200,200,4))\n",
        "y_train = tf.random.uniform(shape=(1000,28,))\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3b56q23Ko__"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSBtUGDIiq4P"
      },
      "source": [
        "## my data + my model(flatten layer, no optimiztion, no loss func)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPfKthDLNxQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "b129c411-d312-4377-a742-b0743adc5621"
      },
      "source": [
        "layer_name = 'flatten_1' \n",
        "intermediate_layer_model = tf.keras.Model(inputs=vit_resnet_backbone_model.input,\n",
        "                                       outputs=vit_resnet_backbone_model.get_layer(layer_name).output)\n",
        "\n",
        "# intermediate_output = intermediate_layer_model(data)\n",
        "for epoch in range(10):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_set.batch(16)):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            # tf.print('x train',x_batch_train)\n",
        "            logits = intermediate_layer_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            tf.print('logits',logits)\n",
        "            # loss_value = hungarian_loss_test(y_batch_train, logits)\n",
        "        #     # loss_value = custom_mse(y_batch_train, logits)\n",
        "            \n",
        "            \n",
        "        # # Use the gradient tape to automatically retrieve\n",
        "        # # the gradients of the trainable variables with respect to the loss.\n",
        "        \n",
        "        # grads = tape.gradient(loss_value, vit_resnet_backbone_model.trainable_weights)\n",
        "\n",
        "        # # Run one step of gradient descent by updating\n",
        "        # # the value of the variables to minimize the loss.\n",
        "        # optimizer.apply_gradients(zip(grads, vit_resnet_backbone_model.trainable_weights))\n",
        "\n",
        "        # loss_tracker.update_state(loss_value)\n",
        "        # custom_mae.update_state(y_batch_train, logits)\n",
        "\n",
        "        # # Log every 200 batches.\n",
        "        # if step % 4 == 0:\n",
        "        #     print(\n",
        "        #         \"Training loss (for one batch) at step %d: %.4f\"\n",
        "        #         % (step, float(loss_value))\n",
        "        #     )\n",
        "        #     # print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
        "        #     # print(\"loss\", loss_tracker.result())\n",
        "        #     print(\"mae\", custom_mae.result().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "logits [[-1.71439505 -0.75186348 0.479402751 ... 0.0353529453 -1.34864712 -0.144142479]\n",
            " [-0.621864796 -0.256180257 -0.340005875 ... 0.577286 -0.0979001895 -1.13587761]\n",
            " [0.097345829 0.468154 -1.09192944 ... -0.490615785 -1.0671593 -1.15583038]\n",
            " ...\n",
            " [0.251539588 -1.3216151 0.489063561 ... 0.894196 0.119414963 0.660185158]\n",
            " [-0.0348597318 1.40203118 0.359657705 ... -0.35415554 -0.175807595 -1.21606171]\n",
            " [0.379231095 0.82060045 -1.5533998 ... -1.85682058 -0.385280848 1.48406839]]\n",
            "logits [[0.851750553 1.3106811 -0.126741618 ... -0.484109879 0.525491774 1.82400239]\n",
            " [1.06322396 0.484337032 0.0382358 ... -0.304710656 0.194525301 -1.09570217]\n",
            " [0.406731516 -0.322989047 0.381424218 ... -0.485583127 1.13479304 -2.2166667]\n",
            " ...\n",
            " [0.25521642 1.55943823 -0.160802409 ... -0.1639027 -1.52097559 -0.75267756]\n",
            " [-1.84177721 0.450192809 0.934633 ... -0.922637284 -1.72229183 -0.706922054]\n",
            " [0.377999842 0.476803 -1.18401575 ... 1.1688447 -1.38231432 -0.88525641]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8934c016edb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# on the GradientTape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# tf.print('x train',x_batch_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Logits for this minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 415\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2723\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2724\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhimOZjri3OG"
      },
      "source": [
        "## my data + full my model, no optimization, no loss function ---> no nan values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q5Loj9XwKMKg",
        "outputId": "a361e279-4247-4c4f-f08d-3979278e4d1b"
      },
      "source": [
        "for epoch in range(10):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_set.batch(16)):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            # tf.print('x train',x_batch_train)\n",
        "            logits = vit_resnet_backbone_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            tf.print('logits',logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "logits [[-0.849678934 0.105679132 0.340049684 ... -0.898546875 0.422087729 -0.377900451]\n",
            " [0.140348166 0.697249353 -0.343105376 ... -0.79787606 -0.369138569 -1.41631031]\n",
            " [-0.295462847 -0.0156862363 0.204963773 ... -0.652339637 0.19438462 -0.371059686]\n",
            " ...\n",
            " [-0.91804409 -0.85144031 -0.0399494767 ... 0.632083058 0.270431042 -1.01818466]\n",
            " [0.216327205 0.41282776 0.681494951 ... -0.427113265 0.622757792 -0.88867414]\n",
            " [-1.32235754 -0.14366217 0.230182663 ... -0.41296643 0.320233911 -0.919894278]]\n",
            "logits [[-1.05077171 0.0924906656 0.576493382 ... -0.460907757 0.195384338 -0.922265172]\n",
            " [-0.73890847 -0.103426181 0.618675709 ... -0.212756261 -0.269826382 -0.938273489]\n",
            " [-1.15895188 0.438050747 -0.0912577063 ... 0.467403352 -0.208006054 -0.502323687]\n",
            " ...\n",
            " [-0.963484108 -0.609876931 0.82511729 ... -0.0708422512 -0.233534783 -0.43787846]\n",
            " [-1.25527072 0.163649872 -0.37629208 ... 0.384696633 0.271535307 -0.732873499]\n",
            " [-0.0571408831 0.412921876 0.521957278 ... 0.129483461 0.277249873 -0.347812265]]\n",
            "logits [[-1.76019084 -0.694347441 0.134287804 ... -0.339994907 -0.188160464 -0.263195783]\n",
            " [-0.253878862 0.922656655 0.269568205 ... -0.371753782 0.39890632 -0.923733592]\n",
            " [-0.590849221 -0.436705023 0.595519066 ... -0.922320426 0.743386626 -0.405299336]\n",
            " ...\n",
            " [-1.05671144 0.360866845 -0.599980831 ... 0.65242213 0.309551597 -0.508076727]\n",
            " [-1.1009475 -0.601962626 -0.164176881 ... -0.77487278 -0.32552281 -1.30470276]\n",
            " [-0.578416765 -0.115280196 0.889856696 ... 0.418540359 0.157517403 -0.554874718]]\n",
            "logits [[-0.874962449 -0.57637167 -0.213753805 ... -0.775084376 0.315546 -0.360599846]\n",
            " [-0.875014961 0.135917544 0.300177783 ... -0.425130367 0.124033771 -0.628180444]\n",
            " [-0.890671 0.186708629 0.125566229 ... -1.036672 0.300960451 -0.831150353]\n",
            " ...\n",
            " [-0.801440299 0.259599805 0.919694483 ... -0.338398516 0.451829135 -0.848774076]\n",
            " [-0.372031659 0.0396518968 0.131033942 ... 0.465582609 0.171092287 -0.906849086]\n",
            " [-0.808267176 -0.281506956 -0.121634282 ... 0.55053246 0.0591648109 -0.0516075231]]\n",
            "logits [[-1.55083573 -0.254119426 0.0456960648 ... -0.44407481 0.162134632 -0.228338823]\n",
            " [-0.698861063 0.337030202 0.305301338 ... -0.457097381 0.301502913 -0.260281086]\n",
            " [-1.6119349 -0.202543125 0.601531327 ... 0.359774143 0.0290356185 -0.930515766]\n",
            " ...\n",
            " [-0.35878548 0.585600376 0.630858779 ... -0.112159647 0.579465747 -0.643342555]\n",
            " [-0.839855254 -0.510211647 0.617955327 ... 0.241539344 -0.119461194 -0.411111414]\n",
            " [-0.982724309 0.320868939 0.322943658 ... 0.284570396 0.715892851 -0.494592845]]\n",
            "logits [[-0.632355034 0.605482 -0.12278372 ... 0.903349936 0.332095444 -0.512412]\n",
            " [-0.833568096 -0.682842851 -0.473025739 ... -0.0721647 -0.50731492 -1.05768275]\n",
            " [-0.934101939 -0.220863253 -0.423594117 ... 0.00255230418 0.222116649 -0.917203724]\n",
            " ...\n",
            " [-0.401643723 -0.29620713 0.390377313 ... -0.919423878 -0.0739294738 -0.517812371]\n",
            " [-0.797740519 0.698736966 0.196119741 ... 0.512404263 -0.196289182 -0.629643857]\n",
            " [-1.06401432 -0.0536992289 0.346163511 ... 0.261743456 -0.392008424 -0.45873329]]\n",
            "Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "logits [[-1.11807179 -0.816422641 0.917054 ... 0.341111034 -0.596158803 -0.340415448]\n",
            " [-0.730702519 -0.036096 1.42878103 ... -0.151338562 0.0540799424 -0.511363089]\n",
            " [-0.266961455 0.0808074921 -0.215179846 ... 0.0920588151 0.035955634 -1.0813185]\n",
            " ...\n",
            " [0.0282099284 -0.231569186 0.563461661 ... 0.160940722 0.684709668 -0.378863126]\n",
            " [-0.466742724 0.332283109 0.324261039 ... -0.54870522 -0.394255817 -0.132022738]\n",
            " [-0.316406906 0.160034344 0.902518868 ... -0.959149 -0.0870398134 -0.781814]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6c41055ea444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# on the GradientTape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# tf.print('x train',x_batch_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvit_resnet_backbone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Logits for this minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 415\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fused_batch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;31m# Currently never reaches here since fused_batch_norm does not support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     output, mean, variance = control_flow_util.smart_cond(\n\u001b[0;32m--> 612\u001b[0;31m         training, train_op, _fused_batch_norm_inference)\n\u001b[0m\u001b[1;32m    613\u001b[0m     \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_add_or_remove_bessels_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/control_flow_util.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m    104\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[1;32m    105\u001b[0m   return tf.__internal__.smart_cond.smart_cond(\n\u001b[0;32m--> 106\u001b[0;31m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    585\u001b[0m           \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m           exponential_avg_factor=exponential_avg_factor)\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fused_batch_norm_training_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mfused_batch_norm\u001b[0;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name, exponential_avg_factor)\u001b[0m\n\u001b[1;32m   1676\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m       \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1678\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1679\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_v3\u001b[0;34m(x, scale, offset, mean, variance, epsilon, exponential_avg_factor, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4264\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FusedBatchNormV3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4265\u001b[0m         \u001b[0;34m\"epsilon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exponential_avg_factor\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexponential_avg_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4266\u001b[0;31m         \"data_format\", data_format, \"is_training\", is_training)\n\u001b[0m\u001b[1;32m   4267\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FusedBatchNormV3Output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CydPRRGUi9YG"
      },
      "source": [
        "##  my data + full model + loss function ---> dont have nan values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_nTJZRCK7ol"
      },
      "source": [
        " my data + full model + loss function ---> dont have nan values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AkjV7_lsK6tw",
        "outputId": "09ea7091-dbdf-4c2f-f0e4-51193eae5664"
      },
      "source": [
        "for epoch in range(10):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_set.batch(16)):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            chosen_model = vit_resnet_backbone_model\n",
        "            logits = chosen_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            tf.print('logits = \\n',logits)\n",
        "            loss_value = hungarian_loss_test(y_batch_train, logits)\n",
        "            # loss_value = custom_mse(y_batch_train, logits)\n",
        "            tf.print('loss = \\n', loss_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "logits = \n",
            " [[-1.13797271 -0.32568267 1.01499796 ... 0.744917095 -0.0203404054 -0.559634566]\n",
            " [-0.670926571 -0.217934519 0.479172319 ... -0.258659273 0.665730536 -0.413513452]\n",
            " [-1.18483174 -0.226722166 -0.0219322518 ... 0.315272957 -0.136533752 -0.382322937]\n",
            " ...\n",
            " [-0.377042264 -0.436741 0.666103363 ... 0.601460218 -0.0621509366 -0.741193473]\n",
            " [-1.25202799 0.12838842 0.264127344 ... -0.29150489 0.513133407 0.0869754106]\n",
            " [-0.352390081 0.471812 0.377215832 ... -0.0650159866 0.411620647 -0.538133144]]\n",
            "loss = \n",
            " 25.3668747\n",
            "logits = \n",
            " [[-0.126417875 -0.211076811 1.18702936 ... -0.771717548 0.937044561 -0.591201246]\n",
            " [-1.0378933 0.143227026 0.447100848 ... -0.26732105 0.216578767 -0.634051085]\n",
            " [-0.880675793 0.231401369 0.844928384 ... 0.333802402 0.187624082 -0.428898]\n",
            " ...\n",
            " [-0.48180297 -0.269625187 0.395590246 ... 1.47106171 -0.210978016 -0.126058444]\n",
            " [-0.408268273 -0.0571499281 0.778343618 ... 0.333613187 0.506498337 -0.405733347]\n",
            " [-0.771617651 0.0543631129 0.182431802 ... 0.279849 0.18705146 0.0360251777]]\n",
            "loss = \n",
            " 23.2244129\n",
            "logits = \n",
            " [[-0.253618777 -0.263301 -0.203470826 ... -0.184567139 0.291617483 -1.01754439]\n",
            " [-0.547511935 -0.298166454 0.66988045 ... -0.192859 -0.146328986 -0.817476153]\n",
            " [-0.942556262 0.412528783 0.88808912 ... 0.191624612 -0.68196547 -0.475077]\n",
            " ...\n",
            " [-0.308376133 0.226373509 0.602784395 ... 0.0229506902 0.624224961 -0.766617835]\n",
            " [-0.738768041 0.605918288 0.509529233 ... -0.403950244 0.409130335 -0.168896854]\n",
            " [-1.0967437 0.259845495 0.446635455 ... -0.0673047304 0.370628774 -0.397222221]]\n",
            "loss = \n",
            " 27.6808243\n",
            "logits = \n",
            " [[0.558601 0.615003228 1.12878013 ... -0.0988789201 0.303695232 -1.14495158]\n",
            " [-0.979875863 0.0103834597 0.291034102 ... 0.211230412 -0.0187638309 -0.679010868]\n",
            " [-0.829735935 -0.362739354 0.425446808 ... -0.416499913 0.280859053 0.186172754]\n",
            " ...\n",
            " [-0.856056154 0.117474325 0.495785147 ... -0.520133138 -0.126720443 -0.892904818]\n",
            " [-1.03954804 -0.606021166 0.320244163 ... -0.207454771 0.149331331 -0.356002092]\n",
            " [-0.212714598 -0.0213118419 0.401678592 ... 0.292068571 -0.188754454 -0.533685863]]\n",
            "loss = \n",
            " 20.4914856\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9499cffa0346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# on the GradientTape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mchosen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvit_resnet_backbone_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchosen_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Logits for this minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits = \\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 415\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1136\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m           name=name)\n\u001b[0m\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   2723\u001b[0m   return squeeze_batch_dims(\n\u001b[1;32m   2724\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH2ZzmCOjEat"
      },
      "source": [
        "## my data + my model + optimizer + loss function( not modify) + gradient ---> dont have nan values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7TU6kfQL5Vi"
      },
      "source": [
        "my data + my model + optimizer + loss function( not modify) + gradient ---> dont have nan values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7eugB1AQLrbg",
        "outputId": "655db6a8-d945-42c5-8c32-dbd5eabc1c17"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "optimizer_sgd = tf.keras.optimizers.SGD(lr=0.002, momentum=0.0, decay=0.0, nesterov=True)\n",
        "for epoch in range(10):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_set.batch(16)):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            chosen_model = vit_resnet_backbone_model\n",
        "            \n",
        "            # tf.print(safte_norm.shape)\n",
        "            logits = chosen_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            tf.print('logits = \\n',logits)\n",
        "            loss_value = hungarian_loss(y_batch_train, logits)\n",
        "            # loss_value = custom_mse(y_batch_train, logits)\n",
        "            tf.print('loss = \\n', loss_value)\n",
        "\n",
        "        grads = tape.gradient(loss_value, chosen_model.trainable_weights)\n",
        "\n",
        "        # max_gradient_norm = 10\n",
        "        # clipped_gradients, _ = tf.clip_by_global_norm(grads,\n",
        "        #                                              max_gradient_norm)\n",
        "        # clipped_gradients = tf.convert_to_tensor(clipped_gradients)\n",
        "        # tf.debugging.check_numerics(clipped_gradients, message='check grads')\n",
        "\n",
        "        optimizer_sgd.apply_gradients(zip(grads, chosen_model.trainable_weights))\n",
        "        # loss_tracker.update_state(loss_value)\n",
        "        # custom_mae.update_state(y_batch_train, logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "logits = \n",
            " [[-0.0654173419 4.73008823 2.26104856 ... 3.18095517 -0.908008516 2.09324265]\n",
            " [-0.972843409 4.49041843 -1.9896 ... 0.62029469 -0.849203885 -0.849065363]\n",
            " [-0.663763463 4.09369087 2.99688959 ... -0.825369716 1.17693496 2.49110603]\n",
            " ...\n",
            " [-0.796415031 -0.371617556 -2.8787086 ... 2.00464296 -0.647639692 0.65770328]\n",
            " [-2.10087562 0.740803599 0.604720473 ... -2.10566187 -1.27455461 -1.11787009]\n",
            " [0.471397 -0.449482 1.86493528 ... 1.32498384 -1.36704159 3.71600342]]\n",
            "\n",
            "reduce_confident_loss =  1.78766632\n",
            "\n",
            "reduce_mean_cls_loss =  0.993769705\n",
            "\n",
            "regression loss =  22.6393967\n",
            "loss = \n",
            " 25.4208336\n",
            "logits = \n",
            " [[-3.17915559 4.56515932 0.354477972 ... -0.413078368 0.750482678 2.51828218]\n",
            " [-0.9279567 3.05670547 -0.216426492 ... 1.81909609 1.33511555 2.44723701]\n",
            " [-1.51241684 4.98562574 -0.670136333 ... 0.540131271 1.02060091 0.209555045]\n",
            " ...\n",
            " [-1.92138064 -0.761775434 1.03304911 ... -0.92437923 0.622802079 -0.814962626]\n",
            " [-4.4182353 -1.28355241 1.06386268 ... -1.06357 0.959830642 1.90051723]\n",
            " [0.202806309 -1.24079597 0.83368963 ... 0.16710116 -2.62681103 1.55444932]]\n",
            "\n",
            "reduce_confident_loss =  1.43583429\n",
            "\n",
            "reduce_mean_cls_loss =  1.12908781\n",
            "\n",
            "regression loss =  20.9038696\n",
            "loss = \n",
            " 23.468792\n",
            "logits = \n",
            " [[-4.21814156 1.98841751 2.10968184 ... -1.12525463 -1.14623678 -3.24167156]\n",
            " [-3.45028806 2.01436234 -1.76094794 ... -2.22716117 -2.45291901 0.221659347]\n",
            " [-1.38032556 -1.70614588 0.787344456 ... -1.35821462 -0.803397596 -0.068755284]\n",
            " ...\n",
            " [-0.10547962 -0.866440356 0.537458122 ... -2.0479157 -0.335775822 2.25986028]\n",
            " [-3.20446682 -0.254349113 1.43844903 ... 0.7112391 -1.77711761 2.32081246]\n",
            " [-1.96432853 -3.77440333 1.55642474 ... 1.22025859 -0.246818423 1.27180731]]\n",
            "\n",
            "reduce_confident_loss =  1.02459884\n",
            "\n",
            "reduce_mean_cls_loss =  1.25512397\n",
            "\n",
            "regression loss =  24.4578762\n",
            "loss = \n",
            " 26.7375984\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cb0e36761cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss = \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# max_gradient_norm = 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    588\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    591\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    592\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1245\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ZnWEsnjJ32"
      },
      "source": [
        "## my data + my model + optimizer + loss function( modified) + gradient ---> **have nan values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ee1XZhYekyG"
      },
      "source": [
        "my data + my model + optimizer + loss function( modified) + gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "89TdpQOregUb",
        "outputId": "49dea3ec-ab81-4557-a4d8-b386328ba4d5"
      },
      "source": [
        "optimizer_sgd = tf.keras.optimizers.SGD(lr=0.002, momentum=0.0, decay=0.0, nesterov=True)\n",
        "for epoch in range(10):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_set.batch(16)):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            tf.debugging.check_numerics(x_batch_train, message='x_batch_train has nan values')\n",
        "\n",
        "            chosen_model = vit_resnet_backbone_model\n",
        "            \n",
        "            # tf.print(safte_norm.shape)\n",
        "            logits = chosen_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            tf.print('logits = \\n',logits)\n",
        "            loss_value = hungarian_loss_test(y_batch_train, logits)\n",
        "            # loss_value = custom_mse(y_batch_train, logits)\n",
        "            tf.print('loss = \\n', loss_value)\n",
        "\n",
        "        grads = tape.gradient(loss_value, chosen_model.trainable_weights)\n",
        "\n",
        "        # max_gradient_norm = 10\n",
        "        # clipped_gradients, _ = tf.clip_by_global_norm(grads,\n",
        "        #                                              max_gradient_norm)\n",
        "        # clipped_gradients = tf.convert_to_tensor(clipped_gradients)\n",
        "        # tf.debugging.check_numerics(clipped_gradients, message='check grads')\n",
        "\n",
        "        optimizer_sgd.apply_gradients(zip(grads, chosen_model.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "logits = \n",
            " [[2.15038824 0.652101457 -0.444036 ... 0.338104784 2.87000799 4.14755821]\n",
            " [2.29936719 1.23025334 0.627502739 ... -0.775661469 -0.923724174 3.18623328]\n",
            " [-0.35323 0.0404041782 0.276872605 ... -2.672328 0.403804809 2.66778183]\n",
            " ...\n",
            " [0.0476428196 2.93960619 0.227254331 ... 0.678838849 0.611553788 -1.22201574]\n",
            " [-2.50211692 0.22348772 0.00728244102 ... -0.0800140873 0.643814623 -2.17208171]\n",
            " [0.951989293 2.79293799 1.33318663 ... -4.10545444 1.72335625 4.20093918]]\n",
            "loss = \n",
            " 24.8120441\n",
            "logits = \n",
            " [[nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " ...\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]\n",
            " [nan nan nan ... nan nan nan]]\n",
            "loss = \n",
            " -nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-25f7f8d15741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Open a GradientTape to record the operations run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2723\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   2724\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPwPQQyIjVxI"
      },
      "source": [
        "## Random dataset + my model ---> dont have nans values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-9NQs8IHXjr"
      },
      "source": [
        "Random dataset + my model ---> dont have nans values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G2ByHO_2jD5z",
        "outputId": "cd60c23b-fbf2-458c-fb32-40b8a9187218"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "optimizer_sgd = tf.keras.optimizers.SGD(lr=0.002, momentum=0.0, decay=0.0, nesterov=True)\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "mae_metric = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
        "custom_mae = custom_MSE()\n",
        "for epoch in range(10):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(dataset_train.batch(16)):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            chosen_model = vit_resnet_backbone_model\n",
        "            logits = chosen_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            tf.print('logits',logits)\n",
        "            loss_value = hungarian_loss_test(y_batch_train, logits)\n",
        "            # loss_value = custom_mse(y_batch_train, logits)\n",
        "            \n",
        "            \n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        \n",
        "        grads = tape.gradient(loss_value, chosen_model.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, chosen_model.trainable_weights))\n",
        "\n",
        "        loss_tracker.update_state(loss_value)\n",
        "        custom_mae.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 4 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            # print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
        "            # print(\"loss\", loss_tracker.result())\n",
        "            print(\"mae\", custom_mae.result().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "logits [[0.0655685216 0.502792 -0.180091292 ... 0.52517134 -0.628251076 0.346105337]\n",
            " [-0.359855294 0.311243445 -0.465051055 ... -0.482927203 -1.00039339 -0.0113883987]\n",
            " [-0.173628181 -0.0616548322 0.318077475 ... -0.228932902 -0.900440633 -0.252889]\n",
            " ...\n",
            " [-0.687800229 0.271022081 0.455194473 ... -0.661325753 -0.62207371 -0.0140837673]\n",
            " [0.103241801 0.0691173 0.221493319 ... -0.168835551 -1.02801752 0.470353752]\n",
            " [-0.202928647 0.230512455 -0.0236757565 ... 0.0794470534 -0.671785176 -0.135828108]]\n",
            "Training loss (for one batch) at step 0: 2.6569\n",
            "mae 1.1328641\n",
            "logits [[0.270061463 -0.736913502 0.227205083 ... -0.0424992368 -0.600838065 0.0136727169]\n",
            " [-1.12225914 -0.398260504 -0.116044149 ... 0.842112362 -1.15992165 -0.458299339]\n",
            " [-0.0434222072 0.212386772 0.285135239 ... -0.36739257 -0.725091517 0.564991117]\n",
            " ...\n",
            " [-0.0422052331 0.440071732 0.0530658849 ... 0.299730539 -1.0475384 -0.0864233896]\n",
            " [-0.953335047 0.272529215 -0.801008046 ... 0.218891606 -0.26519236 0.747643828]\n",
            " [0.286867678 -0.249667451 -1.06725121 ... 0.521721065 -0.375910968 0.44465977]]\n",
            "logits [[0.137183487 -0.0420978814 -1.09765124 ... 0.409019351 -0.612681746 0.824689806]\n",
            " [0.131304964 -0.27481252 -0.194403782 ... -0.119865611 -0.16076152 -0.204491407]\n",
            " [-0.524558067 -0.321303874 0.0418705 ... -0.567633867 -1.40540779 0.242216274]\n",
            " ...\n",
            " [0.523671746 0.278461784 0.367205173 ... -0.0702825561 -0.683334589 -0.382661462]\n",
            " [-0.395362318 -0.433220834 0.732007504 ... -0.582787931 -1.11233556 0.144523695]\n",
            " [-0.374386966 0.327929676 -0.711532414 ... 0.500677526 -1.21073329 -0.229242936]]\n",
            "logits [[0.042195268 0.445079148 -0.381672978 ... 0.608486772 0.107959561 -0.154193223]\n",
            " [-0.310232878 -0.0896262228 0.4106282 ... 0.146340489 -0.882355511 -0.235904887]\n",
            " [0.692975938 0.681337476 -1.2899245 ... -0.461992085 -0.804495811 0.91638732]\n",
            " ...\n",
            " [-0.211027399 0.420058399 0.378408581 ... 0.485843182 0.058745224 -0.338151574]\n",
            " [0.0251346324 0.37475878 -0.859135091 ... 1.31618106 -0.775191 0.27608037]\n",
            " [-0.726454079 0.602160633 0.0630375147 ... 0.612902582 -0.270262599 -0.180484846]]\n",
            "logits [[0.357913375 0.366660535 -0.810863793 ... 0.520072639 -0.641195 0.233301461]\n",
            " [-0.593673587 0.415305078 -0.370659053 ... 0.277893215 -0.666541576 -0.284829468]\n",
            " [0.217489079 1.21940494 0.00633909833 ... 0.849845052 -0.592398226 -0.493129611]\n",
            " ...\n",
            " [0.093644008 0.797472596 -0.454089582 ... 0.535383046 -1.13278091 0.0768043324]\n",
            " [-0.87164396 0.166851401 1.22709882 ... -0.147685707 -0.755805492 -0.152997136]\n",
            " [0.227972612 0.0500010066 -0.516838789 ... 0.650276482 -0.876612604 0.0421482176]]\n",
            "Training loss (for one batch) at step 4: 2.3448\n",
            "mae 0.65348345\n",
            "logits [[0.0931881741 0.301615983 0.359285146 ... -0.453536868 -0.743387461 0.0816930234]\n",
            " [0.307597101 -0.0677624047 -0.679184854 ... -0.0891559273 -0.70372206 0.41904521]\n",
            " [-0.0115507785 -0.0155040482 -0.136086032 ... 0.451006323 -0.619644403 0.24378784]\n",
            " ...\n",
            " [0.441362739 -0.040371418 -0.336292773 ... 0.123226427 -0.536963344 -0.0916287377]\n",
            " [-0.0311726648 -0.240150154 0.182564542 ... -0.0849014446 -1.19709682 -0.0552119315]\n",
            " [0.938616753 -0.298235893 0.330792367 ... -0.74409467 -0.696210206 0.31059432]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e4544e6a2f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# the gradients of the trainable variables with respect to the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Run one step of gradient descent by updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_MaxPoolGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    684\u001b[0m       \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m       \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m       data_format=op.get_attr(\"data_format\"))\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool_grad\u001b[0;34m(orig_input, orig_output, grad, ksize, strides, padding, explicit_paddings, data_format, name)\u001b[0m\n\u001b[1;32m   5758\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MaxPoolGrad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ksize\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5759\u001b[0m         \u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5760\u001b[0;31m         explicit_paddings, \"data_format\", data_format)\n\u001b[0m\u001b[1;32m   5761\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAH8PkhRSVV3"
      },
      "source": [
        "# Built-in training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1AFs-wmKrV"
      },
      "source": [
        "def run_experiment(model, history_ = None):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "    \n",
        "    # 'mean_squared_error'\n",
        "    model.compile(loss= rmsle_K_fit_test, optimizer=optimizer,\n",
        "                  metrics=[custom_MSE()])\n",
        "\n",
        "    checkpoint_filepath = \"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/test_check_point\"\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "    # lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "    reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.00001)\n",
        "\n",
        "    callbacks = [checkpoint_callback, early_stopping_callback, reduce_lr_callback]\n",
        "\n",
        "    if history_ == 'use_generator':\n",
        "        history = model.fit(\n",
        "                    train_set.batch(32),\n",
        "                    validation_data=validation_set.batch(32),\n",
        "                    epochs=1,\n",
        "                    verbose=1,\n",
        "                    shuffle=True)\n",
        "    elif history_ == 'use_fit':\n",
        "        history = model.fit(x=concat_rgb_depth_train,y=np.array(label),batch_size=10,epochs=10,verbose=1,shuffle=True)\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZhvD64jFp-a"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xnEm0ioScCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "574a14d7-8b3d-4635-e0a8-b4354470f0b2"
      },
      "source": [
        "history = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      3/Unknown - 269s 54s/step - loss: nan - custom_mse: nanCan not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "      8/Unknown - 1158s 142s/step - loss: nan - custom_mse: nan"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-7f348dcb3daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_resnet_backbone_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'use_generator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-8bc40a792446>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, history_)\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     shuffle=True)\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhistory_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_fit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_rgb_depth_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-l5ZYaSRS1C"
      },
      "source": [
        "history_1 = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JV90q2zSlzQ"
      },
      "source": [
        "vit_resnet_backbone_model.save(\"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPJITpa_6qFZ"
      },
      "source": [
        "history_2 = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x241PrXyFmFC"
      },
      "source": [
        "# Test with nan values in validation = 0. I forgot to change it\n",
        "history_2 = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBLthxxog61"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAB8_iPpnS5C"
      },
      "source": [
        "vit_resnet_backbone_model.save(\"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZDTtUwp4D9F"
      },
      "source": [
        "vit_resnet_backbone_model.save(\"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NOY5Yg-pWtD"
      },
      "source": [
        "# Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5YjSKmEwC49"
      },
      "source": [
        "num_of_image = '006484'\n",
        "round = 'round1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1KOEjj6x1CV"
      },
      "source": [
        "def show_prediction(num_of_image,round):\n",
        "    # RGB image\n",
        "    test_predict_img_front_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/front_image/{round}/{round}_{num_of_image}.jpg'\n",
        "    test_predict_img_left_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/left_image/{round}/{round}_{num_of_image}.jpg'\n",
        "    test_predict_img_right_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/right_image/{round}/{round}_{num_of_image}.jpg'\n",
        "    test_predict_img_back_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/back_image/{round}/{round}_{num_of_image}.jpg'\n",
        "\n",
        "    test_predict_img_front = read_rgb_image(test_predict_img_path)\n",
        "    test_predict_img_left = read_rgb_image(test_predict_img_left_path)\n",
        "    test_predict_img_right = read_rgb_image(test_predict_img_right_path)\n",
        "    test_predict_img_back = read_rgb_image(test_predict_img_back_path)\n",
        "\n",
        "    # Depth image\n",
        "    test_predict_img_front_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/front_image/{round}/{round}_{num_of_image}.png'\n",
        "    test_predict_img_left_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/left_image/{round}/{round}_{num_of_image}.png'\n",
        "    test_predict_img_righ_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/right_image/{round}/{round}_{num_of_image}.png'\n",
        "    test_predict_img_back_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/back_image/{round}/{round}_{num_of_image}.png'\n",
        "\n",
        "    test_predict_img_depth_front = read_depth_image(test_predict_img_front_depth_path)\n",
        "    test_predict_img_depth_left = read_depth_image(test_predict_img_left_depth_path)\n",
        "    test_predict_img_depth_right = read_depth_image(test_predict_img_righ_depth_path)\n",
        "    test_predict_img_depth_back = read_depth_image(test_predict_img_back_depth_path)\n",
        "\n",
        "    # Concatenate rgb and depth\n",
        "    concatennate_rgb_depth_front = tf.concat([test_predict_img_front,test_predict_img_depth_front],axis=-1)\n",
        "    concatennate_rgb_depth_right = tf.concat([test_predict_img_right,test_predict_img_depth_left],axis=-1)\n",
        "    concatennate_rgb_depth_left = tf.concat([test_predict_img_left,test_predict_img_depth_left],axis=-1)\n",
        "    concatennate_rgb_depth_back = tf.concat([test_predict_img_back,test_predict_img_depth_back],axis=-1)\n",
        "\n",
        "    normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "    normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "    normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "    normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)\n",
        "\n",
        "\n",
        "    normalized_front_expand = tf.expand_dims(normalized_front,axis=0)\n",
        "    normalized_left_expand = tf.expand_dims(normalized_left,axis=0)\n",
        "    normalized_right_expand = tf.expand_dims(normalized_right,axis=0)\n",
        "    normalized_back_expand = tf.expand_dims(normalized_back,axis=0)\n",
        "\n",
        "    concat_all_sides = tf.concat([normalized_front_expand,normalized_left_expand,normalized_right_expand,normalized_back_expand],axis=0)    \n",
        "    concat_all_sides = tf.expand_dims(concat_all_sides,axis = 0)\n",
        "\n",
        "    # Run prediction\n",
        "    prediction = vit_resnet_backbone_model.predict(concat_all_sides)\n",
        "    plt.imshow(cv2.imread(test_predict_img_front_path))\n",
        "    tf.print(prediction.reshape((7,4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd0ao5Cmyj9k"
      },
      "source": [
        "show_prediction(num_of_image,round)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7pUTcAnqk7M"
      },
      "source": [
        "RGB images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSiH4ONVoDfG"
      },
      "source": [
        "test_predict_img_front_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/front_image/round3/round3_{num_of_image}.jpg'\n",
        "test_predict_img_left_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/left_image/round3/round3_{num_of_image}.jpg'\n",
        "test_predict_img_right_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/right_image/round3/round3_{num_of_image}.jpg'\n",
        "test_predict_img_back_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/back_image/round3/round3_{num_of_image}.jpg'\n",
        "\n",
        "test_predict_img_front = read_rgb_image(test_predict_img_path)\n",
        "test_predict_img_left = read_rgb_image(test_predict_img_left_path)\n",
        "test_predict_img_right = read_rgb_image(test_predict_img_right_path)\n",
        "test_predict_img_back = read_rgb_image(test_predict_img_back_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhbhtRtmqnBJ"
      },
      "source": [
        "Depth images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JVjOyq3qo2V"
      },
      "source": [
        "test_predict_img_front_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/front_image/round3/round3_{num_of_image}.png'\n",
        "test_predict_img_left_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/left_image/round3/round3_{num_of_image}.png'\n",
        "test_predict_img_righ_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/right_image/round3/round3_{num_of_image}.png'\n",
        "test_predict_img_back_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/back_image/round3/round3_{num_of_image}.png'\n",
        "\n",
        "test_predict_img_depth_front = read_depth_image(test_predict_img_front_depth_path)\n",
        "test_predict_img_depth_left = read_depth_image(test_predict_img_left_depth_path)\n",
        "test_predict_img_depth_right = read_depth_image(test_predict_img_righ_depth_path)\n",
        "test_predict_img_depth_back = read_depth_image(test_predict_img_back_depth_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6jVI6OzrlLb"
      },
      "source": [
        "Concatenate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtICm_N1rmli"
      },
      "source": [
        "# Concatenate rgb and depth\n",
        "concatennate_rgb_depth_front = tf.concat([test_predict_img_front,test_predict_img_depth_front],axis=-1)\n",
        "concatennate_rgb_depth_right = tf.concat([test_predict_img_right,test_predict_img_depth_left],axis=-1)\n",
        "concatennate_rgb_depth_left = tf.concat([test_predict_img_left,test_predict_img_depth_left],axis=-1)\n",
        "concatennate_rgb_depth_back = tf.concat([test_predict_img_back,test_predict_img_depth_back],axis=-1)\n",
        "\n",
        "normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vin0zGbesRPd"
      },
      "source": [
        "normalized_front_expand = tf.expand_dims(normalized_front,axis=0)\n",
        "normalized_left_expand = tf.expand_dims(normalized_left,axis=0)\n",
        "normalized_right_expand = tf.expand_dims(normalized_right,axis=0)\n",
        "normalized_back_expand = tf.expand_dims(normalized_back,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnfT_nrbuiC9"
      },
      "source": [
        "normalized_back_expand.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAuHuNhetx8e"
      },
      "source": [
        "concat_all_sides = tf.concat([normalized_front_expand,normalized_left_expand,normalized_right_expand,normalized_back_expand],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc6Y-kx1usVE"
      },
      "source": [
        "concat_all_sides = tf.expand_dims(concat_all_sides,axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ang8tTfNuPVM"
      },
      "source": [
        "concat_all_sides.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cC_5YBdYTVs"
      },
      "source": [
        "vit_resnet_backbone_model.predict(concat_all_sides)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBt7nEJL2ASp"
      },
      "source": [
        "# load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebKlm2BsI79"
      },
      "source": [
        "path_to_model = \"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_2\"\n",
        "custom_objects = {'Patches':Patches,'PatchEncoder': PatchEncoder, # Still have a problem here\n",
        "                  'Hungarian_loss': hungarian_loss,\n",
        "                  'custom_MSE': custom_MSE,\n",
        "                  'hungarian_loss_fit':hungarian_loss_fit }\n",
        "reconstructed_model = tf.keras.models.load_model(path_to_model,custom_objects=custom_objects)                                                                                                                                                     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ytdeGoS2GJT"
      },
      "source": [
        "reconstructed_model.predict(concat_all_sides)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3hj1_ubGMva"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}