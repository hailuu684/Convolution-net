{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HungarianLoss+BCELoss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LPi_xPe6kFV1eNTKWwHq1onJsDXep_Mj",
      "authorship_tag": "ABX9TyNgvaTcmgMfFBNVVo/6Kf7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailuu684/Convolution-net/blob/main/HungarianLoss%2BBCELoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcIb8zHyOZgv",
        "outputId": "93e1f448-d2f4-4d0e-b58a-87d29b0cae4b"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Input, Lambda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.19.5\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6fS99gbfgaa",
        "outputId": "68da59a3-c821-48e3-a446-158d107ebef9"
      },
      "source": [
        "!pip install keras-metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-metrics\n",
            "  Downloading keras_metrics-1.1.0-py2.py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.7/dist-packages (from keras-metrics) (2.6.0)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRs5xkLsfi_P"
      },
      "source": [
        "import keras_metrics as km"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3bddNhWAMNT"
      },
      "source": [
        "# Preprocess Hungarian Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOo-EBp8APcz"
      },
      "source": [
        "def replacenan(t):\n",
        "    return tf.where(tf.math.is_nan(t), x = tf.zeros_like(t), y = t)\n",
        "\n",
        "def replace_nan_by_10_square_10(t):\n",
        "    return tf.where(tf.math.is_nan(t), x = 10**10, y = t)\n",
        "\n",
        "def replace_nan_by_1000(t):\n",
        "    return tf.where(tf.math.is_nan(t), x = tf.cast(1000,dtype=tf.float32), y = t)\n",
        "\n",
        "def replace_10_square_10_by_zero(t):\n",
        "    return tf.where(tf.math.equal(t, 10**10), x = 0.0, y = t)\n",
        "\n",
        "def replace_zero_by_two(t):\n",
        "    return tf.where(tf.math.equal(t, 0), x = 2.0, y = t)\n",
        "\n",
        "def replace_two_by_zero(t):\n",
        "    return tf.where(tf.math.equal(t, 2), x = 0.0, y = t)\n",
        "\n",
        "def remove_zero(t):\n",
        "    intermediate_tensor = t\n",
        "    # intermediate_tensor = tf.cast(intermediate_tensor,tf.double)\n",
        "    batch = t.shape[0]\n",
        "    zero_vector = tf.zeros(shape=t.shape, dtype=tf.double)\n",
        "    bool_mask = tf.not_equal(intermediate_tensor, zero_vector)\n",
        "    omit_zeros = tf.boolean_mask(intermediate_tensor, bool_mask)\n",
        "    # tf.print('omit zero',omit_zeros)\n",
        "\n",
        "    non_zero_dimension = int(len(omit_zeros) / batch)\n",
        "\n",
        "    # tf.print('len omit 0',len(omit_zeros))\n",
        "    # tf.print('non_0_dimension', non_zero_dimension)\n",
        "    # tf.print('omit_0 shape',omit_zeros.shape)\n",
        "\n",
        "    # saved_omit_zeros = 0\n",
        "    # if len(omit_zeros) != non_zero_dimension * batch:\n",
        "    #     difference = len(omit_zeros) - non_zero_dimension * batch\n",
        "    #     if difference > 0:\n",
        "    #         omit_zeros = omit_zeros[:-difference]\n",
        "    #     else:\n",
        "    #         random_num = tf.random.uniform(shape=(-difference,),dtype=tf.double)\n",
        "    #         omit_zeros = tf.concat([omit_zeros,random_num],axis=-1)\n",
        "    #         tf.print('omit0 inside if', omit_zeros.shape)\n",
        "\n",
        "    omit_zeros = tf.reshape(omit_zeros,shape=(batch,non_zero_dimension))\n",
        "    return omit_zeros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jujrp1Hc-pRV"
      },
      "source": [
        "# Test data hungarian loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0wC0xScCa_F"
      },
      "source": [
        "y_true_test_h_l = tf.random.uniform(shape=(16,28,),dtype=tf.double)\n",
        "y_pred_test_h_l = tf.random.uniform(shape=(16,28,),dtype=tf.double)\n",
        "\n",
        "y_true_test_h_l = np.array(y_true_test_h_l)\n",
        "y_pred_test_h_l = np.array(y_pred_test_h_l)\n",
        "\n",
        "y_true_test_h_l[:,8:] = np.nan\n",
        "# y_pred_test_h_l[:,8:] = np.nan\n",
        "\n",
        "y_true_test_h_l[:,11] = 0\n",
        "y_true_test_h_l[:,15] = 0\n",
        "y_true_test_h_l[:,19] = 0\n",
        "y_true_test_h_l[:,23] = 0\n",
        "y_true_test_h_l[:,27] = 0\n",
        "\n",
        "y_pred_test_h_l[:,11] = 0\n",
        "y_pred_test_h_l[:,15] = 0\n",
        "y_pred_test_h_l[:,19] = 0\n",
        "y_pred_test_h_l[:,23] = 0\n",
        "y_pred_test_h_l[:,27] = 0\n",
        "\n",
        "# Test 0 like 0 in relative coordinate\n",
        "# y_true_test_h_l[:,2][0] = 0.5\n",
        "# y_true_test_h_l[:,0][0] = 0\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(y_true_test_h_l[:,3])):\n",
        "    y_true_test_h_l[:,2][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "    y_pred_test_h_l[:,2][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "\n",
        "    y_true_test_h_l[:,6][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "    y_pred_test_h_l[:,6][i] = np.random.choice([0,1],p=[0.5, 0.5])\n",
        "\n",
        "    # confident values\n",
        "    y_true_test_h_l[:,3][i] = 1.0\n",
        "    y_pred_test_h_l[:,3][i] = 1.0\n",
        "    y_true_test_h_l[:,7][i] = 1.0\n",
        "    y_pred_test_h_l[:,7][i] = 1.0\n",
        "\n",
        "y_true_test_h_l[1,8] = 2.342432\n",
        "y_true_test_h_l[1,9] = 1.534534\n",
        "y_true_test_h_l[1,10] = 0\n",
        "y_true_test_h_l[1,11] = 1.0\n",
        "\n",
        "y_pred_test_h_l[1,8] = 1.3432\n",
        "y_pred_test_h_l[1,9] = 1.7453\n",
        "y_pred_test_h_l[1,10] = 0.2\n",
        "y_pred_test_h_l[1,11] = 0.9\n",
        "\n",
        "y_true_test_h_l = tf.convert_to_tensor(y_true_test_h_l)\n",
        "y_pred_test_h_l = tf.convert_to_tensor(y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H7OQF-7g3c3"
      },
      "source": [
        "y_pred_test_reshaped = tf.reshape(y_pred_test_h_l,shape=(16,7,4))\n",
        "y_true_test_reshaped = tf.reshape(y_true_test_h_l,shape=(16,7,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoOfyOEmuN4k",
        "outputId": "6996037e-0db0-490b-f498-37d828eacf60"
      },
      "source": [
        "y_pred_test_reshaped"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16, 7, 4), dtype=float64, numpy=\n",
              "array([[[2.63024960e-01, 4.30279114e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [6.07287109e-02, 2.05400830e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [4.50915437e-01, 7.44237141e-01, 2.14419315e-01, 0.00000000e+00],\n",
              "        [4.15061808e-01, 1.84100478e-01, 7.24516639e-01, 0.00000000e+00],\n",
              "        [6.12873881e-01, 7.61515630e-02, 3.63341073e-01, 0.00000000e+00],\n",
              "        [4.68349621e-01, 1.90770821e-01, 2.11607856e-01, 0.00000000e+00],\n",
              "        [5.65204608e-01, 7.26414284e-01, 2.28851583e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[8.92963441e-01, 1.74370707e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [3.59751900e-01, 3.53714908e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [1.34320000e+00, 1.74530000e+00, 2.00000000e-01, 9.00000000e-01],\n",
              "        [1.01773784e-01, 9.63037844e-01, 3.71291615e-01, 0.00000000e+00],\n",
              "        [3.05259201e-01, 9.29526861e-02, 6.47771802e-01, 0.00000000e+00],\n",
              "        [9.10030037e-01, 9.27744101e-01, 4.68753673e-01, 0.00000000e+00],\n",
              "        [6.78573410e-02, 4.69165764e-01, 9.24644780e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[7.61758580e-01, 2.23128249e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [3.27603218e-01, 9.49500746e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [9.37656672e-01, 5.17815627e-01, 1.31209778e-02, 0.00000000e+00],\n",
              "        [8.69957495e-01, 6.58247467e-01, 6.97142260e-02, 0.00000000e+00],\n",
              "        [7.89932655e-01, 4.54103392e-01, 1.65444096e-01, 0.00000000e+00],\n",
              "        [5.42488944e-01, 2.39903535e-01, 7.11202333e-01, 0.00000000e+00],\n",
              "        [7.76443396e-01, 8.09045456e-01, 3.83924898e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[9.29355531e-01, 5.75105704e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [3.54773060e-02, 8.24467993e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [1.06254241e-02, 6.48952255e-01, 9.81071491e-01, 0.00000000e+00],\n",
              "        [3.52473397e-01, 9.61487104e-01, 7.60451414e-01, 0.00000000e+00],\n",
              "        [9.30400721e-01, 9.66278753e-01, 8.43214302e-01, 0.00000000e+00],\n",
              "        [4.77366303e-01, 5.39617111e-01, 9.62492640e-01, 0.00000000e+00],\n",
              "        [7.79926254e-01, 2.22565709e-01, 9.17198116e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[9.23856467e-02, 6.89842558e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [1.34906910e-01, 3.19977191e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [1.58522592e-01, 4.48320587e-01, 3.50588712e-01, 0.00000000e+00],\n",
              "        [7.08037924e-01, 9.97072110e-01, 4.92592591e-01, 0.00000000e+00],\n",
              "        [2.69055117e-01, 5.64324283e-01, 9.32009625e-01, 0.00000000e+00],\n",
              "        [2.59941422e-01, 9.63975851e-01, 2.78940483e-01, 0.00000000e+00],\n",
              "        [2.79169861e-02, 3.62998083e-02, 6.71416283e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[1.07596738e-01, 4.29601369e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [6.53864274e-01, 3.55231536e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [7.79864713e-01, 8.09890042e-01, 1.80524145e-02, 0.00000000e+00],\n",
              "        [3.04206281e-01, 5.96767331e-01, 1.47784693e-01, 0.00000000e+00],\n",
              "        [8.09504506e-01, 4.77578312e-02, 1.64730303e-02, 0.00000000e+00],\n",
              "        [6.50978731e-01, 9.25070207e-01, 8.55284210e-01, 0.00000000e+00],\n",
              "        [6.66171476e-01, 4.90305825e-01, 4.73227064e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[4.32777413e-01, 3.79799791e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [6.00909235e-02, 4.24270324e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [6.61227797e-01, 2.82371811e-01, 1.78476518e-01, 0.00000000e+00],\n",
              "        [8.61997855e-01, 7.09671203e-01, 8.50593325e-01, 0.00000000e+00],\n",
              "        [2.73345572e-01, 5.54720131e-01, 7.90553073e-01, 0.00000000e+00],\n",
              "        [9.74785018e-02, 3.30963175e-01, 6.75732455e-01, 0.00000000e+00],\n",
              "        [8.75588403e-01, 8.75388368e-01, 9.26111577e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[6.00779726e-01, 4.73524365e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [3.91926465e-01, 5.43884309e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [6.05632958e-01, 5.09002897e-01, 7.46505444e-01, 0.00000000e+00],\n",
              "        [3.23931935e-01, 1.15125818e-01, 7.57556459e-01, 0.00000000e+00],\n",
              "        [8.64137802e-01, 5.75847332e-01, 4.88416400e-02, 0.00000000e+00],\n",
              "        [4.06909025e-01, 9.67086129e-02, 4.14020480e-01, 0.00000000e+00],\n",
              "        [1.28965049e-01, 8.16541831e-01, 5.01045824e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[6.14619394e-01, 6.47912669e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [9.09889588e-01, 8.33998535e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [2.52072169e-01, 8.38294127e-01, 4.86130405e-01, 0.00000000e+00],\n",
              "        [4.72702943e-01, 9.28388272e-01, 3.13227099e-01, 0.00000000e+00],\n",
              "        [7.71631892e-01, 5.86945833e-01, 2.19219127e-01, 0.00000000e+00],\n",
              "        [9.28598084e-01, 3.24596797e-01, 2.43646922e-01, 0.00000000e+00],\n",
              "        [1.30907416e-01, 3.25760361e-01, 2.26001299e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[1.87683568e-01, 3.49380648e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [3.38340179e-02, 3.64130551e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [8.59721417e-01, 2.13837989e-01, 2.89983061e-01, 0.00000000e+00],\n",
              "        [3.91984333e-01, 5.20251856e-01, 1.77953017e-01, 0.00000000e+00],\n",
              "        [8.95151792e-02, 2.26367609e-01, 8.42962748e-01, 0.00000000e+00],\n",
              "        [7.50386669e-01, 1.27964981e-02, 5.96205580e-01, 0.00000000e+00],\n",
              "        [4.75733149e-01, 4.77342050e-01, 9.31925453e-02, 0.00000000e+00]],\n",
              "\n",
              "       [[6.57761971e-01, 8.68192571e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [4.14593682e-01, 7.79399262e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [8.61865720e-01, 2.29183044e-01, 4.32878660e-01, 0.00000000e+00],\n",
              "        [1.42173654e-01, 7.06247172e-01, 3.75648996e-01, 0.00000000e+00],\n",
              "        [2.31196152e-01, 1.37631903e-01, 7.96017922e-01, 0.00000000e+00],\n",
              "        [7.98895433e-01, 8.73141115e-01, 1.35990852e-01, 0.00000000e+00],\n",
              "        [6.37086020e-02, 2.23436512e-01, 7.74511030e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[9.25129709e-01, 6.86590075e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [2.09442089e-01, 6.69003698e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [3.28920483e-01, 6.95379064e-01, 2.60944270e-01, 0.00000000e+00],\n",
              "        [6.89091061e-01, 4.55964795e-01, 6.71407249e-01, 0.00000000e+00],\n",
              "        [5.67591362e-01, 7.49489471e-01, 1.78246912e-01, 0.00000000e+00],\n",
              "        [4.83397553e-01, 1.54648104e-01, 3.16171626e-01, 0.00000000e+00],\n",
              "        [3.76053422e-01, 7.26807295e-01, 7.30704243e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[3.52456760e-01, 9.18145553e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [1.16959850e-01, 4.66127419e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [7.63529596e-02, 9.98797725e-01, 4.90656239e-01, 0.00000000e+00],\n",
              "        [7.53636916e-02, 3.81354400e-01, 6.61372685e-01, 0.00000000e+00],\n",
              "        [3.60994548e-02, 1.89971452e-01, 5.27276854e-02, 0.00000000e+00],\n",
              "        [3.25887107e-01, 9.32915046e-01, 2.36125962e-01, 0.00000000e+00],\n",
              "        [3.85161343e-01, 3.04425096e-01, 2.75246897e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[4.27906669e-01, 9.67557595e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [6.88894436e-01, 3.47267495e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [7.29127766e-01, 1.85801148e-01, 5.15355717e-02, 0.00000000e+00],\n",
              "        [5.93613074e-01, 8.59015868e-01, 1.94554995e-01, 0.00000000e+00],\n",
              "        [6.66074619e-01, 1.09805851e-02, 4.76429969e-02, 0.00000000e+00],\n",
              "        [8.86324636e-01, 8.41665484e-01, 5.01382638e-01, 0.00000000e+00],\n",
              "        [7.51469786e-01, 7.37454325e-01, 4.15367007e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[5.30381272e-01, 2.68609449e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [9.54183719e-01, 9.75373050e-01, 1.00000000e+00, 1.00000000e+00],\n",
              "        [7.63980592e-01, 6.96325884e-01, 3.16471010e-01, 0.00000000e+00],\n",
              "        [4.17491100e-01, 4.11101487e-01, 8.43532003e-01, 0.00000000e+00],\n",
              "        [6.49987307e-01, 2.35221676e-01, 1.14836243e-01, 0.00000000e+00],\n",
              "        [5.24788368e-01, 8.30724110e-01, 4.54151368e-01, 0.00000000e+00],\n",
              "        [5.05570913e-01, 7.57484982e-01, 8.41963626e-01, 0.00000000e+00]],\n",
              "\n",
              "       [[1.17503316e-01, 8.85797865e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [9.81349324e-01, 2.29220132e-01, 0.00000000e+00, 1.00000000e+00],\n",
              "        [2.44298749e-01, 8.88779863e-01, 7.73659570e-01, 0.00000000e+00],\n",
              "        [4.15353383e-01, 9.56668298e-01, 6.39113031e-04, 0.00000000e+00],\n",
              "        [3.26666112e-01, 2.74144245e-01, 4.92901080e-01, 0.00000000e+00],\n",
              "        [3.31749681e-01, 2.53976302e-01, 2.71484407e-01, 0.00000000e+00],\n",
              "        [5.86135863e-01, 5.26716938e-01, 8.19560476e-01, 0.00000000e+00]]])>"
            ]
          },
          "metadata": {},
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3lqdnqHSZGi",
        "outputId": "dbbc7b2e-10a0-4a82-bcec-352d11824362"
      },
      "source": [
        "tf.sigmoid(y_pred_test_reshaped)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16, 7, 4), dtype=float64, numpy=\n",
              "array([[[0.56537975, 0.60594032, 0.73105858, 0.73105858],\n",
              "        [0.51517751, 0.55117043, 0.73105858, 0.73105858],\n",
              "        [0.61085687, 0.67792171, 0.55340039, 0.5       ],\n",
              "        [0.60230098, 0.54589556, 0.67360084, 0.5       ],\n",
              "        [0.64859609, 0.5190287 , 0.58984897, 0.5       ],\n",
              "        [0.61499306, 0.54754859, 0.55270544, 0.5       ],\n",
              "        [0.63765593, 0.67401792, 0.5569645 , 0.5       ]],\n",
              "\n",
              "       [[0.70950134, 0.54348256, 0.5       , 0.73105858],\n",
              "        [0.58898037, 0.58751814, 0.73105858, 0.73105858],\n",
              "        [0.79301569, 0.85135901, 0.549834  , 0.7109495 ],\n",
              "        [0.52542151, 0.72372962, 0.59177104, 0.5       ],\n",
              "        [0.57572767, 0.52322145, 0.65650817, 0.5       ],\n",
              "        [0.71300631, 0.71661739, 0.61508873, 0.5       ],\n",
              "        [0.51695783, 0.61518628, 0.71598757, 0.5       ]],\n",
              "\n",
              "       [[0.68173542, 0.55555178, 0.5       , 0.73105858],\n",
              "        [0.58117609, 0.72101476, 0.5       , 0.73105858],\n",
              "        [0.71862607, 0.62663685, 0.5032802 , 0.5       ],\n",
              "        [0.70473685, 0.6588666 , 0.5174215 , 0.5       ],\n",
              "        [0.68781687, 0.61161441, 0.54126694, 0.5       ],\n",
              "        [0.63239122, 0.55968988, 0.67066678, 0.5       ],\n",
              "        [0.68491308, 0.69190606, 0.59481939, 0.5       ]],\n",
              "\n",
              "       [[0.71694452, 0.63994045, 0.5       , 0.73105858],\n",
              "        [0.5088684 , 0.69518395, 0.73105858, 0.73105858],\n",
              "        [0.50265633, 0.65677432, 0.72732077, 0.5       ],\n",
              "        [0.58721724, 0.72341945, 0.68145173, 0.5       ],\n",
              "        [0.71715658, 0.72437715, 0.69914175, 0.5       ],\n",
              "        [0.61712577, 0.63172334, 0.7236206 , 0.5       ],\n",
              "        [0.68566422, 0.55541287, 0.71447086, 0.5       ]],\n",
              "\n",
              "       [[0.52308   , 0.6659319 , 0.5       , 0.73105858],\n",
              "        [0.53367567, 0.57931869, 0.73105858, 0.73105858],\n",
              "        [0.53954786, 0.61023986, 0.58676033, 0.5       ],\n",
              "        [0.66996747, 0.73048253, 0.62071699, 0.5       ],\n",
              "        [0.56686092, 0.6374525 , 0.71748282, 0.5       ],\n",
              "        [0.56462189, 0.72391713, 0.56928645, 0.5       ],\n",
              "        [0.50697879, 0.50907396, 0.66182022, 0.5       ]],\n",
              "\n",
              "       [[0.52687326, 0.60577847, 0.5       , 0.73105858],\n",
              "        [0.65788074, 0.58788564, 0.73105858, 0.73105858],\n",
              "        [0.68565096, 0.69208607, 0.50451298, 0.5       ],\n",
              "        [0.57547045, 0.64491637, 0.53687908, 0.5       ],\n",
              "        [0.69200391, 0.51193719, 0.50411816, 0.5       ],\n",
              "        [0.65723098, 0.71607407, 0.70167445, 0.5       ],\n",
              "        [0.66064536, 0.62017847, 0.61614727, 0.5       ]],\n",
              "\n",
              "       [[0.60653669, 0.59382481, 0.5       , 0.73105858],\n",
              "        [0.51501821, 0.60450465, 0.73105858, 0.73105858],\n",
              "        [0.65953614, 0.57012761, 0.54450106, 0.5       ],\n",
              "        [0.70307789, 0.6703285 , 0.70069159, 0.5       ],\n",
              "        [0.56791405, 0.63523   , 0.68795007, 0.5       ],\n",
              "        [0.52435035, 0.58199371, 0.66278556, 0.5       ],\n",
              "        [0.7059072 , 0.70586567, 0.71628574, 0.5       ]],\n",
              "\n",
              "       [[0.64583467, 0.61621759, 0.73105858, 0.73105858],\n",
              "        [0.59674637, 0.63271554, 0.73105858, 0.73105858],\n",
              "        [0.64694398, 0.6245727 , 0.67841678, 0.5       ],\n",
              "        [0.58028219, 0.52874971, 0.68082298, 0.5       ],\n",
              "        [0.70352443, 0.64011132, 0.51220798, 0.5       ],\n",
              "        [0.60034649, 0.52415833, 0.60205152, 0.5       ],\n",
              "        [0.53219665, 0.69350177, 0.62270507, 0.5       ]],\n",
              "\n",
              "       [[0.64899383, 0.65653993, 0.5       , 0.73105858],\n",
              "        [0.71297757, 0.69719973, 0.73105858, 0.73105858],\n",
              "        [0.56268647, 0.69810582, 0.61919443, 0.5       ],\n",
              "        [0.61602331, 0.71674819, 0.57767276, 0.5       ],\n",
              "        [0.6838738 , 0.64266407, 0.55458635, 0.5       ],\n",
              "        [0.71679078, 0.58044412, 0.56061218, 0.5       ],\n",
              "        [0.5326802 , 0.58072745, 0.55626106, 0.5       ]],\n",
              "\n",
              "       [[0.54678364, 0.58646738, 0.73105858, 0.73105858],\n",
              "        [0.5084577 , 0.59003995, 0.5       , 0.73105858],\n",
              "        [0.70260245, 0.55325671, 0.57199199, 0.5       ],\n",
              "        [0.5967603 , 0.62720666, 0.54437122, 0.5       ],\n",
              "        [0.52236386, 0.55635148, 0.69908884, 0.5       ],\n",
              "        [0.67926295, 0.50319908, 0.64478772, 0.5       ],\n",
              "        [0.61673982, 0.61712004, 0.52328129, 0.5       ]],\n",
              "\n",
              "       [[0.65875747, 0.70436947, 0.73105858, 0.73105858],\n",
              "        [0.60218884, 0.68555063, 0.73105858, 0.73105858],\n",
              "        [0.70305031, 0.55704628, 0.60656086, 0.5       ],\n",
              "        [0.53548366, 0.66957139, 0.59282327, 0.5       ],\n",
              "        [0.55754295, 0.53435376, 0.68912203, 0.5       ],\n",
              "        [0.68973815, 0.70539888, 0.53394542, 0.5       ],\n",
              "        [0.51592177, 0.55562789, 0.68449591, 0.5       ]],\n",
              "\n",
              "       [[0.71608617, 0.66520794, 0.5       , 0.73105858],\n",
              "        [0.55216995, 0.66128003, 0.73105858, 0.73105858],\n",
              "        [0.58149669, 0.66716246, 0.5648684 , 0.5       ],\n",
              "        [0.6657647 , 0.61205648, 0.66181819, 0.5       ],\n",
              "        [0.63820721, 0.67906745, 0.54444412, 0.5       ],\n",
              "        [0.61854984, 0.53858516, 0.57839097, 0.5       ],\n",
              "        [0.59292088, 0.67410426, 0.67495979, 0.5       ]],\n",
              "\n",
              "       [[0.58721321, 0.7146641 , 0.5       , 0.73105858],\n",
              "        [0.52920668, 0.61446676, 0.5       , 0.73105858],\n",
              "        [0.51907897, 0.73082213, 0.62026101, 0.5       ],\n",
              "        [0.51883201, 0.59419973, 0.65956868, 0.5       ],\n",
              "        [0.50902388, 0.54735055, 0.51317887, 0.5       ],\n",
              "        [0.58075831, 0.71766631, 0.55875873, 0.5       ],\n",
              "        [0.59511735, 0.57552391, 0.56838055, 0.5       ]],\n",
              "\n",
              "       [[0.60537369, 0.72463241, 0.73105858, 0.73105858],\n",
              "        [0.66572094, 0.5859548 , 0.73105858, 0.73105858],\n",
              "        [0.67461384, 0.54631712, 0.51288104, 0.5       ],\n",
              "        [0.64419372, 0.702455  , 0.54848591, 0.5       ],\n",
              "        [0.66062364, 0.50274512, 0.5119085 , 0.5       ],\n",
              "        [0.70813112, 0.69881587, 0.6227842 , 0.5       ],\n",
              "        [0.67949887, 0.67643894, 0.60237409, 0.5       ]],\n",
              "\n",
              "       [[0.62957203, 0.56675149, 0.73105858, 0.73105858],\n",
              "        [0.72195578, 0.72618916, 0.73105858, 0.73105858],\n",
              "        [0.68221734, 0.66737267, 0.57846397, 0.5       ],\n",
              "        [0.60288274, 0.60135197, 0.69920857, 0.5       ],\n",
              "        [0.6570076 , 0.55853577, 0.52867755, 0.5       ],\n",
              "        [0.62826676, 0.69650802, 0.6116258 , 0.5       ],\n",
              "        [0.62376762, 0.68080745, 0.69887862, 0.5       ]],\n",
              "\n",
              "       [[0.52934208, 0.70802224, 0.5       , 0.73105858],\n",
              "        [0.72737587, 0.55705543, 0.5       , 0.73105858],\n",
              "        [0.56077273, 0.70863832, 0.684312  , 0.5       ],\n",
              "        [0.60237082, 0.72245425, 0.50015978, 0.5       ],\n",
              "        [0.58094797, 0.56811003, 0.62078961, 0.5       ],\n",
              "        [0.58218504, 0.56315496, 0.56745729, 0.5       ],\n",
              "        [0.64247804, 0.62871706, 0.69414303, 0.5       ]]])>"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH6bMe6gM6SR"
      },
      "source": [
        "y_true_cls_conf_test = y_true_test_reshaped[:,:,2:4] \n",
        "y_pred_cls_conf_test = tf.sigmoid(y_pred_test_reshaped[:,:,2:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcYxWBoTNFfB",
        "outputId": "25f44516-5feb-4cce-cb7d-e0f3f1e1ea27"
      },
      "source": [
        "y_true_cls_conf_test[1,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7, 2), dtype=float64, numpy=\n",
              "array([[ 0.,  1.],\n",
              "       [ 1.,  1.],\n",
              "       [ 0.,  1.],\n",
              "       [nan,  0.],\n",
              "       [nan,  0.],\n",
              "       [nan,  0.],\n",
              "       [nan,  0.]])>"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bCGFHDfoz2Q"
      },
      "source": [
        "# Hungarian loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3SRiEVu5JRG"
      },
      "source": [
        "\n",
        "def get_hungarian_row_col(y_true, y_pred):\n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "\n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "    # Take only 2 first dimension\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "\n",
        "    y_true_ = tf.cast(y_true_,dtype=tf.float32)\n",
        "    y_pred_ = tf.cast(y_pred_,dtype=tf.float32)\n",
        "\n",
        "    # Replace 0( relative_coordinate) --> 2\n",
        "    y_true_ = replace_zero_by_two(y_true_)\n",
        "    y_pred_ = replace_zero_by_two(y_pred_)\n",
        "\n",
        "\n",
        "    # if change nan --> 10^10\n",
        "    y_true_ = replace_nan_by_1000(y_true_) # shape = (16,7,2)\n",
        "    y_pred_ = replace_nan_by_1000(y_pred_)\n",
        "\n",
        "\n",
        "    # Replace 2 --> 0 of relative coordinate of y_true\n",
        "    y_true_ = replace_two_by_zero(y_true_) # shape = (16,7,2)\n",
        "    y_pred_ = replace_two_by_zero(y_pred_)\n",
        "\n",
        "\n",
        "    y_true_test = y_true_\n",
        "    y_pred_test = y_pred_\n",
        "\n",
        "\n",
        "    # tf.print(y_true_.shape)\n",
        "    \n",
        "\n",
        "    store_element = [tf.math.reduce_euclidean_norm(tf.constant((y_pred_[:,i,:2] - y_true_[:,j,:2]),dtype=tf.float32),axis=-1) for i in range(y_true_.shape[1]) for j in range(y_true_.shape[1])]\n",
        "    store_cost = tf.convert_to_tensor(store_element)\n",
        "    store_cost = tf.transpose(store_cost, perm=[1,0])\n",
        "    \n",
        "\n",
        "    cost_batch, cost_H_W = store_cost.shape\n",
        "    cost_reshape = tf.reshape(store_cost,shape=(cost_batch,int(np.sqrt(cost_H_W)), int(np.sqrt(cost_H_W)) ))\n",
        "    # print(cost_reshape.shape) # shape = (16,7,7)\n",
        "    # print(cost_reshape)\n",
        "\n",
        "    pair_row_col = []\n",
        "\n",
        "    for i in range(cost_reshape.shape[0]):\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_reshape[i,:]) # find the row_ind and col_ind for each batch\n",
        "        chosen_elements = zip(row_ind.tolist(), col_ind.tolist())   \n",
        "        pair_row_col.append(chosen_elements)\n",
        "    return pair_row_col\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3hTvbIK6Sv1"
      },
      "source": [
        "get_hungarian_row_col(y_true_test_h_l, y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGmt87nqOiJp"
      },
      "source": [
        "# global store_hungarian_row_col\n",
        "def hungarian_loss(y_true, y_pred): # -----> worked with non-nan values, the trick here is you should not use for loop \n",
        "    from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "\n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "    # Take only 2 first dimension\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "    # print(y_true_)\n",
        "\n",
        "    # tf.print('y_true',y_true_)\n",
        "    # tf.print('y_pred',y_pred_)\n",
        "\n",
        "    # # Remove nan values\n",
        "    # y_true_ = replacenan(y_true_) # shape = (16,7,2)\n",
        "    # y_pred_ = replacenan(y_pred_)\n",
        "    # print('im here after replacenan')\n",
        "\n",
        "    # Replace 0( relative_coordinate) --> 2\n",
        "    y_true_ = replace_zero_by_two(y_true_)\n",
        "    y_pred_ = replace_zero_by_two(y_pred_)\n",
        "\n",
        "\n",
        "    # if change nan --> 10^10\n",
        "    y_true_ = replace_nan_by_10_square_10(y_true_) # shape = (16,7,2)\n",
        "    y_pred_ = replace_nan_by_10_square_10(y_pred_)\n",
        "\n",
        "\n",
        "    # Replace 2 --> 0 of relative coordinate of y_true\n",
        "    y_true_ = replace_two_by_zero(y_true_) # shape = (16,7,2)\n",
        "    y_pred_ = replace_two_by_zero(y_pred_)\n",
        "\n",
        "    # tf.print(y_true_,y_true_.shape)\n",
        "    # tf.print(y_pred_,y_pred_.shape)\n",
        "    # # if change nan --> 10^10 then comment these below 2 lines\n",
        "    # y_true_ = remove_zero(y_true_) # shape = (16,4)\n",
        "    # y_pred_ = remove_zero(y_pred_)\n",
        "\n",
        "    y_true_test = y_true_\n",
        "    y_pred_test = y_pred_\n",
        "\n",
        "    # if change nan --> 10^10\n",
        "    # y_true_ = tf.reshape(y_true_,shape=(batch_size,int(y_true_.shape[1]*y_true_.shape[2]) )) # shape = (16,14)\n",
        "    # y_pred_ = tf.reshape(y_pred_,shape=(batch_size,int(y_pred_.shape[1]*y_pred_.shape[2]) ))\n",
        "    # tf.print(y_true_.shape)\n",
        "    y_true_ = tf.cast(y_true_,dtype=tf.double)\n",
        "    y_pred_ = tf.cast(y_pred_,dtype=tf.double)\n",
        "\n",
        "    # print('y_true shape',y_true_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    store_element = [tf.math.reduce_euclidean_norm(tf.constant((y_true_[:,i,:2] - y_pred_[:,j,:2]),dtype=tf.double),axis=-1) for i in range(y_true_.shape[1]) for j in range(y_true_.shape[1])]\n",
        "    store_cost = tf.convert_to_tensor(store_element)\n",
        "    store_cost = tf.transpose(store_cost, perm=[1,0])\n",
        "    \n",
        "\n",
        "    cost_batch, cost_H_W = store_cost.shape\n",
        "    cost_reshape = tf.reshape(store_cost,shape=(cost_batch,int(np.sqrt(cost_H_W)), int(np.sqrt(cost_H_W)) ))\n",
        "    # print(cost_reshape.shape) # shape = (16,7,7)\n",
        "\n",
        "    final_cost = []\n",
        "    pair_row_col = []\n",
        "\n",
        "     \n",
        "\n",
        "    # store_hungarian_row_col = []\n",
        "    for i in range(cost_reshape.shape[0]):\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_reshape[i,:]) # find the row_ind and col_ind for each batch\n",
        "        chosen_elements = zip(row_ind.tolist(), col_ind.tolist())\n",
        "        # print(row_ind,col_ind)\n",
        "        for row,col in zip(row_ind,col_ind): \n",
        "            if cost_reshape[i,row,col] < 10**10:\n",
        "                final_cost.append(cost_reshape[i,row,col])  \n",
        "                # chosen_elements = zip(row_ind.tolist(), col_ind.tolist())  \n",
        "                # pair_row_col.append(chosen_elements)    \n",
        "        pair_row_col.append(chosen_elements)\n",
        "        # store_hungarian_row_col.append(chosen_elements)\n",
        "\n",
        "    \n",
        "    final_cost = tf.convert_to_tensor(final_cost) \n",
        "    # print('final cost shape',final_cost.shape) # shape = ( 112,) = (16,7)\n",
        "    \n",
        "\n",
        "    cost = tf.cast(final_cost,dtype=tf.float32)\n",
        "    #-----------------------------\n",
        "    #--------Hungarian loss-------\n",
        "    #-----------------------------\n",
        "\n",
        "    # final_cost = tf.reduce_mean(cost,axis=-1) # shape = (batch,)\n",
        "    final_cost = tf.reduce_mean(final_cost) # shape = ()\n",
        "    # tf.print('\\nDone hungarian loss part')\n",
        "    #----------------\n",
        "    #---class loss--- note that 16 is the batch\n",
        "    #----------------\n",
        "    # print(y_true_reshape)\n",
        "\n",
        "    y_true_cls = y_true_reshape[:,:,2] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_cls = tf.sigmoid(y_pred_reshape[:,:,2])\n",
        "    \n",
        "    # Confident scores\n",
        "    y_true_conf = y_true_reshape[:,:,3] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_conf = tf.sigmoid(y_pred_reshape[:,:,3])\n",
        "\n",
        "    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    \n",
        "    y_true_cls_tf = []\n",
        "    y_pred_cls_tf = []\n",
        "    cls_loss = []\n",
        "    store_confident_loss = []\n",
        "    for batch_pair in pair_row_col:\n",
        "        for row,col in batch_pair:\n",
        "            ## y_true_cls[row,:] = [ 1 0 10e+10 10e+10 10e+10 10e+10 10e+10]\n",
        "            # len_cls_label_true = len(np.array(y_true_cls[row,:]))\n",
        "            # len_cls_label_pred = len(np.array(y_pred_cls[col,:]))\n",
        "\n",
        "            if row < len(y_pred_cls) and col < len(y_true_cls):\n",
        "                # Number of object in one frame\n",
        "                num_object_true = np.count_nonzero(~np.isnan(np.array(y_true_cls[col,:])))\n",
        "                # num_object_pred = np.count_nonzero(~np.isnan(np.array(y_pred_cls[col,:])))\n",
        "\n",
        "                # Calculate loss\n",
        "                # print('y_true',y_true_cls[row,:num_object_true])\n",
        "                # print('y_pred',y_pred_cls[col,:num_object_true])\n",
        "            \n",
        "                if len(y_true_cls[col,:num_object_true]) == len(y_pred_cls[row,:num_object_true]):\n",
        "                    y_true_cls_tf.append(y_true_cls[col,:num_object_true])\n",
        "                    y_pred_cls_tf.append(y_pred_cls[row,:num_object_true])\n",
        "                    cls_bce_loss = bce(y_true_cls[col,:num_object_true], y_pred_cls[row,:num_object_true])\n",
        "                    cls_loss.append(cls_bce_loss)\n",
        "                \n",
        "                    # print('y_true',y_true_conf[row,:])\n",
        "                    # print('y_pred',y_pred_conf[col,:])\n",
        "                    confident_loss = bce(y_true_conf[col,:], y_pred_conf[row,:])\n",
        "                    store_confident_loss.append(confident_loss)\n",
        "                else:\n",
        "                    tf.print('number of object in GT is different to the predictions')\n",
        "                    tf.print('number of objects in GT = ', len(y_true_cls[col,:num_object_true]))\n",
        "                    tf.print('number of objects in prediction = ', len(y_pred_cls[row,:num_object_true]))\n",
        "            else:\n",
        "                tf.print('indices from hungarian algorithm is larger than the object')\n",
        "                tf.print('col = \\n', col)\n",
        "                tf.print('row = \\n', row)\n",
        "                tf.print('y true class = \\n', y_true_cls)\n",
        "                tf.print('y pred class = \\n',y_pred_cls)\n",
        "\n",
        "    reduce_mean_cls_loss = tf.reduce_mean(cls_loss)\n",
        "    # tf.print('\\nDone class loss part')\n",
        "    #------------------------\n",
        "    #-----Confident Loss-----\n",
        "    #------------------------\n",
        "    reduce_confident_loss = tf.reduce_mean(store_confident_loss)\n",
        "    # print('confident loss', store_confident_loss)\n",
        "\n",
        "    #----------------------------\n",
        "    #------ SUM OF LOSSES--------\n",
        "    #----------------------------\n",
        "    regression_loss = final_cost\n",
        "\n",
        "    # Training\n",
        "    regression_loss = tf.cast(regression_loss,dtype=tf.float32)\n",
        "\n",
        "    # Testing\n",
        "    # regression_loss = tf.cast(regression_loss,dtype=tf.double)\n",
        "\n",
        "    tf.print('\\nreduce_confident_loss = ', reduce_confident_loss)\n",
        "    tf.print('\\nreduce_mean_cls_loss = ', reduce_mean_cls_loss)\n",
        "    tf.print('\\nregression loss = ', regression_loss)\n",
        "    sum_loss = reduce_mean_cls_loss + regression_loss + reduce_confident_loss\n",
        "    return sum_loss\n",
        "\n",
        "# Use with model.fit() \n",
        "def hungarian_loss_fit(y_true, y_pred):\n",
        "    loss = tf.py_function(func=hungarian_loss, inp=[y_true,y_pred], Tout=tf.float32,)\n",
        "    loss_tensor = tf.convert_to_tensor(loss)\n",
        "    # print('hungarian loss shape',cost_tensor.shape)\n",
        "\n",
        "    return loss_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apji0x1ZRbAi"
      },
      "source": [
        "tf.reshape(y_true_test_h_l,shape=(16,7,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC4yO7JEcZxR"
      },
      "source": [
        "cost_test = np.array([[4, 1, np.inf], [np.inf, 2, 5], [0.5, 2, 0.25]])\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "row_ind, col_ind = linear_sum_assignment(cost_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpeGQ8zcvOrH",
        "outputId": "cc25b020-f702-4e9a-a632-97eabe7dab87"
      },
      "source": [
        "row_ind, col_ind"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 1, 2]), array([0, 1, 2]))"
            ]
          },
          "execution_count": 372,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45gQxfUcvS9x",
        "outputId": "382c649c-22c3-41a4-9e1e-fe0b9b2b0a4d"
      },
      "source": [
        "cost_test[row_ind, col_ind].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11.0"
            ]
          },
          "execution_count": 366,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHDaokbocjBS"
      },
      "source": [
        "test_cost_matrix_true = tf.random.uniform(shape=(2,2))\n",
        "test_cost_matrix_pred = tf.random.uniform(shape=(2,2))\n",
        "store_element = [tf.square(test_cost_matrix_true[:,i] - test_cost_matrix_pred[:,j]) for i in range(test_cost_matrix_true.shape[1]) for j in range(test_cost_matrix_pred.shape[1])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmU_L3ypC3xH",
        "outputId": "29023a8f-1ac6-41cc-f95f-468770b0620e"
      },
      "source": [
        "hungarian_loss(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "reduce_confident_loss =  0.59464411277856144\n",
            "\n",
            "reduce_mean_cls_loss =  0.70010197402111118\n",
            "\n",
            "regression loss =  0.44294418697162219\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=1.7376902737712947>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvx-kGWfH-uQ"
      },
      "source": [
        "# Custom metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmsMDXxQIAlJ"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def custom_mse(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true,dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred,dtype=tf.float32)\n",
        "    loss = K.square(y_pred - y_true)  # (batch_size, 2)\n",
        "    loss = tf.experimental.numpy.nanmean(loss,axis=-1)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIuEafvMIvyC"
      },
      "source": [
        "class custom_MSE(tf.keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self, name='custom_mse', **kwargs):\n",
        "    super(custom_MSE, self).__init__(name=name, **kwargs)\n",
        "    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.convert_to_tensor(y_true)\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    \n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "    \n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "\n",
        "\n",
        "    # choose the prediction if its confident scores > 0.5\n",
        "    y_true_true_cof = y_true_reshape[y_pred_reshape[:,:,3] > 0.5]\n",
        "    y_pred_true_cof = y_pred_reshape[y_pred_reshape[:,:,3] > 0.5]\n",
        "    # tf.print('y y_true_true_cof = ', y_true_true_cof.shape)\n",
        "    # tf.print('y y_pred_true_cof = ', y_pred_true_cof.shape)\n",
        "    # tf.print('y_pred = ', y_pred_reshape)\n",
        "    y_true_ = tf.cast(y_true_true_cof, tf.float32)\n",
        "    y_pred_ = tf.cast(y_pred_true_cof, tf.float32)\n",
        "\n",
        "    # y_true_reg = y_true[:,:2]\n",
        "    # y_pred_reg = y_pred[:,:2]\n",
        "\n",
        "    loss = K.square(y_true_true_cof - y_pred_true_cof)  \n",
        "    \n",
        "    loss = tf.cast(tf.experimental.numpy.nanmean(loss,axis=1),dtype=tf.float32)\n",
        "    # loss = tf.experimental.numpy.nanmean(loss,axis=0)\n",
        "    # tf.print(loss)\n",
        "    if sample_weight is not None:\n",
        "        sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "        values = tf.multiply(values, sample_weight)\n",
        "    \n",
        "    # self.true_positives.assign_add(tf.reduce_mean(loss))\n",
        "    # tf.print(loss)\n",
        "    self.true_positives.assign(tf.reduce_mean(loss))\n",
        "\n",
        "  def result(self):\n",
        "    return self.true_positives\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.true_positives.assign(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj3GbHvUtM_O",
        "outputId": "a3118b05-03ec-4228-82f0-265d196da47f"
      },
      "source": [
        "test_custom_mse = custom_MSE()\n",
        "test_custom_mse(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.23419599>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbmmfX6e4WYJ"
      },
      "source": [
        "# get_coord_metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjyvjb0etWXM"
      },
      "source": [
        "class metric_get_cord_predict(tf.keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self, name='metric_get_cord_predict', **kwargs):\n",
        "    super(metric_get_cord_predict, self).__init__(name=name, **kwargs)\n",
        "    # self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "    self.correct_coord_predict = self.add_weight(name='correct_coord_predict', initializer='zeros',shape=(16,7,2))\n",
        "    self.accuracy = self.add_weight(name='accuracy', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.convert_to_tensor(y_true)\n",
        "    y_pred = tf.convert_to_tensor(y_pred)\n",
        "    \n",
        "    batch_size = tf.shape(y_true)[0]\n",
        "    y_h = int(y_true.shape[1]//4)\n",
        "    \n",
        "    y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "    y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "    # Get relative coordinate\n",
        "    y_true_ = y_true_reshape[:,:,:2] # shape = (16,7,2) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_ = y_pred_reshape[:,:,:2]\n",
        "\n",
        "    # Get class\n",
        "    y_true_cls = y_true_reshape[:,:,2] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "    y_pred_cls = y_pred_reshape[:,:,2]\n",
        "\n",
        "    y_true_ = tf.cast(y_true_, tf.float32)\n",
        "    y_pred_ = tf.cast(y_pred_, tf.float32)\n",
        "\n",
        "    threshold = 3.0\n",
        "    dist = y_true_ - y_pred_\n",
        "    reduced_nan_dist = tf.experimental.numpy.nanmean(dist,axis=1)\n",
        "    reduced_nan_dist = tf.reduce_mean(reduced_nan_dist)\n",
        "    # print(reduced_nan_dist)\n",
        "    accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "    for i in range(tf.shape(y_true_cls)[0]):\n",
        "        \n",
        "        num_object_true = tf.math.count_nonzero(~tf.math.is_nan(y_true_cls[i,:]),dtype=tf.dtypes.int32)\n",
        "        real_obj_y_true_cls = y_true_cls[i,:num_object_true]\n",
        "        real_obj_y_pred_cls = y_pred_cls[i,:num_object_true]\n",
        "        # print(num_object_true)\n",
        "        # print(real_obj_y_true_cls.shape)\n",
        "        self.accuracy.assign(accuracy_metric(real_obj_y_true_cls,real_obj_y_pred_cls))\n",
        "\n",
        "        for j in range(real_obj_y_true_cls.shape[0]):\n",
        "            if real_obj_y_true_cls[j] == real_obj_y_pred_cls[j] and reduced_nan_dist < threshold:\n",
        "                # print('imhere')\n",
        "                self.correct_coord_predict.assign(y_pred_)\n",
        "            else:\n",
        "                # random_var = tf.Variable(initial_value=0,shape=tf.TensorShape(16,7,2))\n",
        "                # print(random_var)\n",
        "                # self.correct_coord_predict.assign(random_var.assign(0))\n",
        "                tf.print('Predicted wrongly')\n",
        "\n",
        "    if sample_weight is not None:\n",
        "        sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "        values = tf.multiply(values, sample_weight)\n",
        "    \n",
        "    # self.true_positives.assign_add(tf.reduce_mean(loss))\n",
        "    # tf.print(loss)\n",
        "    # self.true_positives.assign(tf.reduce_mean(loss))\n",
        "\n",
        "  def result(self):\n",
        "      correct_coord_predict_result = self.correct_coord_predict\n",
        "      accuracy_result = self.accuracy\n",
        "      tf.print('accuracy result = ', accuracy_result.numpy())\n",
        "      return correct_coord_predict_result\n",
        "\n",
        "  def reset_state(self):\n",
        "    # self.true_positives.assign(0)\n",
        "    self.correct_coord_predict.assign(0)\n",
        "    self.accuracy.assign(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI7WAb0vbc5g",
        "outputId": "ca378950-6348-4a47-8efe-2ec470e77731"
      },
      "source": [
        "test_coord_metric = metric_get_cord_predict()\n",
        "test_coord_metric(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "Predicted wrongly\n",
            "accuracy result =  0.3939394\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16, 7, 2), dtype=float32, numpy=\n",
              "array([[[0.9339641 , 0.30822936],\n",
              "        [0.29648185, 0.9589524 ],\n",
              "        [0.77720946, 0.22336856],\n",
              "        [0.98941976, 0.4317422 ],\n",
              "        [0.47636455, 0.406875  ],\n",
              "        [0.27836654, 0.3238091 ],\n",
              "        [0.7602715 , 0.36716923]],\n",
              "\n",
              "       [[0.6906222 , 0.5229631 ],\n",
              "        [0.8426171 , 0.2659138 ],\n",
              "        [1.3432    , 1.7453    ],\n",
              "        [0.75936866, 0.25894392],\n",
              "        [0.9342037 , 0.35677862],\n",
              "        [0.17525631, 0.63060176],\n",
              "        [0.2962745 , 0.55165917]],\n",
              "\n",
              "       [[0.7932265 , 0.8785672 ],\n",
              "        [0.25674862, 0.01160201],\n",
              "        [0.39020705, 0.7622314 ],\n",
              "        [0.49484363, 0.58824784],\n",
              "        [0.10811135, 0.46905312],\n",
              "        [0.00827917, 0.6439175 ],\n",
              "        [0.29944125, 0.6303999 ]],\n",
              "\n",
              "       [[0.74239945, 0.20644717],\n",
              "        [0.8712418 , 0.79856926],\n",
              "        [0.23243798, 0.195503  ],\n",
              "        [0.44706687, 0.80799425],\n",
              "        [0.16180882, 0.56508994],\n",
              "        [0.78702694, 0.5420463 ],\n",
              "        [0.33475602, 0.01555103]],\n",
              "\n",
              "       [[0.09455056, 0.5462002 ],\n",
              "        [0.5369312 , 0.3229815 ],\n",
              "        [0.3561519 , 0.85104144],\n",
              "        [0.49060476, 0.8144108 ],\n",
              "        [0.16394635, 0.03125861],\n",
              "        [0.19414864, 0.27359566],\n",
              "        [0.89997905, 0.14597541]],\n",
              "\n",
              "       [[0.79545677, 0.44401237],\n",
              "        [0.34240162, 0.27802646],\n",
              "        [0.621153  , 0.20487742],\n",
              "        [0.981456  , 0.6783129 ],\n",
              "        [0.21159202, 0.37814707],\n",
              "        [0.86451006, 0.7044553 ],\n",
              "        [0.13780557, 0.77864796]],\n",
              "\n",
              "       [[0.8157296 , 0.81530076],\n",
              "        [0.07550505, 0.72178686],\n",
              "        [0.24663752, 0.33345595],\n",
              "        [0.24412246, 0.9330869 ],\n",
              "        [0.53580827, 0.35883078],\n",
              "        [0.72301567, 0.06243319],\n",
              "        [0.33268097, 0.50318575]],\n",
              "\n",
              "       [[0.7449671 , 0.02532865],\n",
              "        [0.06716036, 0.5011157 ],\n",
              "        [0.39763567, 0.71277994],\n",
              "        [0.9943161 , 0.6165716 ],\n",
              "        [0.7767337 , 0.45850018],\n",
              "        [0.0969853 , 0.5060511 ],\n",
              "        [0.5370952 , 0.7363312 ]],\n",
              "\n",
              "       [[0.8220279 , 0.44334388],\n",
              "        [0.5858255 , 0.38591224],\n",
              "        [0.73726386, 0.5032939 ],\n",
              "        [0.89422786, 0.6164415 ],\n",
              "        [0.56646013, 0.26343358],\n",
              "        [0.99957067, 0.10159653],\n",
              "        [0.56482935, 0.5900881 ]],\n",
              "\n",
              "       [[0.5964849 , 0.5744532 ],\n",
              "        [0.300507  , 0.922012  ],\n",
              "        [0.3764007 , 0.9674058 ],\n",
              "        [0.203593  , 0.24306595],\n",
              "        [0.16347003, 0.42073417],\n",
              "        [0.90577716, 0.56401503],\n",
              "        [0.34250265, 0.9581041 ]],\n",
              "\n",
              "       [[0.24286193, 0.6263473 ],\n",
              "        [0.33862713, 0.7822767 ],\n",
              "        [0.7291025 , 0.8389311 ],\n",
              "        [0.0781574 , 0.9382464 ],\n",
              "        [0.62400913, 0.33667094],\n",
              "        [0.09592631, 0.5231208 ],\n",
              "        [0.00416201, 0.6311505 ]],\n",
              "\n",
              "       [[0.06562743, 0.43219307],\n",
              "        [0.6800431 , 0.7933303 ],\n",
              "        [0.9903626 , 0.22388735],\n",
              "        [0.90997803, 0.37761477],\n",
              "        [0.30548534, 0.76192766],\n",
              "        [0.2071656 , 0.34718865],\n",
              "        [0.51479477, 0.15986335]],\n",
              "\n",
              "       [[0.6940615 , 0.49824816],\n",
              "        [0.8928622 , 0.05406937],\n",
              "        [0.7540448 , 0.73932225],\n",
              "        [0.8723867 , 0.14923525],\n",
              "        [0.08145535, 0.23725545],\n",
              "        [0.19687225, 0.4753202 ],\n",
              "        [0.21143745, 0.994107  ]],\n",
              "\n",
              "       [[0.93216   , 0.573387  ],\n",
              "        [0.42546454, 0.23398696],\n",
              "        [0.40359646, 0.8421658 ],\n",
              "        [0.71608156, 0.47238708],\n",
              "        [0.9378173 , 0.45070356],\n",
              "        [0.7779538 , 0.8497731 ],\n",
              "        [0.36900684, 0.6395187 ]],\n",
              "\n",
              "       [[0.8871572 , 0.73275065],\n",
              "        [0.4380738 , 0.23759998],\n",
              "        [0.97864383, 0.95300066],\n",
              "        [0.98428476, 0.6840698 ],\n",
              "        [0.06509411, 0.4861861 ],\n",
              "        [0.23385261, 0.85010546],\n",
              "        [0.38612854, 0.210759  ]],\n",
              "\n",
              "       [[0.3673003 , 0.15923263],\n",
              "        [0.8000021 , 0.868174  ],\n",
              "        [0.16298884, 0.34961772],\n",
              "        [0.6237846 , 0.40648195],\n",
              "        [0.9893968 , 0.9971079 ],\n",
              "        [0.65100455, 0.95997   ],\n",
              "        [0.7603945 , 0.11613812]]], dtype=float32)>"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-K6Tqx4e3E"
      },
      "source": [
        "# Recall metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df69RTDVvZT6"
      },
      "source": [
        "# https://stackoverflow.com/questions/43076609/how-to-calculate-precision-and-recall-in-keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gtzy6IG9fSL"
      },
      "source": [
        "import sklearn.metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k94ZEYO4v0rR"
      },
      "source": [
        "class Recall(tf.keras.layers.Layer):\n",
        "    \"\"\"Stateful Metric to count the total recall over all batches.\n",
        "\n",
        "    Assumes predictions and targets of shape `(samples, 1)`.\n",
        "\n",
        "    # Arguments\n",
        "        name: String, name for the metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name='recall', **kwargs):\n",
        "        super(Recall, self).__init__(name=name, **kwargs)\n",
        "        self.stateful = True\n",
        "        self.recall = K.variable(value=0.0, dtype='float32')\n",
        "\n",
        "    def reset_states(self):\n",
        "        K.set_value(self.recall, 0.0)\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        batch_size = tf.shape(y_true)[0]\n",
        "        y_h = int(y_true.shape[1]//4)\n",
        "        \n",
        "        y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "        y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "        # Get class\n",
        "        \n",
        "        y_true_cls = y_true_reshape[:,:,2] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "        y_pred_cls = tf.sigmoid(y_pred_reshape[:,:,2])\n",
        "        # tf.print('y_true cls = ', y_true_cls)\n",
        "        # tf.print('y_pred cls = ', y_pred_cls)\n",
        "\n",
        "        # Call the recall metric from tf API\n",
        "        #tf.keras.metrics.Recall(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)\n",
        "        # If threshold is None, then the default is 0.5\n",
        "        recall_metric = tf.keras.metrics.Recall()\n",
        "        total_recall_score = []\n",
        "\n",
        "        store_test_km_label_0 = []\n",
        "        store_test_km_label_1 = []\n",
        "\n",
        "        # Loop over the image in the batch --> because sometimes in the image has 1 object, some images have 3 objects in 1 batch.\n",
        "        for batch in range(tf.shape(y_true_cls)[0]):\n",
        "            \n",
        "            num_object_true = tf.math.count_nonzero(~tf.math.is_nan(y_true_cls[batch,:]),dtype=tf.dtypes.int32)\n",
        "            # tf.print(num_object_true)\n",
        "            recall_score = recall_metric(y_true_cls[batch,:num_object_true],y_pred_cls[batch,:num_object_true])\n",
        "\n",
        "            # -----------------------Test keras metric lib------------------------\n",
        "            test_km_label_0 = km.binary_recall(label=0)\n",
        "            test_km_label_1 = km.binary_recall(label=1)\n",
        "            test_km_0 = test_km_label_0(y_true_cls[batch,:num_object_true],y_pred_cls[batch,:num_object_true])\n",
        "            test_km_1 = test_km_label_1(y_true_cls[batch,:num_object_true],y_pred_cls[batch,:num_object_true])\n",
        "            store_test_km_label_0.append(test_km_0)\n",
        "            store_test_km_label_1.append(test_km_1)\n",
        "            #----------------------------------------------------------------------\n",
        "\n",
        "            total_recall_score.append(recall_score)\n",
        "\n",
        "        # recall_score = recall_metric(y_true_cls,y_pred_cls)\n",
        "        tf.print('test keras metric first label recall = ', tf.reduce_mean(store_test_km_label_0))\n",
        "        tf.print('test keras metric second label recall = ', tf.reduce_mean(store_test_km_label_1))\n",
        "        tf.print('test keras metric',(tf.reduce_mean(store_test_km_label_0)+ tf.reduce_mean(store_test_km_label_1)) / 2  )\n",
        "        recall = tf.reduce_mean(total_recall_score)\n",
        "        self.add_update(K.update(self.recall,\n",
        "                                    recall))\n",
        "        return recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfyTMoDRwswj"
      },
      "source": [
        "test_recall = Recall()\n",
        "test_recall(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQon_61h5t1y"
      },
      "source": [
        "# Recall callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqIwy5LRxiYg"
      },
      "source": [
        "a = [1,2,3,4,5,6]\n",
        "b = [i<3 for i in a] # [true, true, false,false,false]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W1Dnrf-cgeL",
        "outputId": "3749539d-228a-4eef-eb15-0883b1cefa3f"
      },
      "source": [
        "tf.boolean_mask(a,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAJMXuON5s_P"
      },
      "source": [
        "class Recall_callback(tf.keras.layers.Layer):\n",
        "    \"\"\"Stateful Metric to count the total recall over all batches.\n",
        "\n",
        "    Assumes predictions and targets of shape `(samples, 1)`.\n",
        "\n",
        "    # Arguments\n",
        "        name: String, name for the metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name='recall', **kwargs):\n",
        "        super(Recall_callback, self).__init__(name=name, **kwargs)\n",
        "        self.stateful = True\n",
        "        self.recall = K.variable(value=0.0, dtype='float32')\n",
        "\n",
        "    def reset_states(self):\n",
        "        K.set_value(self.recall, 0.0)\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        batch_size = tf.shape(y_true)[0]\n",
        "        y_h = int(y_true.shape[1]//4)\n",
        "        \n",
        "        y_true_reshape = tf.reshape(y_true,shape=(batch_size,y_h,4))\n",
        "        y_pred_reshape = tf.reshape(y_pred,shape=(batch_size,y_h,4))\n",
        "\n",
        "        # Get class and confident scores  \n",
        "        y_true_cls_conf = y_true_reshape[:,:,2:4] \n",
        "        y_pred_cls_conf = tf.sigmoid(y_pred_reshape[:,:,2:4])\n",
        "        \n",
        "\n",
        "\n",
        "        # Get class   \n",
        "        y_true_cls = y_true_reshape[:,:,2] # shape = (16,7,1) for example y_true_test_h_l[:,8:] = np.nan\n",
        "        y_pred_cls = tf.sigmoid(y_pred_reshape[:,:,2])\n",
        "        \n",
        "        # recall from tensorflow\n",
        "        recall_tf = tf.keras.metrics.Recall()\n",
        "        store_recall_tf = []\n",
        "        \n",
        "\n",
        "        pair_row_col = get_hungarian_row_col(y_true, y_pred)\n",
        "\n",
        "        y_true_stack_pair = []\n",
        "        y_pred_stack_pair = []\n",
        "\n",
        "        store_true_object = []\n",
        "        false_negative = 0\n",
        "        true_positive = 0\n",
        "        for i,batch_pair in enumerate(pair_row_col):\n",
        "            for row,col in batch_pair: # y_true is row and y_pred is col ( just changed)\n",
        "                # tf.print(row,col)\n",
        "                \n",
        "                y_true_paired = y_true_cls_conf[i,row,:]\n",
        "                y_pred_paired = y_pred_cls_conf[i,col,:]\n",
        "\n",
        "                # print('y-true-paired',y_true_paired)\n",
        "                # print('y-pred-paired',y_pred_paired)\n",
        "\n",
        "                y_true_stack_pair.append(y_true_paired)\n",
        "                y_pred_stack_pair.append(y_pred_paired)\n",
        "\n",
        "            \n",
        "               \n",
        "        # print(len(store_true_object))\n",
        "        # y_true_stack_pair shape = (112,2)\n",
        "        batch_size_stack_pair = int(len(y_true_stack_pair) / 7)\n",
        "        # tf.print(batch_size_stack_pair)\n",
        "        y_true_stack_pair = tf.reshape(y_true_stack_pair,shape=(batch_size_stack_pair,7,2))\n",
        "        y_pred_stack_pair = tf.reshape(y_pred_stack_pair,shape=(batch_size_stack_pair,7,2))\n",
        "        # print('y-true-paired',y_true_stack_pair)\n",
        "        # print('y-pred-paired',y_pred_stack_pair)\n",
        "        for i in range(len(y_true_stack_pair)):\n",
        "            num_true_object = np.count_nonzero(~np.isnan(np.array(y_true_stack_pair[i,:,0])))\n",
        "            # y_true_cls_true_obj = y_true_stack_pair[i,:num_true_object,0]\n",
        "            # y_pred_cls_true_obj = y_pred_stack_pair[i,:num_true_object,0]\n",
        "\n",
        "            # y_true_conf_true_obj = y_true_stack_pair[i,:num_true_object,1]\n",
        "            # y_pred_conf_true_obj = y_pred_stack_pair[i,:num_true_object,1]\n",
        "\n",
        "            y_true_true_obj = y_true_stack_pair[i,:num_true_object,:]\n",
        "            y_pred_true_obj = y_pred_stack_pair[i,:num_true_object,:]\n",
        "\n",
        "            # get the cls and confident scores if confident scores > 0.5\n",
        "            y_true_true_obj_filtered = y_true_true_obj[y_pred_true_obj[:,1]>0.5]\n",
        "            y_pred_true_obj_filtered = y_pred_true_obj[y_pred_true_obj[:,1]>0.5]\n",
        "\n",
        "\n",
        "            if len(y_true_true_obj_filtered) != 0 and len (y_pred_true_obj_filtered) != 0:\n",
        "                # print('y_true',y_true_true_obj_filtered)\n",
        "                # print('y_pred',y_pred_true_obj_filtered)\n",
        "\n",
        "                # -----------------------using library tensorflow-------------------------\n",
        "                recall = recall_tf(y_true_true_obj_filtered[:,0], y_pred_true_obj_filtered[:,0])\n",
        "                # tf.print('recall = ', recall)\n",
        "                store_recall_tf.append(recall)\n",
        "\n",
        "            if any(tf.math.is_nan(y_true_true_obj_filtered[:,0])) or any(tf.math.is_nan(y_pred_true_obj_filtered[:,0])):\n",
        "                tf.print('y true class = ', y_true_true_obj_filtered[:,0])\n",
        "                tf.print('y pred class = ', y_pred_true_obj_filtered[:,0])\n",
        "                tf.print('num_true_object = ', num_true_object)\n",
        "            # else:\n",
        "                # tf.print('y true class = ', y_true_true_obj_filtered[:,0])\n",
        "                # tf.print('y pred class = ', y_pred_true_obj_filtered[:,0])\n",
        "                # tf.print('num_true_object = ', num_true_object)\n",
        "\n",
        "                # # ------------------------self - implemented------------------------------\n",
        "                # # true_positive = num_true_object\n",
        "                # # True positive  calculations\n",
        "                # y_true_test_self_imp = K.cast(y_true_true_obj_filtered[:,0], 'int32')\n",
        "                # y_pred_test_self_imp = K.cast(K.round(y_pred_true_obj_filtered[:,0]), 'int32')\n",
        "\n",
        "                # correct_preds = K.cast(K.equal(y_pred_test_self_imp, y_true_test_self_imp) , 'int32')\n",
        "                # # tf.print('correct prediction ', correct_preds)\n",
        "                # true_pos = K.cast(K.sum(correct_preds * y_true_test_self_imp) , 'int32')\n",
        "                \n",
        "                # true_positive += true_pos\n",
        "                # false_negative += K.cast(K.sum(K.cast(K.greater(y_pred_test_self_imp, y_true_test_self_imp), 'int32')), 'int32') \n",
        "\n",
        "        # tf.print('false negative',false_negative)\n",
        "        # tf.print('true_positive',true_positive)\n",
        "    \n",
        "\n",
        "        # recall_self_implemented = (K.cast(true_positive, 'float32') / (K.cast(true_positive, 'float32') + K.cast(false_negative, 'float32') + K.cast(K.epsilon(), 'float32')))\n",
        "        # tf.print(store_recall_tf)\n",
        "        if len(store_recall_tf) == 0:\n",
        "            recall = 0\n",
        "        else:\n",
        "            recall = tf.reduce_mean(store_recall_tf)\n",
        "        # tf.print('recall = ', recall)\n",
        "        self.add_update(K.update(self.recall,\n",
        "                                    recall))\n",
        "        return recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbBNVcCI6dZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39283cf3-9aae-4c1c-fd25-b6b20fa5b3ac"
      },
      "source": [
        "test_recall_callback = Recall_callback()\n",
        "test_recall_callback(y_true_test_h_l,y_pred_test_h_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.36482686>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61D_cJqN-r3k"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxlu1leK9D0T",
        "outputId": "2b4b0db2-fa5e-42c3-aa07-195c277d32f2"
      },
      "source": [
        "def add_confident_value(label):\n",
        "    reshape = tf.reshape(label,shape = (7,3))\n",
        "    store_new_values = np.empty(shape = (7,4))\n",
        "\n",
        "    for i in range(reshape.shape[0]):\n",
        "        for j in range(reshape.shape[1]):\n",
        "            store_new_values[i,j] = reshape[i,j]\n",
        "        \n",
        "        if np.isnan(reshape[i,2]):\n",
        "            # print('in if statement',reshape_test_[i,2])\n",
        "            store_new_values[i,3] = 0\n",
        "        else:\n",
        "            # print('in elif statement',reshape_test_[i,2])\n",
        "            store_new_values[i,3] = 1\n",
        "    store_new_values = tf.reshape(store_new_values,shape=(28,))\n",
        "    return store_new_values\n",
        "\n",
        "def plot_images(images):\n",
        "    assert isinstance(images, (list, tuple, np.ndarray))    \n",
        "    cols = min(6, len(images))\n",
        "    rows = 1 + (len(images)-1)//cols\n",
        "    plt.figure(figsize=(3.2*cols, 1.2*rows))\n",
        "    for n, image in enumerate(images):\n",
        "        plt.subplot(rows, cols, n+1)\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "        # plt.xlabel(f'{pred_titles[n]}', size=14, c='blue')\n",
        "        plt.imshow(image)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_dir_name(filename):\n",
        "    pos1 = filename.rfind('_')\n",
        "    pos2 = filename.find('.')\n",
        "    return filename[pos1+1:pos2], filename[0:pos1]\n",
        "\n",
        "TRAIN_SET_PATH = '/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test'\n",
        "town_listdir = os.listdir(TRAIN_SET_PATH)\n",
        "print(town_listdir)\n",
        "\n",
        "def read_rgb_image(rgb_path):\n",
        "    image = cv2.imread(rgb_path)\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    image = cv2.resize(image,(200,200))\n",
        "\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = image[0:,:,:]\n",
        "    image = image/255.0\n",
        "    return image\n",
        "\n",
        "def read_depth_image(depth_path):\n",
        "    depth_img = cv2.imread(depth_path)\n",
        "    depth_img = cv2.cvtColor(depth_img,cv2.COLOR_BGR2RGB)\n",
        "    depth_img = cv2.resize(depth_img,(200,200))\n",
        "\n",
        "    R = depth_img[:, :, 0].astype('float32')\n",
        "    G = depth_img[:, :, 1].astype('float32')\n",
        "    B = depth_img[:, :, 2].astype('float32')\n",
        "    normalized = (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1)\n",
        "    in_meters = 1000 * normalized\n",
        "    in_meters[in_meters == 1000] = 0\n",
        "    in_meters = tf.expand_dims(in_meters,axis=-1)\n",
        "    return in_meters\n",
        "\n",
        "def normalize_on_rgb_depth(concat_img):\n",
        "    # mean = [103.939, 116.779, 123.68, 1]\n",
        "    channel_avg = np.array([0.485, 0.456, 0.406])\n",
        "    channel_std = np.array([0.229, 0.224, 0.225])\n",
        "    # concat_img[:,:,0:3] = (concat_img[:,:,0:3] - channel_avg) / channel_std\n",
        "    \n",
        "    \n",
        "    concat_img_list = concat_img.numpy()\n",
        "    # print(concat_img_list.shape)\n",
        "    concat_img_list[:,:,0:3] = (concat_img_list[:,:,0:3] - channel_avg) / channel_std\n",
        "\n",
        "    concat_img_list[:,:,3][concat_img_list[:,:,3] == 0.0] = 1 # log10(1) = 0 thats what we want, not -inf\n",
        "\n",
        "    concat_img_list[:,:,3] = np.log10(concat_img_list[:,:,3])\n",
        "    concat_img_tensor = tf.convert_to_tensor(concat_img_list)\n",
        "    # concat_img_tensor.astype(np.float32)\n",
        "    # print(concat_img_tensor)\n",
        "    concat_img_tensor = tf.cast(concat_img_tensor,tf.float32)\n",
        "    return concat_img_tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Town01']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JckoguKi-uS8"
      },
      "source": [
        "# Load data train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiF8k3YqOuGN"
      },
      "source": [
        "def load_4_side_data_train():\n",
        "    def fetch_data_pair():\n",
        "        for i,(town) in enumerate(town_listdir): # use enumerate for debugging to check the loops\n",
        "            town_path = os.path.join(TRAIN_SET_PATH,town)\n",
        "\n",
        "            csv_ego_path = os.path.join(town_path,'location.csv')\n",
        "\n",
        "            csv_agent_path = os.path.join(town_path,'agent_vehicle_information.csv')\n",
        "            csv_walker_path = os.path.join(town_path,'walkers_information.csv')\n",
        "            # print(csv_ego_path,csv_agent_path)\n",
        "            rgb_path = os.path.join(town_path,'rgb_image')\n",
        "\n",
        "            front_rgb_path = os.path.join(rgb_path,'front_image')\n",
        "            \n",
        "            round_1_4 = os.listdir(front_rgb_path)\n",
        "            round_1_4.sort()\n",
        "\n",
        "            SET_TRAIN_PATH = [os.path.join(front_rgb_path,round_1_4[i]) for i in range(0,4)]\n",
        "            \n",
        "            df = pd.read_csv(csv_ego_path)\n",
        "            df_agent = pd.read_csv(csv_agent_path)\n",
        "            df_walker = pd.read_csv(csv_walker_path)\n",
        "\n",
        "            #------------------------------------------------\n",
        "            #--------FILL THE MISSING VALUES-----------------\n",
        "            #------------------------------------------------\n",
        "            df['relative_x'] = df['relative_x'].fillna(0)\n",
        "            df['relative_y'] = df['relative_y'].fillna(0)\n",
        "            df['throttle'] = df['throttle'].fillna(0)\n",
        "            df['steer'] = df['steer'].fillna(0)\n",
        "\n",
        "            df_walker['relative_x'] = df_walker['relative_x'].fillna(0)\n",
        "            df_walker['relative_y'] = df_walker['relative_y'].fillna(0)\n",
        "            # df_walker['throttle'] = df_walker['throttle'].fillna(0)\n",
        "            # df_walker['steer'] = df_walker['steer'].fillna(0)\n",
        "            #-------------------------------------------------\n",
        "\n",
        "            \n",
        "            # print('in train',os.path.isfile(csv_agent_path))\n",
        "            for round in SET_TRAIN_PATH:  \n",
        "                for subdir,_,name_images in os.walk(round):\n",
        "                    name_images.sort()\n",
        "                    # print(subdir)\n",
        "                    '''\n",
        "                    subdir: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image/round1\n",
        "                    '''\n",
        "\n",
        "                    # split the directory to get the depth directory\n",
        "                    '''\n",
        "                    head1: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image\n",
        "                    split_round: round1 -> round 4\n",
        "                    '''\n",
        "                    head1, split_round = os.path.split(subdir)\n",
        "                    \n",
        "                    '''\n",
        "                    rgb_img_path_split: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image\n",
        "                    '''\n",
        "                    rgb_img_path_split, front_img_split = os.path.split(head1)\n",
        "                    path_to_town = head1[:97]\n",
        "\n",
        "                    # print(path_to_town)\n",
        "                    left_path = 'left_image'\n",
        "                    right_path = 'right_image'\n",
        "                    back_path = 'back_image'\n",
        "                    front_path = 'front_image'\n",
        "\n",
        "\n",
        "                    path_to_depth = os.path.join(path_to_town,'depth_image')\n",
        "                    subdir_depth_front = os.path.join(path_to_depth, front_img_split)\n",
        "                    subdir_depth_round = os.path.join(subdir_depth_front, split_round)\n",
        "                    # print(subdir_depth_front)\n",
        "\n",
        "                    for name in name_images:\n",
        "                        # print(name)\n",
        "                        path_to_rgb_front_img = os.path.join(subdir,name)\n",
        "                        # print(subdir)\n",
        "\n",
        "                        # path to rgb left image\n",
        "                        path_to_rgb_left = os.path.join(rgb_img_path_split,left_path)\n",
        "                        path_to_rgb_left_img = os.path.join(path_to_rgb_left,split_round)\n",
        "                        path_to_rgb_left_img = os.path.join(path_to_rgb_left_img,name)\n",
        "                        # print(path_to_rgb_left_img)\n",
        "                        # print(os.path.isfile(path_to_rgb_left_img))\n",
        "                        # print(path_to_rgb_left_img)\n",
        "\n",
        "                        # path to rgb right image\n",
        "                        path_to_rgb_right = os.path.join(rgb_img_path_split,right_path)\n",
        "                        path_to_rgb_right_img = os.path.join(path_to_rgb_right,split_round)\n",
        "                        path_to_rgb_right_img = os.path.join(path_to_rgb_right_img,name)\n",
        "                        # print(os.path.isfile(path_to_rgb_right_img))\n",
        "\n",
        "                        # path to rgb back image\n",
        "                        path_to_rgb_back = os.path.join(rgb_img_path_split,back_path)\n",
        "                        path_to_rgb_back_img = os.path.join(path_to_rgb_back,split_round)\n",
        "                        path_to_rgb_back_img = os.path.join(path_to_rgb_back_img,name)\n",
        "                        # print(os.path.isfile(path_to_rgb_back_img))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                        # --------------------Split the path to get the path depth---------------------------\n",
        "\n",
        "                       \n",
        "                        path_to_depth_front = os.path.join(subdir_depth_round,name)\n",
        "                        # path_to_depth_front_img = os.path.join(subdir_depth_round,name)\n",
        "                        # print(os.path.isfile(path_to_depth_front))\n",
        "\n",
        "                        # split the depth path \n",
        "                        path_to_depth_split = path_to_depth_front[:109]\n",
        "                        path_from_depth_round_to_img = path_to_depth_front[122:] # round4/round4_024192.png\n",
        "                        # ----> works when rgb image has .png extension\n",
        "                        \n",
        "                        test = path_to_depth_front[122:142]\n",
        "                        path_from_depth_round_to_img = test + '.png'\n",
        "                        # print(path_from_depth_round_to_img)\n",
        "\n",
        "                        # path to depth front image\n",
        "                        path_to_depth_front = os.path.join(path_to_depth_split,front_path)\n",
        "                        path_to_depth_front_img = os.path.join(path_to_depth_front,path_from_depth_round_to_img)\n",
        "\n",
        "                        # path to depth right image\n",
        "                        path_to_depth_right = os.path.join(path_to_depth_split,right_path)\n",
        "                        path_to_depth_right_img = os.path.join(path_to_depth_right,path_from_depth_round_to_img)\n",
        "                        # print(os.path.isfile(path_to_depth_right))\n",
        "\n",
        "                        # path to depth left image\n",
        "                        path_to_depth_left = os.path.join(path_to_depth_split,left_path)\n",
        "                        path_to_depth_left_img = os.path.join(path_to_depth_left,path_from_depth_round_to_img)\n",
        "                        # print(os.path.isfile(path_to_depth_left))\n",
        "                        # print(path_to_depth_left_img)\n",
        "\n",
        "                        # path to depth back image\n",
        "                        path_to_depth_back = os.path.join(path_to_depth_split,back_path)\n",
        "                        path_to_depth_back_img = os.path.join(path_to_depth_back,path_from_depth_round_to_img)\n",
        "                        # print(os.path.isfile(path_to_depth_back))\n",
        "\n",
        "                        frame,round = get_dir_name(path_to_rgb_front_img)\n",
        "                        try:\n",
        "                            frame = int(frame)\n",
        "                        except:\n",
        "                            print('Can not convert', str ,\"to int\")\n",
        "                            print(frame, \"   \", path_to_rgb_front_img)\n",
        "\n",
        "                        # label_steer = 0\n",
        "                        # label_throttle = 0\n",
        "                        # label_velocity = 0\n",
        "\n",
        "                        #------------------------------\n",
        "                        #------create nan array--------\n",
        "                        #------------------------------\n",
        "                        label_array = np.empty((21,))\n",
        "                        label_array[:] = np.NaN\n",
        "                        label_array = np.array(label_array)\n",
        "                        #-------------------------------\n",
        "\n",
        "                        label = []\n",
        "\n",
        "                        for index,row in df.iterrows():\n",
        "                            if row['frame'] == frame:\n",
        "                                cls = 1\n",
        "                                relative_x = row['relative_x']\n",
        "                                relative_y = row['relative_y']\n",
        "                                label.append([float(relative_x), float(relative_y),int(cls)])\n",
        "                                # print('im okay')\n",
        "\n",
        "                        label_walker = []\n",
        "                    \n",
        "                        for index, row in df_walker.iterrows():\n",
        "                            if row['frame'] == frame:\n",
        "                                cls = 0\n",
        "                                relative_x = row['relative_x']\n",
        "                                relative_y = row['relative_y']\n",
        "                                label_walker.append([float(relative_x),float(relative_y),int(cls)])\n",
        "                            else:\n",
        "                                # print('searching for the correct frame of walker')\n",
        "                                # print('frame of walker',row['frame'])\n",
        "                                # print('frame of images',frame)\n",
        "                                pass\n",
        "                                # label_walker.append([float(np.nan),float(np.nan),int(np.nan)])\n",
        "\n",
        "                        #-------------------------------------------\n",
        "                        #----------TODO---------------------------\n",
        "                        #---------------------------------------\n",
        "                        # Generate again with the relative to the walkers, cannot do it in here because we do not calculate the\n",
        "                        # relative in the same loop\n",
        "\n",
        "                        # After having it, it needs simply add to the let say label_walker, the concatenate label and label_walker\n",
        "                        # then using for loop as below to add value to the NaN arrays\n",
        "                        \n",
        "\n",
        "                        label = np.array(label)\n",
        "                        label_walker = np.array(label_walker)\n",
        "                        \n",
        "                        # print(label.shape)\n",
        "                        \n",
        "\n",
        "                        # if label.shape[0] == 1 and label_agent.shape[0] == 1: # label shape from the df is (1,3). so I need to squeeze it to (3,)\n",
        "                            # print('OKE') \n",
        "                        if label.shape[0] != 0 or label_walker.shape[0] != 0:\n",
        "                            # label = tf.squeeze(label,axis=0)\n",
        "                            # label_agent = tf.squeeze(label_agent,axis=0)\n",
        "                            if label.shape[0] != 1 and len(label) != 0:\n",
        "                                # print(label)\n",
        "                                label = tf.reshape(label,shape=(1,label.shape[0]*label.shape[1]))\n",
        "                            if label_walker.shape[0] != 1 and len(label_walker) != 0:\n",
        "                                # print('label walker',label_walker)\n",
        "                                label_walker = tf.reshape(label_walker,shape=(1,label_walker.shape[0]*label_walker.shape[1]))\n",
        "                            \n",
        "\n",
        "                            # if walker and agent vehicles are seen, then it is concatenated\n",
        "                            if label.shape[0] == 1 and label_walker.shape[0] == 1:\n",
        "                                final_label = tf.concat([label, label_walker],axis=1)\n",
        "                                # print(final_label)\n",
        "                            # if only agent vehicle, then final label is the agent label\n",
        "                            elif label.shape[0] == 1 and label_walker.shape[0] == 0:\n",
        "                                final_label = label\n",
        "                            # if only walker, then final label is the walker\n",
        "                            elif label.shape[0] == 0 and label_walker.shape[0] == 1:\n",
        "                                final_label = label_walker\n",
        "\n",
        "                            label_h, label_w = final_label.shape\n",
        "                            # label_walker_h, label_walker_w = label_walker.shape\n",
        "                            # print(label_h,label_w)\n",
        "                            # final_label = np.array([final_label])\n",
        "\n",
        "                            # if the final label has more than 2 objects, then it needs to reshape to (label_h * label_w, )\n",
        "                            if label_h != 1:\n",
        "                                final_label = tf.reshape(final_label,shape=(label_h * label_w))\n",
        "                                # label = label.numpy()\n",
        "                                final_label = np.array([final_label])\n",
        "\n",
        "                   \n",
        "                            # print(label)\n",
        "\n",
        "                            # Finally, it is filled in the label_array\n",
        "                            for i in range(len(final_label[0])):\n",
        "                                label_array[i] = final_label[0][i]\n",
        "\n",
        "                            \n",
        "                            # concat_label = tf.concat([label, label_agent],axis=-1)\n",
        "\n",
        "                            if os.path.isfile(path_to_rgb_front_img) and os.path.isfile(path_to_rgb_left_img) and os.path.isfile(path_to_rgb_right_img) and os.path.isfile(path_to_rgb_back_img) and os.path.isfile(path_to_depth_front_img) and os.path.isfile(path_to_depth_left_img) and os.path.isfile(path_to_depth_right_img) and os.path.isfile(path_to_depth_back_img):\n",
        "\n",
        "                                # read rgb front images\n",
        "                                front_rgb_image = read_rgb_image(path_to_rgb_front_img)\n",
        "\n",
        "                                # read rgb left images\n",
        "                                left_rgb_image = read_rgb_image(path_to_rgb_left_img)\n",
        "\n",
        "                                # read rgb right images\n",
        "                                right_rgb_image = read_rgb_image(path_to_rgb_right_img)\n",
        "\n",
        "                                # read rgb back images\n",
        "                                back_rgb_image = read_rgb_image(path_to_rgb_back_img)\n",
        "                           \n",
        "                                # read depth front images\n",
        "                                front_depth_image = read_depth_image(path_to_depth_front_img)\n",
        "\n",
        "                                # read depth |left images\n",
        "                                left_depth_image = read_depth_image(path_to_depth_left_img)\n",
        "\n",
        "                                # read depth right images\n",
        "                                right_depth_image = read_depth_image(path_to_depth_right_img)\n",
        "\n",
        "                                # read depth back images\n",
        "                                back_depth_image = read_depth_image(path_to_depth_back_img)\n",
        "\n",
        "                                # Concatenate rgb and depth\n",
        "                                concatennate_rgb_depth_front = tf.concat([front_rgb_image,front_depth_image],axis=-1)\n",
        "                                concatennate_rgb_depth_right = tf.concat([right_rgb_image,right_depth_image],axis=-1)\n",
        "                                concatennate_rgb_depth_left = tf.concat([left_rgb_image,left_depth_image],axis=-1)\n",
        "                                concatennate_rgb_depth_back = tf.concat([back_rgb_image,back_depth_image],axis=-1)\n",
        "\n",
        "                                normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "                                normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "                                normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "                                normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)\n",
        "\n",
        "                                label_array_included_conf_val = add_confident_value(label_array)\n",
        "                                yield [normalized_front,normalized_left,normalized_right,normalized_back], label_array_included_conf_val \n",
        "                            else:\n",
        "                                # print('frame of depth image is not same to rgb image')\n",
        "                                pass                   \n",
        "                        else:\n",
        "                            pass\n",
        "                            \n",
        "    return fetch_data_pair       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MRzErCD_DZr"
      },
      "source": [
        "# Load data validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8HQK2N0_BAk"
      },
      "source": [
        "def load_4_side_data_validation():\n",
        "    def fetch_data_pair():\n",
        "        for i,(town) in enumerate(town_listdir): # use enumerate for debugging to check the loops\n",
        "            town_path = os.path.join(TRAIN_SET_PATH,town)\n",
        "\n",
        "            csv_ego_path = os.path.join(town_path,'location.csv')\n",
        "\n",
        "            csv_agent_path = os.path.join(town_path,'agent_vehicle_information.csv')\n",
        "            # print(csv_ego_path,csv_agent_path)\n",
        "            csv_walker_path = os.path.join(town_path,'walkers_information.csv')\n",
        "            rgb_path = os.path.join(town_path,'rgb_image')\n",
        "\n",
        "            front_rgb_path = os.path.join(rgb_path,'front_image')\n",
        "            \n",
        "            round_1_4 = os.listdir(front_rgb_path)\n",
        "            round_1_4.sort()\n",
        "\n",
        "            # SET_TRAIN_PATH = [os.path.join(front_rgb_path,round_1_4[i]) for i in range(0,4)]\n",
        "            SET_DEV_PATH = os.path.join(front_rgb_path,round_1_4[4])\n",
        "\n",
        "            # print(os.path.isdir(SET_DEV_PATH))\n",
        "\n",
        "            df = pd.read_csv(csv_ego_path)\n",
        "            df_walker = pd.read_csv(csv_walker_path)\n",
        "            #------------------------------------------------\n",
        "            #--------FILL THE MISSING VALUES-----------------\n",
        "            #------------------------------------------------\n",
        "            df['relative_x'] = df['relative_x'].fillna(0)\n",
        "            df['relative_y'] = df['relative_y'].fillna(0)\n",
        "            df['throttle'] = df['throttle'].fillna(0)\n",
        "            df['steer'] = df['steer'].fillna(0)\n",
        "\n",
        "            df_walker['relative_x'] = df_walker['relative_x'].fillna(0)\n",
        "            df_walker['relative_y'] = df_walker['relative_y'].fillna(0)\n",
        "            #-------------------------------------------------\n",
        "            \n",
        "            # df_agent = pd.read_csv(csv_agent_path)\n",
        "            # print('in validation',os.path.isfile(csv_agent_path))\n",
        "            \n",
        "            for subdir,_,name_images in os.walk(SET_DEV_PATH):\n",
        "                name_images.sort()\n",
        "                # print(subdir)\n",
        "                '''\n",
        "                subdir: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image/round1\n",
        "                '''\n",
        "\n",
        "                # split the directory to get the depth directory\n",
        "                '''\n",
        "                head1: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image/front_image\n",
        "                split_round: round1 -> round 4\n",
        "                '''\n",
        "                head1, split_round = os.path.split(subdir)\n",
        "                \n",
        "                '''\n",
        "                rgb_img_path_split: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town02/rgb_image\n",
        "                '''\n",
        "                rgb_img_path_split, front_img_split = os.path.split(head1)\n",
        "                path_to_town = head1[:97]\n",
        "\n",
        "                # print(path_to_town)\n",
        "                left_path = 'left_image'\n",
        "                right_path = 'right_image'\n",
        "                back_path = 'back_image'\n",
        "                front_path = 'front_image'\n",
        "\n",
        "                path_to_depth = os.path.join(path_to_town,'depth_image')\n",
        "                subdir_depth_front = os.path.join(path_to_depth, front_img_split)\n",
        "                subdir_depth_round = os.path.join(subdir_depth_front, split_round)\n",
        "                # print(subdir_depth_front)\n",
        "\n",
        "                for name in name_images:\n",
        "                    # print(name)\n",
        "                    path_to_rgb_front_img = os.path.join(subdir,name)\n",
        "                    # print(subdir)\n",
        "\n",
        "                    # path to rgb left image\n",
        "                    path_to_rgb_left = os.path.join(rgb_img_path_split,left_path)\n",
        "                    path_to_rgb_left_img = os.path.join(path_to_rgb_left,split_round)\n",
        "                    path_to_rgb_left_img = os.path.join(path_to_rgb_left_img,name)\n",
        "                    # print(os.path.isfile(path_to_rgb_left_img))\n",
        "                    # print(path_to_rgb_left_img)\n",
        "\n",
        "                    # path to rgb right image\n",
        "                    path_to_rgb_right = os.path.join(rgb_img_path_split,right_path)\n",
        "                    path_to_rgb_right_img = os.path.join(path_to_rgb_right,split_round)\n",
        "                    path_to_rgb_right_img = os.path.join(path_to_rgb_right_img,name)\n",
        "                    # print(os.path.isfile(path_to_rgb_right_img))\n",
        "\n",
        "                    # path to rgb back image\n",
        "                    path_to_rgb_back = os.path.join(rgb_img_path_split,back_path)\n",
        "                    path_to_rgb_back_img = os.path.join(path_to_rgb_back,split_round)\n",
        "                    path_to_rgb_back_img = os.path.join(path_to_rgb_back_img,name)\n",
        "                    # print(os.path.isfile(path_to_rgb_back_img))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    # path to depth front image\n",
        "                    path_to_depth_front = os.path.join(subdir_depth_round,name)\n",
        "                    \n",
        "\n",
        "                    #-----------------------------------------------------------------------------\n",
        "                    #--------------------------- split the depth path ----------------------------\n",
        "                    #-----------------------------------------------------------------------------\n",
        "                    path_to_depth_split = path_to_depth_front[:109]\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    path_from_depth_round_to_img = path_to_depth_front[122:] # round4/round4_024192.png\n",
        "                    test = path_to_depth_front[122:142]\n",
        "                    path_from_depth_round_to_img = test + '.png'\n",
        "                    # print(path_from_depth_round_to_img)\n",
        "\n",
        "                    # path to depth front image\n",
        "                    path_to_depth_front = os.path.join(path_to_depth_split,front_path)\n",
        "                    path_to_depth_front_img = os.path.join(path_to_depth_front,path_from_depth_round_to_img)\n",
        "                    # print(os.path.isfile(path_to_depth_front_img))\n",
        "                    # print(path_to_depth_front_img)\n",
        "\n",
        "                    # path to depth right image\n",
        "                    path_to_depth_right = os.path.join(path_to_depth_split,right_path)\n",
        "                    path_to_depth_right_img = os.path.join(path_to_depth_right,path_from_depth_round_to_img)\n",
        "                    # print(path_to_depth_right_img)\n",
        "                    # print('path to depth right is ',os.path.isfile(path_to_depth_right_img))\n",
        "\n",
        "                    # path to depth left image\n",
        "                    path_to_depth_left = os.path.join(path_to_depth_split,left_path)\n",
        "                    path_to_depth_left_img = os.path.join(path_to_depth_left,path_from_depth_round_to_img)\n",
        "                    # print(os.path.isfile(path_to_depth_left_img))\n",
        "                    # print(path_to_depth_left_img)\n",
        "\n",
        "                    # path to depth back image\n",
        "                    path_to_depth_back = os.path.join(path_to_depth_split,back_path)\n",
        "                    path_to_depth_back_img = os.path.join(path_to_depth_back,path_from_depth_round_to_img)\n",
        "                    # print(os.path.isfile(path_to_depth_back_img))\n",
        "\n",
        "                    # print(f'path to rgb front is {os.path.isfile(path_to_rgb_front_img)}\\n\n",
        "                    #         path to rgb left is {os.path.isfile(path_to_rgb_left_img)}\\n\n",
        "                    #         path to rgb right is {os.path.isfile(path_to_rgb_right_img)}\\n\n",
        "                    #         path to rgb back is {os.path.isfile(path_to_rgb_back_img)}\\n\n",
        "                    #         path to depth front is ')\n",
        "\n",
        "                    frame,round = get_dir_name(path_to_rgb_front_img)\n",
        "                    try:\n",
        "                        frame = int(frame)\n",
        "                    except:\n",
        "                        print('Can not convert', str ,\"to int\")\n",
        "\n",
        "                    \n",
        "                    #------------------------------\n",
        "                    #------create nan array--------\n",
        "                    #------------------------------\n",
        "                    label_array = np.empty((21,))\n",
        "                    label_array[:] = np.NaN\n",
        "                    label_array = np.array(label_array)\n",
        "                    #-------------------------------\n",
        "\n",
        "                    label = []\n",
        "\n",
        "                    for index,row in df.iterrows():\n",
        "                        if row['frame'] == frame: \n",
        "                            cls = 1                  \n",
        "                            relative_x = row['relative_x']\n",
        "                            relative_y = row['relative_y']\n",
        "                            label.append([float(relative_x), float(relative_y),int(cls)])\n",
        "                    \n",
        "\n",
        "                    label_walker = []\n",
        "                    for index, row in df_walker.iterrows():\n",
        "                        if row['frame'] == frame:\n",
        "                            cls = 0\n",
        "                            relative_x = row['relative_x']\n",
        "                            relative_y = row['relative_y']\n",
        "                            label_walker.append([float(relative_x),float(relative_y),int(cls)])\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "                    label = np.array(label)\n",
        "                    label_walker = np.array(label_walker)\n",
        "\n",
        "                    # if label.shape[0] == 1 and label_agent.shape[0] == 1:\n",
        "                    if label.shape[0] != 0 or label_walker.shape[0] != 0:\n",
        "\n",
        "                        if label.shape[0] != 1 and len(label) != 0:\n",
        "                                # print(label)\n",
        "                                label = tf.reshape(label,shape=(1,label.shape[0]*label.shape[1]))\n",
        "                        if label_walker.shape[0] != 1 and len(label_walker) != 0:\n",
        "                            # print('label walker',label_walker)\n",
        "                            label_walker = tf.reshape(label_walker,shape=(1,label_walker.shape[0]*label_walker.shape[1]))\n",
        "\n",
        "                        if label.shape[0] == 1 and label_walker.shape[0] == 1:\n",
        "                            final_label = tf.concat([label, label_walker],axis=1)\n",
        "                            # print(final_label)\n",
        "                        elif label.shape[0] == 1 and label_walker.shape[0] == 0:\n",
        "                            final_label = label\n",
        "                        elif label.shape[0] == 0 and label_walker.shape[0] == 1:\n",
        "                            final_label = label_walker\n",
        "\n",
        "                        label_h, label_w = final_label.shape\n",
        "                        # label_walker_h, label_walker_w = label_walker.shape\n",
        "                        # print(label_h,label_w)\n",
        "                        # final_label = np.array([final_label])\n",
        "                        if label_h != 1:\n",
        "                            final_label = tf.reshape(final_label,shape=(label_h * label_w))\n",
        "                            # label = label.numpy()\n",
        "                            final_label = np.array([final_label])\n",
        "\n",
        "                \n",
        "                        # print(label)\n",
        "                        for i in range(len(final_label[0])):\n",
        "                            label_array[i] = final_label[0][i]\n",
        "\n",
        "                        if os.path.isfile(path_to_rgb_front_img) and os.path.isfile(path_to_rgb_left_img) and os.path.isfile(path_to_rgb_right_img) and os.path.isfile(path_to_rgb_back_img) and os.path.isfile(path_to_depth_front_img) and os.path.isfile(path_to_depth_left_img) and os.path.isfile(path_to_depth_right_img) and os.path.isfile(path_to_depth_back_img):\n",
        "\n",
        "                            # read rgb front images\n",
        "                            front_rgb_image = read_rgb_image(path_to_rgb_front_img)\n",
        "\n",
        "                            # read rgb left images\n",
        "                            left_rgb_image = read_rgb_image(path_to_rgb_left_img)\n",
        "                            # print(path_to_rgb_left_img)\n",
        "\n",
        "                            # read rgb right images\n",
        "                            right_rgb_image = read_rgb_image(path_to_rgb_right_img)\n",
        "\n",
        "                            # read rgb back images\n",
        "                            back_rgb_image = read_rgb_image(path_to_rgb_back_img)\n",
        "                         \n",
        "                            # read depth front images\n",
        "                            front_depth_image = read_depth_image(path_to_depth_front_img)\n",
        "\n",
        "                            # read depth left images\n",
        "                            left_depth_image = read_depth_image(path_to_depth_left_img)\n",
        "\n",
        "                            # read depth right images\n",
        "                            right_depth_image = read_depth_image(path_to_depth_right_img)\n",
        "\n",
        "                            # read depth back images\n",
        "                            back_depth_image = read_depth_image(path_to_depth_back_img)\n",
        "\n",
        "                            # Concatenate rgb and depth\n",
        "                            concatennate_rgb_depth_front = tf.concat([front_rgb_image,front_depth_image],axis=-1)\n",
        "                            concatennate_rgb_depth_right = tf.concat([right_rgb_image,right_depth_image],axis=-1)\n",
        "                            concatennate_rgb_depth_left = tf.concat([left_rgb_image,left_depth_image],axis=-1)\n",
        "                            concatennate_rgb_depth_back = tf.concat([back_rgb_image,back_depth_image],axis=-1)\n",
        "\n",
        "                            normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "                            normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "                            normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "                            normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)\n",
        "\n",
        "                            label_array_included_conf_val = add_confident_value(label_array)\n",
        "                            yield [normalized_front,normalized_left,normalized_right,normalized_back], label_array_included_conf_val\n",
        "                        else:\n",
        "                            pass\n",
        "                            # print('frame of depth image is not same to rgb image') \n",
        "                            # print(path_to_rgb_front_img)                        \n",
        "                    else:\n",
        "                        pass\n",
        "                        # print('validation label is Null or has shape (2,2)')\n",
        "    return fetch_data_pair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPJjsNtm-6JT"
      },
      "source": [
        "# Load data from generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50xX2Zg19asf"
      },
      "source": [
        "IMG_SHAPE = (4,200, 200, 4)\n",
        "LABEL_SHAPE = (28,)\n",
        "OUTPUT = 28 # same as label shape\n",
        "train_set = tf.data.Dataset.from_generator(load_4_side_data_train(), \n",
        "                                           output_types=(tf.float32, tf.float64),\n",
        "                                           output_shapes=(tf.TensorShape(IMG_SHAPE), tf.TensorShape(LABEL_SHAPE)\n",
        "                                            ) )\n",
        "\n",
        "validation_set = tf.data.Dataset.from_generator(load_4_side_data_validation(), output_types=(tf.float32, tf.float64),\n",
        "                                         output_shapes=(tf.TensorShape(IMG_SHAPE), tf.TensorShape(LABEL_SHAPE)\n",
        "                                         ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kaz80cfO1dQ",
        "outputId": "97441e28-1e96-422d-8029-08e045162f32"
      },
      "source": [
        "for i, (feature,labels) in enumerate(train_set.take(60)):\n",
        "    # print(i)\n",
        "    if i % 10 ==0:\n",
        "        # print(i)\n",
        "        print(feature[0])\n",
        "        # plot_images(feature[0])\n",
        "        print(labels)\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[-0.25130573 -0.12745096  0.1127669   0.        ]\n",
            "  [-0.21705624 -0.10994396  0.14762528  0.        ]\n",
            "  [-0.19993149 -0.10994396  0.16505449  0.        ]\n",
            "  ...\n",
            "  [-0.31980476 -0.17997196  0.1127669   0.        ]\n",
            "  [-0.31980476 -0.17997196  0.1301961   0.        ]\n",
            "  [-0.3369295  -0.19747896  0.1127669   0.        ]]\n",
            "\n",
            " [[-0.23418099 -0.10994396  0.1127669   0.        ]\n",
            "  [-0.21705624 -0.10994396  0.1301961   0.        ]\n",
            "  [-0.18280673 -0.09243695  0.18248367  0.        ]\n",
            "  ...\n",
            "  [-0.25130573 -0.14495796  0.16505449  0.        ]\n",
            "  [-0.2684305  -0.14495796  0.14762528  0.        ]\n",
            "  [-0.31980476 -0.17997196  0.1301961   0.        ]]\n",
            "\n",
            " [[-0.25130573 -0.14495796  0.1127669   0.        ]\n",
            "  [-0.19993149 -0.10994396  0.14762528  0.        ]\n",
            "  [-0.18280673 -0.07492995  0.18248367  0.        ]\n",
            "  ...\n",
            "  [-0.23418099 -0.12745096  0.18248367  0.        ]\n",
            "  [-0.28555524 -0.14495796  0.14762528  0.        ]\n",
            "  [-0.35405427 -0.19747896  0.1127669   0.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.67343104  0.64285725  0.7053596   0.49784002]\n",
            "  [ 0.7590548   0.6778712   0.75764716  0.49784002]\n",
            "  [ 0.74193007  0.7303923   0.79250556  0.49784002]\n",
            "  ...\n",
            "  [ 0.63918155  0.6603643   0.6879304   0.49784824]\n",
            "  [ 0.74193007  0.74789923  0.80993474  0.49784824]\n",
            "  [ 0.77617955  0.7303923   0.80993474  0.49784824]]\n",
            "\n",
            " [[ 0.74193007  0.6603643   0.740218    0.49364895]\n",
            "  [ 0.74193007  0.64285725  0.80993474  0.49364895]\n",
            "  [ 0.74193007  0.71288526  0.7750764   0.49364895]\n",
            "  ...\n",
            "  [ 0.7933043   0.76540625  0.7750764   0.49365723]\n",
            "  [ 0.77617955  0.74789923  0.80993474  0.49365723]\n",
            "  [ 0.6905558   0.64285725  0.7750764   0.49365723]]\n",
            "\n",
            " [[ 0.74193007  0.7303923   0.7750764   0.48950088]\n",
            "  [ 0.65630627  0.64285725  0.6879304   0.48950088]\n",
            "  [ 0.7076805   0.6778712   0.6879304   0.48950088]\n",
            "  ...\n",
            "  [ 0.8275538   0.8004203   0.87965155  0.48950088]\n",
            "  [ 0.77617955  0.74789923  0.8622223   0.48950088]\n",
            "  [ 0.6905558   0.69537824  0.80993474  0.48950088]]], shape=(200, 200, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 22.25017 -19.08486   0.        1.            nan       nan       nan\n",
            "   0.            nan       nan       nan   0.            nan       nan\n",
            "       nan   0.            nan       nan       nan   0.            nan\n",
            "       nan       nan   0.            nan       nan       nan   0.     ], shape=(28,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_dm7vkJiZ8F"
      },
      "source": [
        "# Load resnet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPpJ3ttdiDIB"
      },
      "source": [
        "#identity_block\n",
        "\n",
        "def identity_block(X, f, filters, stage, block, time):\n",
        "    \"\"\"\n",
        "    Implementation of the identity block as defined in Figure 3\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch_' + f'{time}'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch_' + f'{time}'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "#convolutional_block\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "    \"\"\"\n",
        "    Implementation of the convolutional block as defined in Figure 4\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
        "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
        "    stage -- integer, used to name the layers, depending on their position in the network\n",
        "    block -- string/character, used to name the layers, depending on their position in the network\n",
        "    s -- Integer, specifying the stride to be used\n",
        "    \n",
        "    Returns:\n",
        "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s),padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(F2, (f, f), strides = (1,1), padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(F3, (1, 1), strides = (1,1), padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "\n",
        "    ##### SHORTCUT PATH ####\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X,X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def ResNet50(time, input_shape = (32, 32, 3), classes = 14):\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    # X_input = Input(input_shape)\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(input_shape)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(32, (7, 7), strides = (1, 1), name = f'conv_{time}', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = f'bn_conv_{time}')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [32, 32, 128], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='b', time = time)\n",
        "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='c', time = time)\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='b', time = time)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='c', time = time)\n",
        "    # X = identity_block(X, 3, [64, 64, 256], stage=3, block='d', time = time)\n",
        "\n",
        "    # Stage 4 \n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='b', time = time)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='c', time = time)\n",
        "    # X = identity_block(X, 3, [128, 128, 512], stage=4, block='d', time = time)\n",
        "    # X = identity_block(X, 3, [128, 128, 512], stage=4, block='e', time = time)\n",
        "    # X = identity_block(X, 3, [128, 128, 512], stage=4, block='f', time = time)\n",
        "\n",
        "    # Stage 5 \n",
        "    X = convolutional_block(X, f = 3, filters = [256,256, 1024], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='b', time = time)\n",
        "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='c', time = time)\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D(pool_size=(2,2), name=f'avg_pool_{time}')(X)\n",
        "    \n",
        "\n",
        "    # output layer\n",
        "    X = Dense(512)(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "    X = Dense(128)(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "    X = Dense(64)(X)\n",
        "    X = layers.Dropout(0.5)(X)\n",
        "    X = Dense(32)(X) # shape = (none, 12, 17, 32)\n",
        "    # X = Flatten()(X)\n",
        "    \n",
        "    # X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # # Create model\n",
        "    # model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy-pQa45ifUu"
      },
      "source": [
        "# Load ViT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdhw6dFDicsF"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, ZeroPadding2D, AveragePooling2D, Flatten\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "# TODO: add the config layer to each class, so they can be saved\n",
        "# --> done\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "# Problem when loading the model: __init__() missing 1 required positional argument projection_dim\n",
        "#--------------------------------------------------------\n",
        "#---------UNCOMMENT THIS FOR TRAINING--------------------\n",
        "#--------------------------------------------------------\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        # a = self.prj_dim\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_patches': self.num_patches,\n",
        "            'projection': self.projection,\n",
        "            'position_embedding': self.position_embedding,\n",
        "            'projection_dim': self.projection_dim   \n",
        "            })\n",
        "        return config\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------\n",
        "#---------------USE THIS FOR LOAD THE MODEL---------------------\n",
        "#---------------------------------------------------------------\n",
        "# class PatchEncoder(layers.Layer):\n",
        "#     def __init__(self, num_patches, **kwargs):\n",
        "#         super(PatchEncoder, self).__init__()\n",
        "#         self.num_patches = num_patches\n",
        "#         self.projection = layers.Dense(units=projection_dim)\n",
        "#         self.position_embedding = layers.Embedding(\n",
        "#             input_dim=num_patches, output_dim=projection_dim\n",
        "#         )\n",
        "\n",
        "#     def call(self, patch):\n",
        "#         positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "#         encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "#         return encoded\n",
        "\n",
        "#     def get_config(self):\n",
        "#         config = super().get_config().copy()\n",
        "#         config.update({\n",
        "#             'num_patches': self.num_patches,\n",
        "#             'projection': self.projection,\n",
        "#             'position_embedding': self.position_embedding \n",
        "#             })\n",
        "#         return config\n",
        "\n",
        "\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size,**kwargs):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        images = tf.image.resize(images,size=(image_size,image_size))\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'patch_size': self.patch_size,              \n",
        "            })\n",
        "        return config\n",
        "\n",
        "def ViT_feature(input, i):\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(input)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    \n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        # append_inputs.append(input)\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.2\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.2)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6,name=f'feature_layer_{i}')(encoded_patches) \n",
        "    # print(representation.shape) \n",
        "    return representation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvkrr7GiijLl"
      },
      "source": [
        "# Set ViT hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1hasBN-ihO8"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 12  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813iImfUiq5u"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9zHr9eil3o"
      },
      "source": [
        "def create_resnet_on_4_sides():\n",
        "    # shape = (None, 4, 600, 800, 4)\n",
        "    inputs = layers.Input(shape=IMG_SHAPE)\n",
        "\n",
        "    branch_outputs = []\n",
        "    \n",
        "    for i in range(0,4):\n",
        "        \n",
        "\n",
        "        # Set the clear session, keras starts a blank state at every iteration and memory consumption is constant over time\n",
        "        # tf.keras.backend.clear_session()\n",
        "\n",
        "        resnet_feature = ResNet50(input_shape=inputs[:,i,:,:,:],time=i)\n",
        "        to_vit = ViT_feature(resnet_feature, i) \n",
        "        branch_outputs.append(to_vit)\n",
        "    \n",
        "\n",
        "    # expand the dim to (none, 6336,1) just for testing\n",
        "    # branch_out_expand = []\n",
        "    # for branch_out in branch_outputs:\n",
        "    #     branch_out = tf.expand_dims(branch_out,axis=-1)\n",
        "    #     # branch_out = tf.transpose(branch_out,perm=[0,2,1])\n",
        "    #     branch_out_expand.append(branch_out)\n",
        "\n",
        "    stack_output = Concatenate()(branch_outputs) # shape = (None, 4, 256) which is 4 patches and 256 features\n",
        "    # stack_output = Add()(branch_outputs)\n",
        "    # stack_input = Concatenate()(branch_input)\n",
        "    # # print(stack_output.shape)\n",
        "    # dropout_output = layers.Dropout(0.5)(stack_output)\n",
        "    stack_output = layers.Flatten()(stack_output)\n",
        "    stack_output = layers.Dense(512)(stack_output)\n",
        "    stack_output = layers.Dropout(0.2)(stack_output)\n",
        "    stack_output = layers.Dense(128)(stack_output)\n",
        "    stack_output = layers.Dense(64)(stack_output)\n",
        "    stack_output = layers.Dense(OUTPUT)(stack_output)\n",
        "    # stack_output = layers.Reshape(target_shape=(7,4))(stack_output)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=stack_output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i_FXRU5jKEH"
      },
      "source": [
        "vit_resnet_backbone_model = create_resnet_on_4_sides()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GyJhADMcIGS",
        "outputId": "576d6955-3737-447f-c665-b491c60331ec"
      },
      "source": [
        "vit_resnet_backbone_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 4, 200, 200, 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_12 (Sl (None, 200, 200, 4)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_13 (Sl (None, 200, 200, 4)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_14 (Sl (None, 200, 200, 4)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_15 (Sl (None, 200, 200, 4)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPadding2 (None, 206, 206, 4)  0           tf.__operators__.getitem_12[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_13 (ZeroPadding2 (None, 206, 206, 4)  0           tf.__operators__.getitem_13[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_14 (ZeroPadding2 (None, 206, 206, 4)  0           tf.__operators__.getitem_14[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_15 (ZeroPadding2 (None, 206, 206, 4)  0           tf.__operators__.getitem_15[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "conv_0 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_12[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_13[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_14[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 200, 200, 32) 6304        zero_padding2d_15[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_0 (BatchNormalization)  (None, 200, 200, 32) 128         conv_0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_1 (BatchNormalization)  (None, 200, 200, 32) 128         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_2 (BatchNormalization)  (None, 200, 200, 32) 128         conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "bn_conv_3 (BatchNormalization)  (None, 200, 200, 32) 128         conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_444 (Activation)     (None, 200, 200, 32) 0           bn_conv_0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_481 (Activation)     (None, 200, 200, 32) 0           bn_conv_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_518 (Activation)     (None, 200, 200, 32) 0           bn_conv_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_555 (Activation)     (None, 200, 200, 32) 0           bn_conv_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 66, 66, 32)   0           activation_444[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 66, 66, 32)   0           activation_481[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 66, 66, 32)   0           activation_518[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 66, 66, 32)   0           activation_555[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 66, 66, 32)   1056        max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 66, 66, 32)   1056        max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_224 (Conv2D)             (None, 66, 66, 32)   1056        max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 66, 66, 32)   1056        max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_192 (BatchN (None, 66, 66, 32)   128         conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_208 (BatchN (None, 66, 66, 32)   128         conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_224 (BatchN (None, 66, 66, 32)   128         conv2d_224[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_240 (BatchN (None, 66, 66, 32)   128         conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_445 (Activation)     (None, 66, 66, 32)   0           batch_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_482 (Activation)     (None, 66, 66, 32)   0           batch_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_519 (Activation)     (None, 66, 66, 32)   0           batch_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_556 (Activation)     (None, 66, 66, 32)   0           batch_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 66, 66, 32)   9248        activation_445[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 66, 66, 32)   9248        activation_482[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_225 (Conv2D)             (None, 66, 66, 32)   9248        activation_519[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 66, 66, 32)   9248        activation_556[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_193 (BatchN (None, 66, 66, 32)   128         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_209 (BatchN (None, 66, 66, 32)   128         conv2d_209[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_225 (BatchN (None, 66, 66, 32)   128         conv2d_225[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_241 (BatchN (None, 66, 66, 32)   128         conv2d_241[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_446 (Activation)     (None, 66, 66, 32)   0           batch_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_483 (Activation)     (None, 66, 66, 32)   0           batch_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_520 (Activation)     (None, 66, 66, 32)   0           batch_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_557 (Activation)     (None, 66, 66, 32)   0           batch_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 66, 66, 128)  4224        activation_446[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 66, 66, 128)  4224        max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 66, 66, 128)  4224        activation_483[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 66, 66, 128)  4224        max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_226 (Conv2D)             (None, 66, 66, 128)  4224        activation_520[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_227 (Conv2D)             (None, 66, 66, 128)  4224        max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 66, 66, 128)  4224        activation_557[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_243 (Conv2D)             (None, 66, 66, 128)  4224        max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_194 (BatchN (None, 66, 66, 128)  512         conv2d_194[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_195 (BatchN (None, 66, 66, 128)  512         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_210 (BatchN (None, 66, 66, 128)  512         conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_211 (BatchN (None, 66, 66, 128)  512         conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_226 (BatchN (None, 66, 66, 128)  512         conv2d_226[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_227 (BatchN (None, 66, 66, 128)  512         conv2d_227[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_242 (BatchN (None, 66, 66, 128)  512         conv2d_242[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_243 (BatchN (None, 66, 66, 128)  512         conv2d_243[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_333 (Add)                   (None, 66, 66, 128)  0           batch_normalization_194[0][0]    \n",
            "                                                                 batch_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_361 (Add)                   (None, 66, 66, 128)  0           batch_normalization_210[0][0]    \n",
            "                                                                 batch_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_389 (Add)                   (None, 66, 66, 128)  0           batch_normalization_226[0][0]    \n",
            "                                                                 batch_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_417 (Add)                   (None, 66, 66, 128)  0           batch_normalization_242[0][0]    \n",
            "                                                                 batch_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_447 (Activation)     (None, 66, 66, 128)  0           add_333[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_484 (Activation)     (None, 66, 66, 128)  0           add_361[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_521 (Activation)     (None, 66, 66, 128)  0           add_389[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_558 (Activation)     (None, 66, 66, 128)  0           add_417[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_02a (Conv2D)       (None, 66, 66, 32)   4128        activation_447[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_12a (Conv2D)       (None, 66, 66, 32)   4128        activation_484[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_22a (Conv2D)       (None, 66, 66, 32)   4128        activation_521[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_32a (Conv2D)       (None, 66, 66, 32)   4128        activation_558[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_02a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_12a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_22a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_32a (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_448 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_485 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_522 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_559 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_02b (Conv2D)       (None, 66, 66, 32)   9248        activation_448[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_12b (Conv2D)       (None, 66, 66, 32)   9248        activation_485[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_22b (Conv2D)       (None, 66, 66, 32)   9248        activation_522[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_32b (Conv2D)       (None, 66, 66, 32)   9248        activation_559[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_02b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_12b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_22b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_32b (BatchNormaliza (None, 66, 66, 32)   128         res2b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_449 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_486 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_523 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_560 (Activation)     (None, 66, 66, 32)   0           bn2b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_02c (Conv2D)       (None, 66, 66, 128)  4224        activation_449[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_12c (Conv2D)       (None, 66, 66, 128)  4224        activation_486[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_22c (Conv2D)       (None, 66, 66, 128)  4224        activation_523[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2b_branch_32c (Conv2D)       (None, 66, 66, 128)  4224        activation_560[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_02c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_12c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_22c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2b_branch_32c (BatchNormaliza (None, 66, 66, 128)  512         res2b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_334 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_02c[0][0]            \n",
            "                                                                 activation_447[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_362 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_12c[0][0]            \n",
            "                                                                 activation_484[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_390 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_22c[0][0]            \n",
            "                                                                 activation_521[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_418 (Add)                   (None, 66, 66, 128)  0           bn2b_branch_32c[0][0]            \n",
            "                                                                 activation_558[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_450 (Activation)     (None, 66, 66, 128)  0           add_334[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_487 (Activation)     (None, 66, 66, 128)  0           add_362[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_524 (Activation)     (None, 66, 66, 128)  0           add_390[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_561 (Activation)     (None, 66, 66, 128)  0           add_418[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_02a (Conv2D)       (None, 66, 66, 32)   4128        activation_450[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_12a (Conv2D)       (None, 66, 66, 32)   4128        activation_487[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_22a (Conv2D)       (None, 66, 66, 32)   4128        activation_524[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_32a (Conv2D)       (None, 66, 66, 32)   4128        activation_561[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_02a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_12a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_22a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_32a (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_451 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_488 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_525 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_562 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_02b (Conv2D)       (None, 66, 66, 32)   9248        activation_451[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_12b (Conv2D)       (None, 66, 66, 32)   9248        activation_488[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_22b (Conv2D)       (None, 66, 66, 32)   9248        activation_525[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_32b (Conv2D)       (None, 66, 66, 32)   9248        activation_562[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_02b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_12b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_22b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_32b (BatchNormaliza (None, 66, 66, 32)   128         res2c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_452 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_489 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_526 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_563 (Activation)     (None, 66, 66, 32)   0           bn2c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_02c (Conv2D)       (None, 66, 66, 128)  4224        activation_452[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_12c (Conv2D)       (None, 66, 66, 128)  4224        activation_489[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_22c (Conv2D)       (None, 66, 66, 128)  4224        activation_526[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res2c_branch_32c (Conv2D)       (None, 66, 66, 128)  4224        activation_563[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_02c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_12c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_22c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn2c_branch_32c (BatchNormaliza (None, 66, 66, 128)  512         res2c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_335 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_02c[0][0]            \n",
            "                                                                 activation_450[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_363 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_12c[0][0]            \n",
            "                                                                 activation_487[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_391 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_22c[0][0]            \n",
            "                                                                 activation_524[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_419 (Add)                   (None, 66, 66, 128)  0           bn2c_branch_32c[0][0]            \n",
            "                                                                 activation_561[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_453 (Activation)     (None, 66, 66, 128)  0           add_335[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_490 (Activation)     (None, 66, 66, 128)  0           add_363[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_527 (Activation)     (None, 66, 66, 128)  0           add_391[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_564 (Activation)     (None, 66, 66, 128)  0           add_419[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 33, 33, 64)   8256        activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 33, 33, 64)   8256        activation_490[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_228 (Conv2D)             (None, 33, 33, 64)   8256        activation_527[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_244 (Conv2D)             (None, 33, 33, 64)   8256        activation_564[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_196 (BatchN (None, 33, 33, 64)   256         conv2d_196[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_212 (BatchN (None, 33, 33, 64)   256         conv2d_212[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_228 (BatchN (None, 33, 33, 64)   256         conv2d_228[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_244 (BatchN (None, 33, 33, 64)   256         conv2d_244[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_454 (Activation)     (None, 33, 33, 64)   0           batch_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_491 (Activation)     (None, 33, 33, 64)   0           batch_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_528 (Activation)     (None, 33, 33, 64)   0           batch_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_565 (Activation)     (None, 33, 33, 64)   0           batch_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 33, 33, 64)   36928       activation_454[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 33, 33, 64)   36928       activation_491[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_229 (Conv2D)             (None, 33, 33, 64)   36928       activation_528[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_245 (Conv2D)             (None, 33, 33, 64)   36928       activation_565[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_197 (BatchN (None, 33, 33, 64)   256         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_213 (BatchN (None, 33, 33, 64)   256         conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_229 (BatchN (None, 33, 33, 64)   256         conv2d_229[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_245 (BatchN (None, 33, 33, 64)   256         conv2d_245[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_455 (Activation)     (None, 33, 33, 64)   0           batch_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_492 (Activation)     (None, 33, 33, 64)   0           batch_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_529 (Activation)     (None, 33, 33, 64)   0           batch_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_566 (Activation)     (None, 33, 33, 64)   0           batch_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 33, 33, 256)  16640       activation_455[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 33, 33, 256)  33024       activation_453[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 33, 33, 256)  16640       activation_492[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 33, 33, 256)  33024       activation_490[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_230 (Conv2D)             (None, 33, 33, 256)  16640       activation_529[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_231 (Conv2D)             (None, 33, 33, 256)  33024       activation_527[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_246 (Conv2D)             (None, 33, 33, 256)  16640       activation_566[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_247 (Conv2D)             (None, 33, 33, 256)  33024       activation_564[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_198 (BatchN (None, 33, 33, 256)  1024        conv2d_198[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_199 (BatchN (None, 33, 33, 256)  1024        conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_214 (BatchN (None, 33, 33, 256)  1024        conv2d_214[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_215 (BatchN (None, 33, 33, 256)  1024        conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_230 (BatchN (None, 33, 33, 256)  1024        conv2d_230[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_231 (BatchN (None, 33, 33, 256)  1024        conv2d_231[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_246 (BatchN (None, 33, 33, 256)  1024        conv2d_246[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_247 (BatchN (None, 33, 33, 256)  1024        conv2d_247[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_336 (Add)                   (None, 33, 33, 256)  0           batch_normalization_198[0][0]    \n",
            "                                                                 batch_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_364 (Add)                   (None, 33, 33, 256)  0           batch_normalization_214[0][0]    \n",
            "                                                                 batch_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_392 (Add)                   (None, 33, 33, 256)  0           batch_normalization_230[0][0]    \n",
            "                                                                 batch_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_420 (Add)                   (None, 33, 33, 256)  0           batch_normalization_246[0][0]    \n",
            "                                                                 batch_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_456 (Activation)     (None, 33, 33, 256)  0           add_336[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_493 (Activation)     (None, 33, 33, 256)  0           add_364[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_530 (Activation)     (None, 33, 33, 256)  0           add_392[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_567 (Activation)     (None, 33, 33, 256)  0           add_420[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_02a (Conv2D)       (None, 33, 33, 64)   16448       activation_456[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_12a (Conv2D)       (None, 33, 33, 64)   16448       activation_493[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_22a (Conv2D)       (None, 33, 33, 64)   16448       activation_530[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_32a (Conv2D)       (None, 33, 33, 64)   16448       activation_567[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_02a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_12a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_22a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_32a (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_457 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_494 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_531 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_568 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_02b (Conv2D)       (None, 33, 33, 64)   36928       activation_457[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_12b (Conv2D)       (None, 33, 33, 64)   36928       activation_494[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_22b (Conv2D)       (None, 33, 33, 64)   36928       activation_531[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_32b (Conv2D)       (None, 33, 33, 64)   36928       activation_568[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_02b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_12b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_22b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_32b (BatchNormaliza (None, 33, 33, 64)   256         res3b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_458 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_495 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_532 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_569 (Activation)     (None, 33, 33, 64)   0           bn3b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_02c (Conv2D)       (None, 33, 33, 256)  16640       activation_458[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_12c (Conv2D)       (None, 33, 33, 256)  16640       activation_495[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_22c (Conv2D)       (None, 33, 33, 256)  16640       activation_532[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3b_branch_32c (Conv2D)       (None, 33, 33, 256)  16640       activation_569[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_02c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_12c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_22c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3b_branch_32c (BatchNormaliza (None, 33, 33, 256)  1024        res3b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_337 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_02c[0][0]            \n",
            "                                                                 activation_456[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_365 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_12c[0][0]            \n",
            "                                                                 activation_493[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_393 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_22c[0][0]            \n",
            "                                                                 activation_530[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_421 (Add)                   (None, 33, 33, 256)  0           bn3b_branch_32c[0][0]            \n",
            "                                                                 activation_567[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_459 (Activation)     (None, 33, 33, 256)  0           add_337[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_496 (Activation)     (None, 33, 33, 256)  0           add_365[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_533 (Activation)     (None, 33, 33, 256)  0           add_393[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_570 (Activation)     (None, 33, 33, 256)  0           add_421[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_02a (Conv2D)       (None, 33, 33, 64)   16448       activation_459[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_12a (Conv2D)       (None, 33, 33, 64)   16448       activation_496[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_22a (Conv2D)       (None, 33, 33, 64)   16448       activation_533[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_32a (Conv2D)       (None, 33, 33, 64)   16448       activation_570[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_02a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_12a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_22a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_32a (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_460 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_497 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_534 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_571 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_02b (Conv2D)       (None, 33, 33, 64)   36928       activation_460[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_12b (Conv2D)       (None, 33, 33, 64)   36928       activation_497[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_22b (Conv2D)       (None, 33, 33, 64)   36928       activation_534[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_32b (Conv2D)       (None, 33, 33, 64)   36928       activation_571[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_02b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_12b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_22b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_32b (BatchNormaliza (None, 33, 33, 64)   256         res3c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_461 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_498 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_535 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_572 (Activation)     (None, 33, 33, 64)   0           bn3c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_02c (Conv2D)       (None, 33, 33, 256)  16640       activation_461[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_12c (Conv2D)       (None, 33, 33, 256)  16640       activation_498[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_22c (Conv2D)       (None, 33, 33, 256)  16640       activation_535[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res3c_branch_32c (Conv2D)       (None, 33, 33, 256)  16640       activation_572[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_02c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_12c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_22c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn3c_branch_32c (BatchNormaliza (None, 33, 33, 256)  1024        res3c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_338 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_02c[0][0]            \n",
            "                                                                 activation_459[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_366 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_12c[0][0]            \n",
            "                                                                 activation_496[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_394 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_22c[0][0]            \n",
            "                                                                 activation_533[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_422 (Add)                   (None, 33, 33, 256)  0           bn3c_branch_32c[0][0]            \n",
            "                                                                 activation_570[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_462 (Activation)     (None, 33, 33, 256)  0           add_338[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_499 (Activation)     (None, 33, 33, 256)  0           add_366[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_536 (Activation)     (None, 33, 33, 256)  0           add_394[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_573 (Activation)     (None, 33, 33, 256)  0           add_422[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 17, 17, 128)  32896       activation_462[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 17, 17, 128)  32896       activation_499[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 17, 17, 128)  32896       activation_536[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_248 (Conv2D)             (None, 17, 17, 128)  32896       activation_573[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_200 (BatchN (None, 17, 17, 128)  512         conv2d_200[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_216 (BatchN (None, 17, 17, 128)  512         conv2d_216[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_232 (BatchN (None, 17, 17, 128)  512         conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_248 (BatchN (None, 17, 17, 128)  512         conv2d_248[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_463 (Activation)     (None, 17, 17, 128)  0           batch_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_500 (Activation)     (None, 17, 17, 128)  0           batch_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_537 (Activation)     (None, 17, 17, 128)  0           batch_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_574 (Activation)     (None, 17, 17, 128)  0           batch_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 17, 17, 128)  147584      activation_463[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_217 (Conv2D)             (None, 17, 17, 128)  147584      activation_500[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 17, 17, 128)  147584      activation_537[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_249 (Conv2D)             (None, 17, 17, 128)  147584      activation_574[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_201 (BatchN (None, 17, 17, 128)  512         conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_217 (BatchN (None, 17, 17, 128)  512         conv2d_217[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_233 (BatchN (None, 17, 17, 128)  512         conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_249 (BatchN (None, 17, 17, 128)  512         conv2d_249[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_464 (Activation)     (None, 17, 17, 128)  0           batch_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_501 (Activation)     (None, 17, 17, 128)  0           batch_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_538 (Activation)     (None, 17, 17, 128)  0           batch_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_575 (Activation)     (None, 17, 17, 128)  0           batch_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 17, 17, 512)  66048       activation_464[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 17, 17, 512)  131584      activation_462[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_218 (Conv2D)             (None, 17, 17, 512)  66048       activation_501[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_219 (Conv2D)             (None, 17, 17, 512)  131584      activation_499[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 17, 17, 512)  66048       activation_538[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 17, 17, 512)  131584      activation_536[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_250 (Conv2D)             (None, 17, 17, 512)  66048       activation_575[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_251 (Conv2D)             (None, 17, 17, 512)  131584      activation_573[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_202 (BatchN (None, 17, 17, 512)  2048        conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_203 (BatchN (None, 17, 17, 512)  2048        conv2d_203[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_218 (BatchN (None, 17, 17, 512)  2048        conv2d_218[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_219 (BatchN (None, 17, 17, 512)  2048        conv2d_219[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_234 (BatchN (None, 17, 17, 512)  2048        conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_235 (BatchN (None, 17, 17, 512)  2048        conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_250 (BatchN (None, 17, 17, 512)  2048        conv2d_250[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_251 (BatchN (None, 17, 17, 512)  2048        conv2d_251[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_339 (Add)                   (None, 17, 17, 512)  0           batch_normalization_202[0][0]    \n",
            "                                                                 batch_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_367 (Add)                   (None, 17, 17, 512)  0           batch_normalization_218[0][0]    \n",
            "                                                                 batch_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_395 (Add)                   (None, 17, 17, 512)  0           batch_normalization_234[0][0]    \n",
            "                                                                 batch_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_423 (Add)                   (None, 17, 17, 512)  0           batch_normalization_250[0][0]    \n",
            "                                                                 batch_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_465 (Activation)     (None, 17, 17, 512)  0           add_339[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_502 (Activation)     (None, 17, 17, 512)  0           add_367[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_539 (Activation)     (None, 17, 17, 512)  0           add_395[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_576 (Activation)     (None, 17, 17, 512)  0           add_423[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_02a (Conv2D)       (None, 17, 17, 128)  65664       activation_465[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_12a (Conv2D)       (None, 17, 17, 128)  65664       activation_502[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_22a (Conv2D)       (None, 17, 17, 128)  65664       activation_539[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_32a (Conv2D)       (None, 17, 17, 128)  65664       activation_576[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_02a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_12a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_22a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_32a (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_466 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_503 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_540 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_577 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_02b (Conv2D)       (None, 17, 17, 128)  147584      activation_466[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_12b (Conv2D)       (None, 17, 17, 128)  147584      activation_503[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_22b (Conv2D)       (None, 17, 17, 128)  147584      activation_540[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_32b (Conv2D)       (None, 17, 17, 128)  147584      activation_577[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_02b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_12b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_22b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_32b (BatchNormaliza (None, 17, 17, 128)  512         res4b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_467 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_504 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_541 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_578 (Activation)     (None, 17, 17, 128)  0           bn4b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_02c (Conv2D)       (None, 17, 17, 512)  66048       activation_467[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_12c (Conv2D)       (None, 17, 17, 512)  66048       activation_504[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_22c (Conv2D)       (None, 17, 17, 512)  66048       activation_541[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4b_branch_32c (Conv2D)       (None, 17, 17, 512)  66048       activation_578[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_02c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_12c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_22c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4b_branch_32c (BatchNormaliza (None, 17, 17, 512)  2048        res4b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_340 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_02c[0][0]            \n",
            "                                                                 activation_465[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_368 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_12c[0][0]            \n",
            "                                                                 activation_502[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_396 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_22c[0][0]            \n",
            "                                                                 activation_539[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_424 (Add)                   (None, 17, 17, 512)  0           bn4b_branch_32c[0][0]            \n",
            "                                                                 activation_576[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_468 (Activation)     (None, 17, 17, 512)  0           add_340[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_505 (Activation)     (None, 17, 17, 512)  0           add_368[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_542 (Activation)     (None, 17, 17, 512)  0           add_396[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_579 (Activation)     (None, 17, 17, 512)  0           add_424[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_02a (Conv2D)       (None, 17, 17, 128)  65664       activation_468[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_12a (Conv2D)       (None, 17, 17, 128)  65664       activation_505[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_22a (Conv2D)       (None, 17, 17, 128)  65664       activation_542[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_32a (Conv2D)       (None, 17, 17, 128)  65664       activation_579[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_02a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_12a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_22a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_32a (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_469 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_506 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_543 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_580 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_02b (Conv2D)       (None, 17, 17, 128)  147584      activation_469[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_12b (Conv2D)       (None, 17, 17, 128)  147584      activation_506[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_22b (Conv2D)       (None, 17, 17, 128)  147584      activation_543[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_32b (Conv2D)       (None, 17, 17, 128)  147584      activation_580[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_02b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_12b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_22b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_32b (BatchNormaliza (None, 17, 17, 128)  512         res4c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_470 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_507 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_544 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_581 (Activation)     (None, 17, 17, 128)  0           bn4c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_02c (Conv2D)       (None, 17, 17, 512)  66048       activation_470[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_12c (Conv2D)       (None, 17, 17, 512)  66048       activation_507[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_22c (Conv2D)       (None, 17, 17, 512)  66048       activation_544[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res4c_branch_32c (Conv2D)       (None, 17, 17, 512)  66048       activation_581[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_02c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_12c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_22c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn4c_branch_32c (BatchNormaliza (None, 17, 17, 512)  2048        res4c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_341 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_02c[0][0]            \n",
            "                                                                 activation_468[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_369 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_12c[0][0]            \n",
            "                                                                 activation_505[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_397 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_22c[0][0]            \n",
            "                                                                 activation_542[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_425 (Add)                   (None, 17, 17, 512)  0           bn4c_branch_32c[0][0]            \n",
            "                                                                 activation_579[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_471 (Activation)     (None, 17, 17, 512)  0           add_341[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_508 (Activation)     (None, 17, 17, 512)  0           add_369[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_545 (Activation)     (None, 17, 17, 512)  0           add_397[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_582 (Activation)     (None, 17, 17, 512)  0           add_425[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 9, 9, 256)    131328      activation_471[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_220 (Conv2D)             (None, 9, 9, 256)    131328      activation_508[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 9, 9, 256)    131328      activation_545[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_252 (Conv2D)             (None, 9, 9, 256)    131328      activation_582[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_204 (BatchN (None, 9, 9, 256)    1024        conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_220 (BatchN (None, 9, 9, 256)    1024        conv2d_220[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_236 (BatchN (None, 9, 9, 256)    1024        conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_252 (BatchN (None, 9, 9, 256)    1024        conv2d_252[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_472 (Activation)     (None, 9, 9, 256)    0           batch_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_509 (Activation)     (None, 9, 9, 256)    0           batch_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_546 (Activation)     (None, 9, 9, 256)    0           batch_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_583 (Activation)     (None, 9, 9, 256)    0           batch_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 9, 9, 256)    590080      activation_472[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_221 (Conv2D)             (None, 9, 9, 256)    590080      activation_509[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 9, 9, 256)    590080      activation_546[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_253 (Conv2D)             (None, 9, 9, 256)    590080      activation_583[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_205 (BatchN (None, 9, 9, 256)    1024        conv2d_205[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_221 (BatchN (None, 9, 9, 256)    1024        conv2d_221[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_237 (BatchN (None, 9, 9, 256)    1024        conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_253 (BatchN (None, 9, 9, 256)    1024        conv2d_253[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_473 (Activation)     (None, 9, 9, 256)    0           batch_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_510 (Activation)     (None, 9, 9, 256)    0           batch_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_547 (Activation)     (None, 9, 9, 256)    0           batch_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_584 (Activation)     (None, 9, 9, 256)    0           batch_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 9, 9, 1024)   263168      activation_473[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 9, 9, 1024)   525312      activation_471[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_222 (Conv2D)             (None, 9, 9, 1024)   263168      activation_510[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_223 (Conv2D)             (None, 9, 9, 1024)   525312      activation_508[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 9, 9, 1024)   263168      activation_547[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 9, 9, 1024)   525312      activation_545[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 9, 9, 1024)   263168      activation_584[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 9, 9, 1024)   525312      activation_582[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_206 (BatchN (None, 9, 9, 1024)   4096        conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_207 (BatchN (None, 9, 9, 1024)   4096        conv2d_207[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_222 (BatchN (None, 9, 9, 1024)   4096        conv2d_222[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_223 (BatchN (None, 9, 9, 1024)   4096        conv2d_223[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_238 (BatchN (None, 9, 9, 1024)   4096        conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_239 (BatchN (None, 9, 9, 1024)   4096        conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_254 (BatchN (None, 9, 9, 1024)   4096        conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_255 (BatchN (None, 9, 9, 1024)   4096        conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_342 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_206[0][0]    \n",
            "                                                                 batch_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_370 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_222[0][0]    \n",
            "                                                                 batch_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_398 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_238[0][0]    \n",
            "                                                                 batch_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_426 (Add)                   (None, 9, 9, 1024)   0           batch_normalization_254[0][0]    \n",
            "                                                                 batch_normalization_255[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_474 (Activation)     (None, 9, 9, 1024)   0           add_342[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_511 (Activation)     (None, 9, 9, 1024)   0           add_370[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_548 (Activation)     (None, 9, 9, 1024)   0           add_398[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_585 (Activation)     (None, 9, 9, 1024)   0           add_426[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_02a (Conv2D)       (None, 9, 9, 256)    262400      activation_474[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_12a (Conv2D)       (None, 9, 9, 256)    262400      activation_511[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_22a (Conv2D)       (None, 9, 9, 256)    262400      activation_548[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_32a (Conv2D)       (None, 9, 9, 256)    262400      activation_585[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_02a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_12a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_22a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_32a (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_475 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_512 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_549 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_586 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_02b (Conv2D)       (None, 9, 9, 256)    590080      activation_475[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_12b (Conv2D)       (None, 9, 9, 256)    590080      activation_512[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_22b (Conv2D)       (None, 9, 9, 256)    590080      activation_549[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_32b (Conv2D)       (None, 9, 9, 256)    590080      activation_586[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_02b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_12b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_22b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_32b (BatchNormaliza (None, 9, 9, 256)    1024        res5b_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_476 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_513 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_550 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_587 (Activation)     (None, 9, 9, 256)    0           bn5b_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_02c (Conv2D)       (None, 9, 9, 1024)   263168      activation_476[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_12c (Conv2D)       (None, 9, 9, 1024)   263168      activation_513[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_22c (Conv2D)       (None, 9, 9, 1024)   263168      activation_550[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5b_branch_32c (Conv2D)       (None, 9, 9, 1024)   263168      activation_587[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_02c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_12c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_22c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5b_branch_32c (BatchNormaliza (None, 9, 9, 1024)   4096        res5b_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_343 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_02c[0][0]            \n",
            "                                                                 activation_474[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_371 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_12c[0][0]            \n",
            "                                                                 activation_511[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_399 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_22c[0][0]            \n",
            "                                                                 activation_548[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_427 (Add)                   (None, 9, 9, 1024)   0           bn5b_branch_32c[0][0]            \n",
            "                                                                 activation_585[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_477 (Activation)     (None, 9, 9, 1024)   0           add_343[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_514 (Activation)     (None, 9, 9, 1024)   0           add_371[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_551 (Activation)     (None, 9, 9, 1024)   0           add_399[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_588 (Activation)     (None, 9, 9, 1024)   0           add_427[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_02a (Conv2D)       (None, 9, 9, 256)    262400      activation_477[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_12a (Conv2D)       (None, 9, 9, 256)    262400      activation_514[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_22a (Conv2D)       (None, 9, 9, 256)    262400      activation_551[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_32a (Conv2D)       (None, 9, 9, 256)    262400      activation_588[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_02a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_02a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_12a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_12a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_22a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_22a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_32a (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_32a[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_478 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_02a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_515 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_12a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_552 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_22a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_589 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_32a[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_02b (Conv2D)       (None, 9, 9, 256)    590080      activation_478[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_12b (Conv2D)       (None, 9, 9, 256)    590080      activation_515[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_22b (Conv2D)       (None, 9, 9, 256)    590080      activation_552[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_32b (Conv2D)       (None, 9, 9, 256)    590080      activation_589[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_02b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_02b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_12b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_12b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_22b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_22b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_32b (BatchNormaliza (None, 9, 9, 256)    1024        res5c_branch_32b[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_479 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_02b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_516 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_12b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_553 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_22b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_590 (Activation)     (None, 9, 9, 256)    0           bn5c_branch_32b[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_02c (Conv2D)       (None, 9, 9, 1024)   263168      activation_479[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_12c (Conv2D)       (None, 9, 9, 1024)   263168      activation_516[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_22c (Conv2D)       (None, 9, 9, 1024)   263168      activation_553[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "res5c_branch_32c (Conv2D)       (None, 9, 9, 1024)   263168      activation_590[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_02c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_02c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_12c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_12c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_22c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_22c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bn5c_branch_32c (BatchNormaliza (None, 9, 9, 1024)   4096        res5c_branch_32c[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_344 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_02c[0][0]            \n",
            "                                                                 activation_477[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_372 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_12c[0][0]            \n",
            "                                                                 activation_514[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_400 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_22c[0][0]            \n",
            "                                                                 activation_551[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_428 (Add)                   (None, 9, 9, 1024)   0           bn5c_branch_32c[0][0]            \n",
            "                                                                 activation_588[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_480 (Activation)     (None, 9, 9, 1024)   0           add_344[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_517 (Activation)     (None, 9, 9, 1024)   0           add_372[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_554 (Activation)     (None, 9, 9, 1024)   0           add_400[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_591 (Activation)     (None, 9, 9, 1024)   0           add_428[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_0 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_480[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_1 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_517[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_2 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_554[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool_3 (AveragePooling2D)   (None, 4, 4, 1024)   0           activation_591[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_257 (Dense)               (None, 4, 4, 512)    524800      avg_pool_0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_278 (Dense)               (None, 4, 4, 512)    524800      avg_pool_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_299 (Dense)               (None, 4, 4, 512)    524800      avg_pool_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_320 (Dense)               (None, 4, 4, 512)    524800      avg_pool_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_226 (Dropout)           (None, 4, 4, 512)    0           dense_257[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_245 (Dropout)           (None, 4, 4, 512)    0           dense_278[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_264 (Dropout)           (None, 4, 4, 512)    0           dense_299[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_283 (Dropout)           (None, 4, 4, 512)    0           dense_320[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_258 (Dense)               (None, 4, 4, 128)    65664       dropout_226[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_279 (Dense)               (None, 4, 4, 128)    65664       dropout_245[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_300 (Dense)               (None, 4, 4, 128)    65664       dropout_264[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_321 (Dense)               (None, 4, 4, 128)    65664       dropout_283[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_227 (Dropout)           (None, 4, 4, 128)    0           dense_258[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_246 (Dropout)           (None, 4, 4, 128)    0           dense_279[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_265 (Dropout)           (None, 4, 4, 128)    0           dense_300[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_284 (Dropout)           (None, 4, 4, 128)    0           dense_321[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_259 (Dense)               (None, 4, 4, 64)     8256        dropout_227[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_280 (Dense)               (None, 4, 4, 64)     8256        dropout_246[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_301 (Dense)               (None, 4, 4, 64)     8256        dropout_265[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_322 (Dense)               (None, 4, 4, 64)     8256        dropout_284[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_228 (Dropout)           (None, 4, 4, 64)     0           dense_259[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_247 (Dropout)           (None, 4, 4, 64)     0           dense_280[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_266 (Dropout)           (None, 4, 4, 64)     0           dense_301[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_285 (Dropout)           (None, 4, 4, 64)     0           dense_322[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_260 (Dense)               (None, 4, 4, 32)     2080        dropout_228[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_281 (Dense)               (None, 4, 4, 32)     2080        dropout_247[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_302 (Dense)               (None, 4, 4, 32)     2080        dropout_266[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_323 (Dense)               (None, 4, 4, 32)     2080        dropout_285[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "patches_12 (Patches)            (None, None, 1152)   0           dense_260[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patches_13 (Patches)            (None, None, 1152)   0           dense_281[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patches_14 (Patches)            (None, None, 1152)   0           dense_302[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patches_15 (Patches)            (None, None, 1152)   0           dense_323[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_12 (PatchEncoder) (None, 4, 64)        74048       patches_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_13 (PatchEncoder) (None, 4, 64)        74048       patches_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_14 (PatchEncoder) (None, 4, 64)        74048       patches_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "patch_encoder_15 (PatchEncoder) (None, 4, 64)        74048       patches_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_190 (LayerN (None, 4, 64)        128         patch_encoder_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_206 (LayerN (None, 4, 64)        128         patch_encoder_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_222 (LayerN (None, 4, 64)        128         patch_encoder_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_238 (LayerN (None, 4, 64)        128         patch_encoder_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_95 (MultiH (None, 4, 64)        66368       layer_normalization_190[0][0]    \n",
            "                                                                 layer_normalization_190[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_103 (Multi (None, 4, 64)        66368       layer_normalization_206[0][0]    \n",
            "                                                                 layer_normalization_206[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_111 (Multi (None, 4, 64)        66368       layer_normalization_222[0][0]    \n",
            "                                                                 layer_normalization_222[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_119 (Multi (None, 4, 64)        66368       layer_normalization_238[0][0]    \n",
            "                                                                 layer_normalization_238[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_345 (Add)                   (None, 4, 64)        0           multi_head_attention_95[0][0]    \n",
            "                                                                 patch_encoder_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_373 (Add)                   (None, 4, 64)        0           multi_head_attention_103[0][0]   \n",
            "                                                                 patch_encoder_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_401 (Add)                   (None, 4, 64)        0           multi_head_attention_111[0][0]   \n",
            "                                                                 patch_encoder_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_429 (Add)                   (None, 4, 64)        0           multi_head_attention_119[0][0]   \n",
            "                                                                 patch_encoder_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_191 (LayerN (None, 4, 64)        128         add_345[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_207 (LayerN (None, 4, 64)        128         add_373[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_223 (LayerN (None, 4, 64)        128         add_401[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_239 (LayerN (None, 4, 64)        128         add_429[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_262 (Dense)               (None, 4, 128)       8320        layer_normalization_191[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_283 (Dense)               (None, 4, 128)       8320        layer_normalization_207[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_304 (Dense)               (None, 4, 128)       8320        layer_normalization_223[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_325 (Dense)               (None, 4, 128)       8320        layer_normalization_239[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_229 (Dropout)           (None, 4, 128)       0           dense_262[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_248 (Dropout)           (None, 4, 128)       0           dense_283[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_267 (Dropout)           (None, 4, 128)       0           dense_304[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_286 (Dropout)           (None, 4, 128)       0           dense_325[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_263 (Dense)               (None, 4, 64)        8256        dropout_229[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_284 (Dense)               (None, 4, 64)        8256        dropout_248[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_305 (Dense)               (None, 4, 64)        8256        dropout_267[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_326 (Dense)               (None, 4, 64)        8256        dropout_286[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_230 (Dropout)           (None, 4, 64)        0           dense_263[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_249 (Dropout)           (None, 4, 64)        0           dense_284[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_268 (Dropout)           (None, 4, 64)        0           dense_305[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_287 (Dropout)           (None, 4, 64)        0           dense_326[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_346 (Add)                   (None, 4, 64)        0           dropout_230[0][0]                \n",
            "                                                                 add_345[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_374 (Add)                   (None, 4, 64)        0           dropout_249[0][0]                \n",
            "                                                                 add_373[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_402 (Add)                   (None, 4, 64)        0           dropout_268[0][0]                \n",
            "                                                                 add_401[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_430 (Add)                   (None, 4, 64)        0           dropout_287[0][0]                \n",
            "                                                                 add_429[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_192 (LayerN (None, 4, 64)        128         add_346[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_208 (LayerN (None, 4, 64)        128         add_374[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_224 (LayerN (None, 4, 64)        128         add_402[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_240 (LayerN (None, 4, 64)        128         add_430[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_96 (MultiH (None, 4, 64)        66368       layer_normalization_192[0][0]    \n",
            "                                                                 layer_normalization_192[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_104 (Multi (None, 4, 64)        66368       layer_normalization_208[0][0]    \n",
            "                                                                 layer_normalization_208[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_112 (Multi (None, 4, 64)        66368       layer_normalization_224[0][0]    \n",
            "                                                                 layer_normalization_224[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_120 (Multi (None, 4, 64)        66368       layer_normalization_240[0][0]    \n",
            "                                                                 layer_normalization_240[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_347 (Add)                   (None, 4, 64)        0           multi_head_attention_96[0][0]    \n",
            "                                                                 add_346[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_375 (Add)                   (None, 4, 64)        0           multi_head_attention_104[0][0]   \n",
            "                                                                 add_374[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_403 (Add)                   (None, 4, 64)        0           multi_head_attention_112[0][0]   \n",
            "                                                                 add_402[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_431 (Add)                   (None, 4, 64)        0           multi_head_attention_120[0][0]   \n",
            "                                                                 add_430[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_193 (LayerN (None, 4, 64)        128         add_347[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_209 (LayerN (None, 4, 64)        128         add_375[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_225 (LayerN (None, 4, 64)        128         add_403[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_241 (LayerN (None, 4, 64)        128         add_431[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_264 (Dense)               (None, 4, 128)       8320        layer_normalization_193[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_285 (Dense)               (None, 4, 128)       8320        layer_normalization_209[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_306 (Dense)               (None, 4, 128)       8320        layer_normalization_225[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_327 (Dense)               (None, 4, 128)       8320        layer_normalization_241[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_231 (Dropout)           (None, 4, 128)       0           dense_264[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_250 (Dropout)           (None, 4, 128)       0           dense_285[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_269 (Dropout)           (None, 4, 128)       0           dense_306[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_288 (Dropout)           (None, 4, 128)       0           dense_327[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_265 (Dense)               (None, 4, 64)        8256        dropout_231[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_286 (Dense)               (None, 4, 64)        8256        dropout_250[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_307 (Dense)               (None, 4, 64)        8256        dropout_269[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_328 (Dense)               (None, 4, 64)        8256        dropout_288[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_232 (Dropout)           (None, 4, 64)        0           dense_265[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_251 (Dropout)           (None, 4, 64)        0           dense_286[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_270 (Dropout)           (None, 4, 64)        0           dense_307[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_289 (Dropout)           (None, 4, 64)        0           dense_328[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_348 (Add)                   (None, 4, 64)        0           dropout_232[0][0]                \n",
            "                                                                 add_347[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_376 (Add)                   (None, 4, 64)        0           dropout_251[0][0]                \n",
            "                                                                 add_375[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_404 (Add)                   (None, 4, 64)        0           dropout_270[0][0]                \n",
            "                                                                 add_403[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_432 (Add)                   (None, 4, 64)        0           dropout_289[0][0]                \n",
            "                                                                 add_431[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_194 (LayerN (None, 4, 64)        128         add_348[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_210 (LayerN (None, 4, 64)        128         add_376[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_226 (LayerN (None, 4, 64)        128         add_404[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_242 (LayerN (None, 4, 64)        128         add_432[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_97 (MultiH (None, 4, 64)        66368       layer_normalization_194[0][0]    \n",
            "                                                                 layer_normalization_194[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_105 (Multi (None, 4, 64)        66368       layer_normalization_210[0][0]    \n",
            "                                                                 layer_normalization_210[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_113 (Multi (None, 4, 64)        66368       layer_normalization_226[0][0]    \n",
            "                                                                 layer_normalization_226[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_121 (Multi (None, 4, 64)        66368       layer_normalization_242[0][0]    \n",
            "                                                                 layer_normalization_242[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_349 (Add)                   (None, 4, 64)        0           multi_head_attention_97[0][0]    \n",
            "                                                                 add_348[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_377 (Add)                   (None, 4, 64)        0           multi_head_attention_105[0][0]   \n",
            "                                                                 add_376[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_405 (Add)                   (None, 4, 64)        0           multi_head_attention_113[0][0]   \n",
            "                                                                 add_404[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_433 (Add)                   (None, 4, 64)        0           multi_head_attention_121[0][0]   \n",
            "                                                                 add_432[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_195 (LayerN (None, 4, 64)        128         add_349[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_211 (LayerN (None, 4, 64)        128         add_377[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_227 (LayerN (None, 4, 64)        128         add_405[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_243 (LayerN (None, 4, 64)        128         add_433[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_266 (Dense)               (None, 4, 128)       8320        layer_normalization_195[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_287 (Dense)               (None, 4, 128)       8320        layer_normalization_211[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_308 (Dense)               (None, 4, 128)       8320        layer_normalization_227[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_329 (Dense)               (None, 4, 128)       8320        layer_normalization_243[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_233 (Dropout)           (None, 4, 128)       0           dense_266[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_252 (Dropout)           (None, 4, 128)       0           dense_287[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_271 (Dropout)           (None, 4, 128)       0           dense_308[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_290 (Dropout)           (None, 4, 128)       0           dense_329[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_267 (Dense)               (None, 4, 64)        8256        dropout_233[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_288 (Dense)               (None, 4, 64)        8256        dropout_252[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_309 (Dense)               (None, 4, 64)        8256        dropout_271[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_330 (Dense)               (None, 4, 64)        8256        dropout_290[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_234 (Dropout)           (None, 4, 64)        0           dense_267[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_253 (Dropout)           (None, 4, 64)        0           dense_288[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_272 (Dropout)           (None, 4, 64)        0           dense_309[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_291 (Dropout)           (None, 4, 64)        0           dense_330[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_350 (Add)                   (None, 4, 64)        0           dropout_234[0][0]                \n",
            "                                                                 add_349[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_378 (Add)                   (None, 4, 64)        0           dropout_253[0][0]                \n",
            "                                                                 add_377[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_406 (Add)                   (None, 4, 64)        0           dropout_272[0][0]                \n",
            "                                                                 add_405[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_434 (Add)                   (None, 4, 64)        0           dropout_291[0][0]                \n",
            "                                                                 add_433[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_196 (LayerN (None, 4, 64)        128         add_350[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_212 (LayerN (None, 4, 64)        128         add_378[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_228 (LayerN (None, 4, 64)        128         add_406[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_244 (LayerN (None, 4, 64)        128         add_434[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_98 (MultiH (None, 4, 64)        66368       layer_normalization_196[0][0]    \n",
            "                                                                 layer_normalization_196[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_106 (Multi (None, 4, 64)        66368       layer_normalization_212[0][0]    \n",
            "                                                                 layer_normalization_212[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_114 (Multi (None, 4, 64)        66368       layer_normalization_228[0][0]    \n",
            "                                                                 layer_normalization_228[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_122 (Multi (None, 4, 64)        66368       layer_normalization_244[0][0]    \n",
            "                                                                 layer_normalization_244[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_351 (Add)                   (None, 4, 64)        0           multi_head_attention_98[0][0]    \n",
            "                                                                 add_350[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_379 (Add)                   (None, 4, 64)        0           multi_head_attention_106[0][0]   \n",
            "                                                                 add_378[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_407 (Add)                   (None, 4, 64)        0           multi_head_attention_114[0][0]   \n",
            "                                                                 add_406[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_435 (Add)                   (None, 4, 64)        0           multi_head_attention_122[0][0]   \n",
            "                                                                 add_434[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_197 (LayerN (None, 4, 64)        128         add_351[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_213 (LayerN (None, 4, 64)        128         add_379[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_229 (LayerN (None, 4, 64)        128         add_407[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_245 (LayerN (None, 4, 64)        128         add_435[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_268 (Dense)               (None, 4, 128)       8320        layer_normalization_197[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_289 (Dense)               (None, 4, 128)       8320        layer_normalization_213[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_310 (Dense)               (None, 4, 128)       8320        layer_normalization_229[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_331 (Dense)               (None, 4, 128)       8320        layer_normalization_245[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_235 (Dropout)           (None, 4, 128)       0           dense_268[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_254 (Dropout)           (None, 4, 128)       0           dense_289[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_273 (Dropout)           (None, 4, 128)       0           dense_310[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_292 (Dropout)           (None, 4, 128)       0           dense_331[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_269 (Dense)               (None, 4, 64)        8256        dropout_235[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_290 (Dense)               (None, 4, 64)        8256        dropout_254[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_311 (Dense)               (None, 4, 64)        8256        dropout_273[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_332 (Dense)               (None, 4, 64)        8256        dropout_292[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_236 (Dropout)           (None, 4, 64)        0           dense_269[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_255 (Dropout)           (None, 4, 64)        0           dense_290[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_274 (Dropout)           (None, 4, 64)        0           dense_311[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_293 (Dropout)           (None, 4, 64)        0           dense_332[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_352 (Add)                   (None, 4, 64)        0           dropout_236[0][0]                \n",
            "                                                                 add_351[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_380 (Add)                   (None, 4, 64)        0           dropout_255[0][0]                \n",
            "                                                                 add_379[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_408 (Add)                   (None, 4, 64)        0           dropout_274[0][0]                \n",
            "                                                                 add_407[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_436 (Add)                   (None, 4, 64)        0           dropout_293[0][0]                \n",
            "                                                                 add_435[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_198 (LayerN (None, 4, 64)        128         add_352[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_214 (LayerN (None, 4, 64)        128         add_380[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_230 (LayerN (None, 4, 64)        128         add_408[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_246 (LayerN (None, 4, 64)        128         add_436[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_99 (MultiH (None, 4, 64)        66368       layer_normalization_198[0][0]    \n",
            "                                                                 layer_normalization_198[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_107 (Multi (None, 4, 64)        66368       layer_normalization_214[0][0]    \n",
            "                                                                 layer_normalization_214[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_115 (Multi (None, 4, 64)        66368       layer_normalization_230[0][0]    \n",
            "                                                                 layer_normalization_230[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_123 (Multi (None, 4, 64)        66368       layer_normalization_246[0][0]    \n",
            "                                                                 layer_normalization_246[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_353 (Add)                   (None, 4, 64)        0           multi_head_attention_99[0][0]    \n",
            "                                                                 add_352[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_381 (Add)                   (None, 4, 64)        0           multi_head_attention_107[0][0]   \n",
            "                                                                 add_380[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_409 (Add)                   (None, 4, 64)        0           multi_head_attention_115[0][0]   \n",
            "                                                                 add_408[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_437 (Add)                   (None, 4, 64)        0           multi_head_attention_123[0][0]   \n",
            "                                                                 add_436[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_199 (LayerN (None, 4, 64)        128         add_353[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_215 (LayerN (None, 4, 64)        128         add_381[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_231 (LayerN (None, 4, 64)        128         add_409[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_247 (LayerN (None, 4, 64)        128         add_437[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_270 (Dense)               (None, 4, 128)       8320        layer_normalization_199[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_291 (Dense)               (None, 4, 128)       8320        layer_normalization_215[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_312 (Dense)               (None, 4, 128)       8320        layer_normalization_231[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_333 (Dense)               (None, 4, 128)       8320        layer_normalization_247[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_237 (Dropout)           (None, 4, 128)       0           dense_270[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_256 (Dropout)           (None, 4, 128)       0           dense_291[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_275 (Dropout)           (None, 4, 128)       0           dense_312[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_294 (Dropout)           (None, 4, 128)       0           dense_333[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_271 (Dense)               (None, 4, 64)        8256        dropout_237[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_292 (Dense)               (None, 4, 64)        8256        dropout_256[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_313 (Dense)               (None, 4, 64)        8256        dropout_275[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_334 (Dense)               (None, 4, 64)        8256        dropout_294[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_238 (Dropout)           (None, 4, 64)        0           dense_271[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_257 (Dropout)           (None, 4, 64)        0           dense_292[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_276 (Dropout)           (None, 4, 64)        0           dense_313[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_295 (Dropout)           (None, 4, 64)        0           dense_334[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_354 (Add)                   (None, 4, 64)        0           dropout_238[0][0]                \n",
            "                                                                 add_353[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_382 (Add)                   (None, 4, 64)        0           dropout_257[0][0]                \n",
            "                                                                 add_381[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_410 (Add)                   (None, 4, 64)        0           dropout_276[0][0]                \n",
            "                                                                 add_409[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_438 (Add)                   (None, 4, 64)        0           dropout_295[0][0]                \n",
            "                                                                 add_437[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_200 (LayerN (None, 4, 64)        128         add_354[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_216 (LayerN (None, 4, 64)        128         add_382[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_232 (LayerN (None, 4, 64)        128         add_410[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_248 (LayerN (None, 4, 64)        128         add_438[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_100 (Multi (None, 4, 64)        66368       layer_normalization_200[0][0]    \n",
            "                                                                 layer_normalization_200[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_108 (Multi (None, 4, 64)        66368       layer_normalization_216[0][0]    \n",
            "                                                                 layer_normalization_216[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_116 (Multi (None, 4, 64)        66368       layer_normalization_232[0][0]    \n",
            "                                                                 layer_normalization_232[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_124 (Multi (None, 4, 64)        66368       layer_normalization_248[0][0]    \n",
            "                                                                 layer_normalization_248[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_355 (Add)                   (None, 4, 64)        0           multi_head_attention_100[0][0]   \n",
            "                                                                 add_354[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_383 (Add)                   (None, 4, 64)        0           multi_head_attention_108[0][0]   \n",
            "                                                                 add_382[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_411 (Add)                   (None, 4, 64)        0           multi_head_attention_116[0][0]   \n",
            "                                                                 add_410[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_439 (Add)                   (None, 4, 64)        0           multi_head_attention_124[0][0]   \n",
            "                                                                 add_438[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_201 (LayerN (None, 4, 64)        128         add_355[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_217 (LayerN (None, 4, 64)        128         add_383[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_233 (LayerN (None, 4, 64)        128         add_411[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_249 (LayerN (None, 4, 64)        128         add_439[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_272 (Dense)               (None, 4, 128)       8320        layer_normalization_201[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_293 (Dense)               (None, 4, 128)       8320        layer_normalization_217[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_314 (Dense)               (None, 4, 128)       8320        layer_normalization_233[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_335 (Dense)               (None, 4, 128)       8320        layer_normalization_249[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_239 (Dropout)           (None, 4, 128)       0           dense_272[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_258 (Dropout)           (None, 4, 128)       0           dense_293[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_277 (Dropout)           (None, 4, 128)       0           dense_314[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_296 (Dropout)           (None, 4, 128)       0           dense_335[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_273 (Dense)               (None, 4, 64)        8256        dropout_239[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_294 (Dense)               (None, 4, 64)        8256        dropout_258[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_315 (Dense)               (None, 4, 64)        8256        dropout_277[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_336 (Dense)               (None, 4, 64)        8256        dropout_296[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_240 (Dropout)           (None, 4, 64)        0           dense_273[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_259 (Dropout)           (None, 4, 64)        0           dense_294[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_278 (Dropout)           (None, 4, 64)        0           dense_315[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_297 (Dropout)           (None, 4, 64)        0           dense_336[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_356 (Add)                   (None, 4, 64)        0           dropout_240[0][0]                \n",
            "                                                                 add_355[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_384 (Add)                   (None, 4, 64)        0           dropout_259[0][0]                \n",
            "                                                                 add_383[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_412 (Add)                   (None, 4, 64)        0           dropout_278[0][0]                \n",
            "                                                                 add_411[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_440 (Add)                   (None, 4, 64)        0           dropout_297[0][0]                \n",
            "                                                                 add_439[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_202 (LayerN (None, 4, 64)        128         add_356[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_218 (LayerN (None, 4, 64)        128         add_384[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_234 (LayerN (None, 4, 64)        128         add_412[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_250 (LayerN (None, 4, 64)        128         add_440[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_101 (Multi (None, 4, 64)        66368       layer_normalization_202[0][0]    \n",
            "                                                                 layer_normalization_202[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_109 (Multi (None, 4, 64)        66368       layer_normalization_218[0][0]    \n",
            "                                                                 layer_normalization_218[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_117 (Multi (None, 4, 64)        66368       layer_normalization_234[0][0]    \n",
            "                                                                 layer_normalization_234[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_125 (Multi (None, 4, 64)        66368       layer_normalization_250[0][0]    \n",
            "                                                                 layer_normalization_250[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_357 (Add)                   (None, 4, 64)        0           multi_head_attention_101[0][0]   \n",
            "                                                                 add_356[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_385 (Add)                   (None, 4, 64)        0           multi_head_attention_109[0][0]   \n",
            "                                                                 add_384[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_413 (Add)                   (None, 4, 64)        0           multi_head_attention_117[0][0]   \n",
            "                                                                 add_412[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_441 (Add)                   (None, 4, 64)        0           multi_head_attention_125[0][0]   \n",
            "                                                                 add_440[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_203 (LayerN (None, 4, 64)        128         add_357[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_219 (LayerN (None, 4, 64)        128         add_385[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_235 (LayerN (None, 4, 64)        128         add_413[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_251 (LayerN (None, 4, 64)        128         add_441[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_274 (Dense)               (None, 4, 128)       8320        layer_normalization_203[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_295 (Dense)               (None, 4, 128)       8320        layer_normalization_219[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_316 (Dense)               (None, 4, 128)       8320        layer_normalization_235[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_337 (Dense)               (None, 4, 128)       8320        layer_normalization_251[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_241 (Dropout)           (None, 4, 128)       0           dense_274[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_260 (Dropout)           (None, 4, 128)       0           dense_295[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_279 (Dropout)           (None, 4, 128)       0           dense_316[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_298 (Dropout)           (None, 4, 128)       0           dense_337[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_275 (Dense)               (None, 4, 64)        8256        dropout_241[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_296 (Dense)               (None, 4, 64)        8256        dropout_260[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_317 (Dense)               (None, 4, 64)        8256        dropout_279[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_338 (Dense)               (None, 4, 64)        8256        dropout_298[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_242 (Dropout)           (None, 4, 64)        0           dense_275[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_261 (Dropout)           (None, 4, 64)        0           dense_296[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_280 (Dropout)           (None, 4, 64)        0           dense_317[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_299 (Dropout)           (None, 4, 64)        0           dense_338[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_358 (Add)                   (None, 4, 64)        0           dropout_242[0][0]                \n",
            "                                                                 add_357[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_386 (Add)                   (None, 4, 64)        0           dropout_261[0][0]                \n",
            "                                                                 add_385[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_414 (Add)                   (None, 4, 64)        0           dropout_280[0][0]                \n",
            "                                                                 add_413[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_442 (Add)                   (None, 4, 64)        0           dropout_299[0][0]                \n",
            "                                                                 add_441[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_204 (LayerN (None, 4, 64)        128         add_358[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_220 (LayerN (None, 4, 64)        128         add_386[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_236 (LayerN (None, 4, 64)        128         add_414[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_252 (LayerN (None, 4, 64)        128         add_442[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_102 (Multi (None, 4, 64)        66368       layer_normalization_204[0][0]    \n",
            "                                                                 layer_normalization_204[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_110 (Multi (None, 4, 64)        66368       layer_normalization_220[0][0]    \n",
            "                                                                 layer_normalization_220[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_118 (Multi (None, 4, 64)        66368       layer_normalization_236[0][0]    \n",
            "                                                                 layer_normalization_236[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_126 (Multi (None, 4, 64)        66368       layer_normalization_252[0][0]    \n",
            "                                                                 layer_normalization_252[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "add_359 (Add)                   (None, 4, 64)        0           multi_head_attention_102[0][0]   \n",
            "                                                                 add_358[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_387 (Add)                   (None, 4, 64)        0           multi_head_attention_110[0][0]   \n",
            "                                                                 add_386[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_415 (Add)                   (None, 4, 64)        0           multi_head_attention_118[0][0]   \n",
            "                                                                 add_414[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_443 (Add)                   (None, 4, 64)        0           multi_head_attention_126[0][0]   \n",
            "                                                                 add_442[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_205 (LayerN (None, 4, 64)        128         add_359[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_221 (LayerN (None, 4, 64)        128         add_387[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_237 (LayerN (None, 4, 64)        128         add_415[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_253 (LayerN (None, 4, 64)        128         add_443[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_276 (Dense)               (None, 4, 128)       8320        layer_normalization_205[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_297 (Dense)               (None, 4, 128)       8320        layer_normalization_221[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_318 (Dense)               (None, 4, 128)       8320        layer_normalization_237[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_339 (Dense)               (None, 4, 128)       8320        layer_normalization_253[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_243 (Dropout)           (None, 4, 128)       0           dense_276[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_262 (Dropout)           (None, 4, 128)       0           dense_297[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_281 (Dropout)           (None, 4, 128)       0           dense_318[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_300 (Dropout)           (None, 4, 128)       0           dense_339[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_277 (Dense)               (None, 4, 64)        8256        dropout_243[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_298 (Dense)               (None, 4, 64)        8256        dropout_262[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_319 (Dense)               (None, 4, 64)        8256        dropout_281[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_340 (Dense)               (None, 4, 64)        8256        dropout_300[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_244 (Dropout)           (None, 4, 64)        0           dense_277[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_263 (Dropout)           (None, 4, 64)        0           dense_298[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_282 (Dropout)           (None, 4, 64)        0           dense_319[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_301 (Dropout)           (None, 4, 64)        0           dense_340[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_360 (Add)                   (None, 4, 64)        0           dropout_244[0][0]                \n",
            "                                                                 add_359[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_388 (Add)                   (None, 4, 64)        0           dropout_263[0][0]                \n",
            "                                                                 add_387[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_416 (Add)                   (None, 4, 64)        0           dropout_282[0][0]                \n",
            "                                                                 add_415[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_444 (Add)                   (None, 4, 64)        0           dropout_301[0][0]                \n",
            "                                                                 add_443[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_0 (LayerNormaliza (None, 4, 64)        128         add_360[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_1 (LayerNormaliza (None, 4, 64)        128         add_388[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_2 (LayerNormaliza (None, 4, 64)        128         add_416[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "feature_layer_3 (LayerNormaliza (None, 4, 64)        128         add_444[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 4, 256)       0           feature_layer_0[0][0]            \n",
            "                                                                 feature_layer_1[0][0]            \n",
            "                                                                 feature_layer_2[0][0]            \n",
            "                                                                 feature_layer_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 1024)         0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_341 (Dense)               (None, 512)          524800      flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_302 (Dropout)           (None, 512)          0           dense_341[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_342 (Dense)               (None, 128)          65664       dropout_302[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_343 (Dense)               (None, 64)           8256        dense_342[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_344 (Dense)               (None, 28)           1820        dense_343[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 7, 4)         0           dense_344[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 26,024,412\n",
            "Trainable params: 25,939,676\n",
            "Non-trainable params: 84,736\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVv1duthjCV2"
      },
      "source": [
        "# Custom Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G2ByHO_2jD5z",
        "outputId": "b28258ae-32a4-44f5-d351-486106a76186"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "mae_metric = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
        "custom_mae = custom_MSE()\n",
        "for epoch in range(10):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_set.batch(16)):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            # y_batch_train = replace_nan_by_10_square_10(y_batch_train)\n",
        "            # y_batch_train = replace_nan_by_10_square_10(y_batch_train)\n",
        "\n",
        "            logits = vit_resnet_backbone_model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            # print(y_batch_train.shape)\n",
        "            # print(logits.shape)\n",
        "            \n",
        "            # Replace the relative_coor from 0 --> 2, so I can change nan --> 0 and delete it\n",
        "            # y_batch_train = replace_zero_by_two(y_batch_train)\n",
        "            # tf.print(y_batch_train.shape)\n",
        "\n",
        "            loss_value = hungarian_loss(y_batch_train, logits)\n",
        "            # loss_value = custom_mse(y_batch_train, logits)\n",
        "            \n",
        "            \n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        \n",
        "        grads = tape.gradient(loss_value, vit_resnet_backbone_model.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, vit_resnet_backbone_model.trainable_weights))\n",
        "\n",
        "        loss_tracker.update_state(loss_value)\n",
        "        custom_mae.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 4 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            # print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n",
        "            # print(\"loss\", loss_tracker.result())\n",
        "            print(\"mae\", custom_mae.result().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "\n",
            "reduce_confident_loss =  0.976069868\n",
            "\n",
            "reduce_mean_cls_loss =  1.2221148\n",
            "\n",
            "regression loss =  111.3358\n",
            "Training loss (for one batch) at step 0: 113.5340\n",
            "mae 2427.045\n",
            "\n",
            "reduce_confident_loss =  0.842420399\n",
            "\n",
            "reduce_mean_cls_loss =  0.48239851\n",
            "\n",
            "regression loss =  54.6963844\n",
            "\n",
            "reduce_confident_loss =  1.22162795\n",
            "\n",
            "reduce_mean_cls_loss =  1.30813432\n",
            "\n",
            "regression loss =  46.8039246\n",
            "\n",
            "reduce_confident_loss =  0.680973053\n",
            "\n",
            "reduce_mean_cls_loss =  0.689755738\n",
            "\n",
            "regression loss =  42.509037\n",
            "\n",
            "reduce_confident_loss =  0.707977951\n",
            "\n",
            "reduce_mean_cls_loss =  1.08896375\n",
            "\n",
            "regression loss =  39.3511543\n",
            "Training loss (for one batch) at step 4: 41.1481\n",
            "mae 2987.7078\n",
            "\n",
            "reduce_confident_loss =  0.523468852\n",
            "\n",
            "reduce_mean_cls_loss =  0.184675917\n",
            "\n",
            "regression loss =  36.0433426\n",
            "\n",
            "reduce_confident_loss =  0.461517543\n",
            "\n",
            "reduce_mean_cls_loss =  0.138658077\n",
            "\n",
            "regression loss =  48.503315\n",
            "\n",
            "reduce_confident_loss =  0.385650367\n",
            "\n",
            "reduce_mean_cls_loss =  0.156587213\n",
            "\n",
            "regression loss =  25.9488869\n",
            "\n",
            "reduce_confident_loss =  0.805320442\n",
            "\n",
            "reduce_mean_cls_loss =  0.227434427\n",
            "\n",
            "regression loss =  20.3869038\n",
            "Training loss (for one batch) at step 8: 21.4197\n",
            "mae 4576.3086\n",
            "\n",
            "reduce_confident_loss =  0.89781028\n",
            "\n",
            "reduce_mean_cls_loss =  0.0172885619\n",
            "\n",
            "regression loss =  17.4469814\n",
            "\n",
            "reduce_confident_loss =  0.75540477\n",
            "\n",
            "reduce_mean_cls_loss =  0.0553802513\n",
            "\n",
            "regression loss =  21.3034687\n",
            "\n",
            "reduce_confident_loss =  0.731685638\n",
            "\n",
            "reduce_mean_cls_loss =  0.0312977098\n",
            "\n",
            "regression loss =  20.208128\n",
            "\n",
            "reduce_confident_loss =  0.633097112\n",
            "\n",
            "reduce_mean_cls_loss =  0.0190565325\n",
            "\n",
            "regression loss =  11.7740269\n",
            "Training loss (for one batch) at step 12: 12.4262\n",
            "mae 6363.5\n",
            "\n",
            "reduce_confident_loss =  0.385985643\n",
            "\n",
            "reduce_mean_cls_loss =  0.00454351213\n",
            "\n",
            "regression loss =  15.6306887\n",
            "\n",
            "reduce_confident_loss =  0.416426182\n",
            "\n",
            "reduce_mean_cls_loss =  0.00119494309\n",
            "\n",
            "regression loss =  13.6127815\n",
            "\n",
            "reduce_confident_loss =  0.502060294\n",
            "\n",
            "reduce_mean_cls_loss =  0.00221197749\n",
            "\n",
            "regression loss =  12.4426517\n",
            "\n",
            "reduce_confident_loss =  0.401310176\n",
            "\n",
            "reduce_mean_cls_loss =  0.00132711732\n",
            "\n",
            "regression loss =  37.8058319\n",
            "Training loss (for one batch) at step 16: 38.2085\n",
            "mae 6714.492\n",
            "\n",
            "reduce_confident_loss =  0.900378883\n",
            "\n",
            "reduce_mean_cls_loss =  0.00157412165\n",
            "\n",
            "regression loss =  63.6896744\n",
            "\n",
            "reduce_confident_loss =  0.832227826\n",
            "\n",
            "reduce_mean_cls_loss =  0.000249145931\n",
            "\n",
            "regression loss =  66.5605545\n",
            "\n",
            "reduce_confident_loss =  0.473599911\n",
            "\n",
            "reduce_mean_cls_loss =  0.00100651756\n",
            "\n",
            "regression loss =  68.648262\n",
            "\n",
            "reduce_confident_loss =  0.571725428\n",
            "\n",
            "reduce_mean_cls_loss =  8.97319187e-05\n",
            "\n",
            "regression loss =  51.0781174\n",
            "Training loss (for one batch) at step 20: 51.6499\n",
            "mae 9661.858\n",
            "\n",
            "reduce_confident_loss =  0.526384234\n",
            "\n",
            "reduce_mean_cls_loss =  0.000115662115\n",
            "\n",
            "regression loss =  63.0234947\n",
            "\n",
            "reduce_confident_loss =  0.616891563\n",
            "\n",
            "reduce_mean_cls_loss =  0.000123208127\n",
            "\n",
            "regression loss =  29.6868267\n",
            "\n",
            "reduce_confident_loss =  0.863639176\n",
            "\n",
            "reduce_mean_cls_loss =  0.000218845089\n",
            "\n",
            "regression loss =  11.8188782\n",
            "\n",
            "reduce_confident_loss =  0.672427475\n",
            "\n",
            "reduce_mean_cls_loss =  7.26920553e-05\n",
            "\n",
            "regression loss =  11.582489\n",
            "Training loss (for one batch) at step 24: 12.2550\n",
            "mae 14392.957\n",
            "\n",
            "reduce_confident_loss =  0.703934848\n",
            "\n",
            "reduce_mean_cls_loss =  0.000222662857\n",
            "\n",
            "regression loss =  29.4035168\n",
            "\n",
            "reduce_confident_loss =  0.463403642\n",
            "\n",
            "reduce_mean_cls_loss =  5.19778681\n",
            "\n",
            "regression loss =  19.9877777\n",
            "\n",
            "reduce_confident_loss =  0.356396288\n",
            "\n",
            "reduce_mean_cls_loss =  5.59828854\n",
            "\n",
            "regression loss =  13.8213234\n",
            "\n",
            "reduce_confident_loss =  0.496903628\n",
            "\n",
            "reduce_mean_cls_loss =  4.96123505\n",
            "\n",
            "regression loss =  12.3626194\n",
            "Training loss (for one batch) at step 28: 17.8208\n",
            "mae 17059.484\n",
            "\n",
            "reduce_confident_loss =  0.60118711\n",
            "\n",
            "reduce_mean_cls_loss =  4.08386278\n",
            "\n",
            "regression loss =  12.5322561\n",
            "\n",
            "reduce_confident_loss =  0.684667885\n",
            "\n",
            "reduce_mean_cls_loss =  5.78844738\n",
            "\n",
            "regression loss =  14.5528088\n",
            "\n",
            "reduce_confident_loss =  0.837050617\n",
            "\n",
            "reduce_mean_cls_loss =  5.29601812\n",
            "\n",
            "regression loss =  12.3156462\n",
            "\n",
            "reduce_confident_loss =  1.86246383\n",
            "\n",
            "reduce_mean_cls_loss =  0.00190895889\n",
            "\n",
            "regression loss =  17.0331573\n",
            "Training loss (for one batch) at step 32: 18.8975\n",
            "mae 18756.846\n",
            "\n",
            "reduce_confident_loss =  1.71790564\n",
            "\n",
            "reduce_mean_cls_loss =  0.000123468344\n",
            "\n",
            "regression loss =  5.26340389\n",
            "\n",
            "reduce_confident_loss =  1.69495261\n",
            "\n",
            "reduce_mean_cls_loss =  0.000163388569\n",
            "\n",
            "regression loss =  7.70134401\n",
            "\n",
            "reduce_confident_loss =  1.75026345\n",
            "\n",
            "reduce_mean_cls_loss =  0.000365502405\n",
            "\n",
            "regression loss =  83.138916\n",
            "\n",
            "reduce_confident_loss =  1.97899282\n",
            "\n",
            "reduce_mean_cls_loss =  6.54738283\n",
            "\n",
            "regression loss =  130.416428\n",
            "Training loss (for one batch) at step 36: 138.9428\n",
            "mae 36680.45\n",
            "\n",
            "reduce_confident_loss =  2.36661506\n",
            "\n",
            "reduce_mean_cls_loss =  5.91795683\n",
            "\n",
            "regression loss =  97.8147\n",
            "\n",
            "reduce_confident_loss =  1.67447186\n",
            "\n",
            "reduce_mean_cls_loss =  5.26450682\n",
            "\n",
            "regression loss =  128.782532\n",
            "\n",
            "reduce_confident_loss =  0.982820034\n",
            "\n",
            "reduce_mean_cls_loss =  6.93791866\n",
            "\n",
            "regression loss =  124.95295\n",
            "\n",
            "reduce_confident_loss =  1.84989059\n",
            "\n",
            "reduce_mean_cls_loss =  2.91531634\n",
            "\n",
            "regression loss =  156.5504\n",
            "Training loss (for one batch) at step 40: 161.3156\n",
            "mae 70704.8\n",
            "\n",
            "reduce_confident_loss =  1.80217969\n",
            "\n",
            "reduce_mean_cls_loss =  4.07566071\n",
            "\n",
            "regression loss =  175.966171\n",
            "\n",
            "reduce_confident_loss =  2.11238098\n",
            "\n",
            "reduce_mean_cls_loss =  0.973505676\n",
            "\n",
            "regression loss =  142.04686\n",
            "\n",
            "reduce_confident_loss =  1.7215718\n",
            "\n",
            "reduce_mean_cls_loss =  0.272384107\n",
            "\n",
            "regression loss =  17.4294891\n",
            "\n",
            "reduce_confident_loss =  2.24330688\n",
            "\n",
            "reduce_mean_cls_loss =  0.608515799\n",
            "\n",
            "regression loss =  25.0373764\n",
            "Training loss (for one batch) at step 44: 27.8892\n",
            "mae 106319.38\n",
            "\n",
            "reduce_confident_loss =  1.51527429\n",
            "\n",
            "reduce_mean_cls_loss =  0.120637022\n",
            "\n",
            "regression loss =  39.0823593\n",
            "\n",
            "reduce_confident_loss =  1.55971551\n",
            "\n",
            "reduce_mean_cls_loss =  0.0318562277\n",
            "\n",
            "regression loss =  63.9033852\n",
            "\n",
            "reduce_confident_loss =  1.16942537\n",
            "\n",
            "reduce_mean_cls_loss =  2.19541192\n",
            "\n",
            "regression loss =  22.6835442\n",
            "\n",
            "reduce_confident_loss =  0.666814506\n",
            "\n",
            "reduce_mean_cls_loss =  3.36327\n",
            "\n",
            "regression loss =  17.3070335\n",
            "Training loss (for one batch) at step 48: 21.3371\n",
            "mae 116406.59\n",
            "\n",
            "reduce_confident_loss =  0.813148618\n",
            "\n",
            "reduce_mean_cls_loss =  3.85749865\n",
            "\n",
            "regression loss =  11.1714573\n",
            "\n",
            "reduce_confident_loss =  0.78067553\n",
            "\n",
            "reduce_mean_cls_loss =  3.86832452\n",
            "\n",
            "regression loss =  4.75675488\n",
            "\n",
            "reduce_confident_loss =  0.609608352\n",
            "\n",
            "reduce_mean_cls_loss =  3.230901\n",
            "\n",
            "regression loss =  3.71924114\n",
            "\n",
            "reduce_confident_loss =  0.286360532\n",
            "\n",
            "reduce_mean_cls_loss =  1.57281327\n",
            "\n",
            "regression loss =  70.3048\n",
            "Training loss (for one batch) at step 52: 72.1640\n",
            "mae 119449.17\n",
            "\n",
            "reduce_confident_loss =  0.266747773\n",
            "\n",
            "reduce_mean_cls_loss =  1.41705537\n",
            "\n",
            "regression loss =  224.274368\n",
            "\n",
            "reduce_confident_loss =  0.14302893\n",
            "\n",
            "reduce_mean_cls_loss =  2.47233701\n",
            "\n",
            "regression loss =  219.026352\n",
            "\n",
            "reduce_confident_loss =  0.14622137\n",
            "\n",
            "reduce_mean_cls_loss =  2.1927743\n",
            "\n",
            "regression loss =  211.775574\n",
            "\n",
            "reduce_confident_loss =  0.241421029\n",
            "\n",
            "reduce_mean_cls_loss =  2.00581384\n",
            "\n",
            "regression loss =  66.8459549\n",
            "Training loss (for one batch) at step 56: 69.0932\n",
            "mae 197949.58\n",
            "\n",
            "reduce_confident_loss =  0.575163245\n",
            "\n",
            "reduce_mean_cls_loss =  3.34919691\n",
            "\n",
            "regression loss =  5.31987\n",
            "\n",
            "reduce_confident_loss =  0.753256261\n",
            "\n",
            "reduce_mean_cls_loss =  5.88260412\n",
            "\n",
            "regression loss =  29.5144272\n",
            "\n",
            "reduce_confident_loss =  0.816401064\n",
            "\n",
            "reduce_mean_cls_loss =  3.64021635\n",
            "\n",
            "regression loss =  30.2835693\n",
            "\n",
            "reduce_confident_loss =  0.340971529\n",
            "\n",
            "reduce_mean_cls_loss =  0.250409067\n",
            "\n",
            "regression loss =  25.7018719\n",
            "Training loss (for one batch) at step 60: 26.2933\n",
            "mae 201546.72\n",
            "\n",
            "reduce_confident_loss =  0.217984691\n",
            "\n",
            "reduce_mean_cls_loss =  0.286083192\n",
            "\n",
            "regression loss =  60.6538391\n",
            "\n",
            "reduce_confident_loss =  0.822771549\n",
            "\n",
            "reduce_mean_cls_loss =  5.4117856\n",
            "\n",
            "regression loss =  81.6057205\n",
            "\n",
            "reduce_confident_loss =  0.346694201\n",
            "\n",
            "reduce_mean_cls_loss =  0.0600378513\n",
            "\n",
            "regression loss =  34.0233116\n",
            "\n",
            "reduce_confident_loss =  0.28537789\n",
            "\n",
            "reduce_mean_cls_loss =  0.0708296746\n",
            "\n",
            "regression loss =  19.8662815\n",
            "Training loss (for one batch) at step 64: 20.2225\n",
            "mae 206856.03\n",
            "\n",
            "reduce_confident_loss =  0.213985607\n",
            "\n",
            "reduce_mean_cls_loss =  0.0886188895\n",
            "\n",
            "regression loss =  20.080307\n",
            "\n",
            "reduce_confident_loss =  0.147486284\n",
            "\n",
            "reduce_mean_cls_loss =  0.0210594442\n",
            "\n",
            "regression loss =  19.675396\n",
            "\n",
            "reduce_confident_loss =  0.139615178\n",
            "\n",
            "reduce_mean_cls_loss =  0.0260210317\n",
            "\n",
            "regression loss =  15.4848537\n",
            "\n",
            "reduce_confident_loss =  0.16206412\n",
            "\n",
            "reduce_mean_cls_loss =  0.0289637353\n",
            "\n",
            "regression loss =  13.5923824\n",
            "Training loss (for one batch) at step 68: 13.7834\n",
            "mae 213499.0\n",
            "\n",
            "reduce_confident_loss =  0.215481296\n",
            "\n",
            "reduce_mean_cls_loss =  0.0112880366\n",
            "\n",
            "regression loss =  15.9862309\n",
            "\n",
            "reduce_confident_loss =  0.122226156\n",
            "\n",
            "reduce_mean_cls_loss =  0.00756816426\n",
            "\n",
            "regression loss =  9.79985905\n",
            "\n",
            "reduce_confident_loss =  0.480990171\n",
            "\n",
            "reduce_mean_cls_loss =  3.12101102\n",
            "\n",
            "regression loss =  15.3352528\n",
            "last batches dont have enough values to reshape. Temporary set it to 0\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fc028f63aa89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# the gradients of the trainable variables with respect to the loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvit_resnet_backbone_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Run one step of gradient descent by updating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0mflat_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsTrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         logging.vlog(\n\u001b[1;32m   1058\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The dtype of the target tensor must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop_util.py\u001b[0m in \u001b[0;36mIsTrainable\u001b[0;34m(tensor_or_dtype)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_or_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   return dtype.base_dtype in (dtypes.float16, dtypes.float32, dtypes.float64,\n\u001b[1;32m     60\u001b[0m                               \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m   raise TypeError(\"Cannot convert value %r to a TensorFlow DType.\" %\n\u001b[0;32m--> 722\u001b[0;31m                   (type_value,))\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert value 0 to a TensorFlow DType."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAH8PkhRSVV3"
      },
      "source": [
        "# Built-in training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1AFs-wmKrV"
      },
      "source": [
        "def run_experiment(model, history_ = None):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "    \n",
        "    # 'mean_squared_error'\n",
        "    metrics = [custom_MSE(),Recall_callback()]\n",
        "    model.compile(loss= hungarian_loss_fit, optimizer=tf.keras.optimizers.Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
        "                  metrics=metrics)\n",
        "\n",
        "    checkpoint_filepath = \"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/test_check_point\"\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "    # lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "    reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.00001)\n",
        "\n",
        "    callbacks = [checkpoint_callback, early_stopping_callback, reduce_lr_callback]\n",
        "\n",
        "    # use recall in callback\n",
        "    # myCallBack = tf.keras.callbacks.LambdaCallback(on_epoch_end=Recall_callback())\n",
        "\n",
        "    if history_ == 'use_generator':\n",
        "        history = model.fit(\n",
        "                    train_set.batch(16),\n",
        "                    validation_data=validation_set.batch(16),\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    shuffle=True, callbacks = callbacks )\n",
        "    elif history_ == 'use_fit':\n",
        "        history = model.fit(x=concat_rgb_depth_train,y=np.array(label),batch_size=10,epochs=10,verbose=1,shuffle=True)\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZhvD64jFp-a"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwWQ-qtYxTio"
      },
      "source": [
        "# Using with the recall metric to avoid error tf.function-decorated function tried to create variables on non-first call.\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8xnEm0ioScCS",
        "outputId": "27af5aaf-4cfe-45f1-e474-fd8804462755"
      },
      "source": [
        "history = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\n",
            "reduce_confident_loss =  0.177656204\n",
            "\n",
            "reduce_mean_cls_loss =  0.137777954\n",
            "\n",
            "regression loss =  4.69291687\n",
            "      1/Unknown - 42s 42s/step - loss: 5.0084 - custom_mse: 154.2410 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.607314944\n",
            "\n",
            "reduce_mean_cls_loss =  0.913956106\n",
            "\n",
            "regression loss =  9.29787064\n",
            "      2/Unknown - 79s 37s/step - loss: 7.9137 - custom_mse: 115.1843 - recall: 0.1729    \n",
            "reduce_confident_loss =  0.602571607\n",
            "\n",
            "reduce_mean_cls_loss =  0.669356465\n",
            "\n",
            "regression loss =  4.0135088\n",
            "      3/Unknown - 114s 36s/step - loss: 7.0376 - custom_mse: 124.2325 - recall: 0.1361\n",
            "reduce_confident_loss =  0.11576508\n",
            "\n",
            "reduce_mean_cls_loss =  0.431027949\n",
            "\n",
            "regression loss =  5.34008503\n",
            "      4/Unknown - 148s 35s/step - loss: 6.7500 - custom_mse: 155.3518 - recall: 0.1021\n",
            "reduce_confident_loss =  0.386350304\n",
            "\n",
            "reduce_mean_cls_loss =  0.152152389\n",
            "\n",
            "regression loss =  8.1292038\n",
            "      5/Unknown - 182s 35s/step - loss: 7.1335 - custom_mse: 105.1364 - recall: 0.0817\n",
            "reduce_confident_loss =  0.190953329\n",
            "\n",
            "reduce_mean_cls_loss =  0.221101046\n",
            "\n",
            "regression loss =  9.55687904\n",
            "      6/Unknown - 216s 35s/step - loss: 7.6061 - custom_mse: 76.4513 - recall: 0.0680 \n",
            "reduce_confident_loss =  0.101693757\n",
            "\n",
            "reduce_mean_cls_loss =  0.253478765\n",
            "\n",
            "regression loss =  10.7494049\n",
            "      7/Unknown - 249s 34s/step - loss: 8.1059 - custom_mse: 60.1297 - recall: 0.0583\n",
            "reduce_confident_loss =  0.25442788\n",
            "\n",
            "reduce_mean_cls_loss =  0.323151737\n",
            "\n",
            "regression loss =  15.9563484\n",
            "      8/Unknown - 282s 34s/step - loss: 9.1594 - custom_mse: 68.1380 - recall: 0.0510\n",
            "reduce_confident_loss =  0.425000608\n",
            "\n",
            "reduce_mean_cls_loss =  1.29360521\n",
            "\n",
            "regression loss =  15.3751278\n",
            "      9/Unknown - 316s 34s/step - loss: 10.0410 - custom_mse: 112.9742 - recall: 0.0454\n",
            "reduce_confident_loss =  0.394117504\n",
            "\n",
            "reduce_mean_cls_loss =  0.944439054\n",
            "\n",
            "regression loss =  9.89846134\n",
            "     10/Unknown - 381s 38s/step - loss: 10.1606 - custom_mse: 97.7496 - recall: 0.0408 \n",
            "reduce_confident_loss =  0.490692\n",
            "\n",
            "reduce_mean_cls_loss =  0.458549768\n",
            "\n",
            "regression loss =  8.74482\n",
            "     11/Unknown - 417s 37s/step - loss: 10.1182 - custom_mse: 13.3460 - recall: 0.1236\n",
            "reduce_confident_loss =  0.675676227\n",
            "\n",
            "reduce_mean_cls_loss =  0.981627047\n",
            "\n",
            "regression loss =  13.4932575\n",
            "     12/Unknown - 453s 37s/step - loss: 10.5375 - custom_mse: 23.9304 - recall: 0.1591\n",
            "reduce_confident_loss =  0.527875245\n",
            "\n",
            "reduce_mean_cls_loss =  0.659931064\n",
            "\n",
            "regression loss =  14.8099327\n",
            "     13/Unknown - 488s 37s/step - loss: 10.9575 - custom_mse: 103.6661 - recall: 0.1746\n",
            "reduce_confident_loss =  0.629018307\n",
            "\n",
            "reduce_mean_cls_loss =  1.47864747\n",
            "\n",
            "regression loss =  12.3276281\n",
            "     14/Unknown - 523s 37s/step - loss: 11.2060 - custom_mse: 102.1935 - recall: 0.1914\n",
            "reduce_confident_loss =  0.569809854\n",
            "\n",
            "reduce_mean_cls_loss =  0.7543208\n",
            "\n",
            "regression loss =  14.3451853\n",
            "     15/Unknown - 557s 37s/step - loss: 11.5035 - custom_mse: 95.9885 - recall: 0.2153 \n",
            "reduce_confident_loss =  0.272438437\n",
            "\n",
            "reduce_mean_cls_loss =  0.521823883\n",
            "\n",
            "regression loss =  11.320962\n",
            "     16/Unknown - 593s 37s/step - loss: 11.5417 - custom_mse: 86.0764 - recall: 0.2289\n",
            "reduce_confident_loss =  0.701513827\n",
            "\n",
            "reduce_mean_cls_loss =  0.482928425\n",
            "\n",
            "regression loss =  14.4792166\n",
            "     17/Unknown - 627s 37s/step - loss: 11.7842 - custom_mse: 75.3732 - recall: 0.2154\n",
            "reduce_confident_loss =  0.761385083\n",
            "\n",
            "reduce_mean_cls_loss =  0.119519591\n",
            "\n",
            "regression loss =  14.8635778\n",
            "     18/Unknown - 669s 37s/step - loss: 12.0042 - custom_mse: 64.8584 - recall: 0.2035\n",
            "reduce_confident_loss =  0.504360497\n",
            "\n",
            "reduce_mean_cls_loss =  0.150079757\n",
            "\n",
            "regression loss =  16.2825813\n",
            "     19/Unknown - 706s 37s/step - loss: 12.2638 - custom_mse: 94.7893 - recall: 0.1928\n",
            "reduce_confident_loss =  0.3437635\n",
            "\n",
            "reduce_mean_cls_loss =  0.255246401\n",
            "\n",
            "regression loss =  14.888586\n",
            "     20/Unknown - 742s 37s/step - loss: 12.4250 - custom_mse: 140.5481 - recall: 0.2206\n",
            "reduce_confident_loss =  0.223959953\n",
            "\n",
            "reduce_mean_cls_loss =  0.271261752\n",
            "\n",
            "regression loss =  6.54526806\n",
            "     21/Unknown - 777s 37s/step - loss: 12.1686 - custom_mse: 98.6105 - recall: 0.2101 \n",
            "reduce_confident_loss =  0.194137856\n",
            "\n",
            "reduce_mean_cls_loss =  0.0168393087\n",
            "\n",
            "regression loss =  6.41581\n",
            "     22/Unknown - 811s 37s/step - loss: 11.9167 - custom_mse: 80.5098 - recall: 0.2006\n",
            "reduce_confident_loss =  0.477755\n",
            "\n",
            "reduce_mean_cls_loss =  0.193850726\n",
            "\n",
            "regression loss =  7.66465569\n",
            "     23/Unknown - 883s 38s/step - loss: 11.7611 - custom_mse: 81.0943 - recall: 0.1918\n",
            "reduce_confident_loss =  0.51002264\n",
            "\n",
            "reduce_mean_cls_loss =  0.0706671625\n",
            "\n",
            "regression loss =  6.54473495\n",
            "     24/Unknown - 961s 40s/step - loss: 11.5679 - custom_mse: 63.6764 - recall: 0.1838\n",
            "reduce_confident_loss =  0.763168275\n",
            "\n",
            "reduce_mean_cls_loss =  0.502185404\n",
            "\n",
            "regression loss =  7.70733\n",
            "     25/Unknown - 1031s 41s/step - loss: 11.4641 - custom_mse: 58.0196 - recall: 0.2165\n",
            "reduce_confident_loss =  1.31250644\n",
            "\n",
            "reduce_mean_cls_loss =  0.918377578\n",
            "\n",
            "regression loss =  8.44942665\n",
            "     26/Unknown - 1105s 43s/step - loss: 11.4339 - custom_mse: 32.5185 - recall: 0.2466\n",
            "reduce_confident_loss =  1.13718486\n",
            "\n",
            "reduce_mean_cls_loss =  0.952113926\n",
            "\n",
            "regression loss =  10.4251261\n",
            "     27/Unknown - 1174s 44s/step - loss: 11.4740 - custom_mse: 68.7634 - recall: 0.2678\n",
            "reduce_confident_loss =  0.664295375\n",
            "\n",
            "reduce_mean_cls_loss =  0.473202646\n",
            "\n",
            "regression loss =  11.2947187\n",
            "     28/Unknown - 1268s 45s/step - loss: 11.5082 - custom_mse: 94.7426 - recall: 0.2717\n",
            "reduce_confident_loss =  0.664560914\n",
            "\n",
            "reduce_mean_cls_loss =  0.0142427618\n",
            "\n",
            "regression loss =  14.3020163\n",
            "     29/Unknown - 1345s 47s/step - loss: 11.6279 - custom_mse: 163.6308 - recall: 0.2623\n",
            "reduce_confident_loss =  0.0989739075\n",
            "\n",
            "reduce_mean_cls_loss =  2.11480832\n",
            "\n",
            "regression loss =  12.52071\n",
            "     30/Unknown - 1415s 47s/step - loss: 11.7315 - custom_mse: 166.6728 - recall: 0.2869\n",
            "reduce_confident_loss =  0.0735529661\n",
            "\n",
            "reduce_mean_cls_loss =  2.91083479\n",
            "\n",
            "regression loss =  9.39399815\n",
            "     31/Unknown - 1481s 48s/step - loss: 11.7524 - custom_mse: 114.9807 - recall: 0.2776\n",
            "reduce_confident_loss =  0.350307912\n",
            "\n",
            "reduce_mean_cls_loss =  4.75363159\n",
            "\n",
            "regression loss =  12.5356169\n",
            "     32/Unknown - 1549s 49s/step - loss: 11.9363 - custom_mse: 187.0100 - recall: 0.2689\n",
            "reduce_confident_loss =  0.111098506\n",
            "\n",
            "reduce_mean_cls_loss =  4.08766603\n",
            "\n",
            "regression loss =  11.8291368\n",
            "     33/Unknown - 1618s 49s/step - loss: 12.0603 - custom_mse: 183.6168 - recall: 0.2608\n",
            "reduce_confident_loss =  0.122848973\n",
            "\n",
            "reduce_mean_cls_loss =  3.89082813\n",
            "\n",
            "regression loss =  8.65076828\n",
            "     34/Unknown - 1663s 49s/step - loss: 12.0715 - custom_mse: 136.9022 - recall: 0.2531\n",
            "reduce_confident_loss =  0.651655734\n",
            "\n",
            "reduce_mean_cls_loss =  3.80267072\n",
            "\n",
            "regression loss =  16.0835018\n",
            "\n",
            "reduce_confident_loss =  0.128558308\n",
            "\n",
            "reduce_mean_cls_loss =  1.04836071\n",
            "\n",
            "regression loss =  12.1768732\n",
            "\n",
            "reduce_confident_loss =  0.656415403\n",
            "\n",
            "reduce_mean_cls_loss =  3.7349472\n",
            "\n",
            "regression loss =  14.298666\n",
            "\n",
            "reduce_confident_loss =  0.648584485\n",
            "\n",
            "reduce_mean_cls_loss =  0.0254222881\n",
            "\n",
            "regression loss =  14.881897\n",
            "\n",
            "reduce_confident_loss =  0.27856791\n",
            "\n",
            "reduce_mean_cls_loss =  1.8090055\n",
            "\n",
            "regression loss =  11.9643965\n",
            "\n",
            "reduce_confident_loss =  0.64820534\n",
            "\n",
            "reduce_mean_cls_loss =  3.67421508\n",
            "\n",
            "regression loss =  11.3098488\n",
            "\n",
            "reduce_confident_loss =  0.645940781\n",
            "\n",
            "reduce_mean_cls_loss =  3.65843248\n",
            "\n",
            "regression loss =  13.8237257\n",
            "\n",
            "reduce_confident_loss =  0.651234746\n",
            "\n",
            "reduce_mean_cls_loss =  3.68181872\n",
            "\n",
            "regression loss =  13.2908821\n",
            "\n",
            "reduce_confident_loss =  0.501151383\n",
            "\n",
            "reduce_mean_cls_loss =  2.91401935\n",
            "\n",
            "regression loss =  13.7387581\n",
            "\n",
            "reduce_confident_loss =  0.126845509\n",
            "\n",
            "reduce_mean_cls_loss =  1.06287503\n",
            "\n",
            "regression loss =  17.7530594\n",
            "\n",
            "reduce_confident_loss =  0.12840879\n",
            "\n",
            "reduce_mean_cls_loss =  0.914594114\n",
            "\n",
            "regression loss =  13.0792713\n",
            "\n",
            "reduce_confident_loss =  0.653547287\n",
            "\n",
            "reduce_mean_cls_loss =  3.69886899\n",
            "\n",
            "regression loss =  19.6486168\n",
            "\n",
            "reduce_confident_loss =  0.653674662\n",
            "\n",
            "reduce_mean_cls_loss =  0.0252863411\n",
            "\n",
            "regression loss =  20.0761642\n",
            "34/34 [==============================] - 2312s 69s/step - loss: 12.0715 - custom_mse: 136.9022 - recall: 0.2531 - val_loss: 17.4371 - val_custom_mse: 110.3197 - val_recall: 0.0705\n",
            "Epoch 2/10\n",
            "\n",
            "reduce_confident_loss =  0.315468103\n",
            "\n",
            "reduce_mean_cls_loss =  1.10031629\n",
            "\n",
            "regression loss =  12.2277098\n",
            " 1/34 [..............................] - ETA: 19:56 - loss: 13.6435 - custom_mse: 256.3106 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.330614656\n",
            "\n",
            "reduce_mean_cls_loss =  0.442900836\n",
            "\n",
            "regression loss =  10.1976299\n",
            " 2/34 [>.............................] - ETA: 18:21 - loss: 12.3073 - custom_mse: 64.0465 - recall: 0.3808     \n",
            "reduce_confident_loss =  0.311637461\n",
            "\n",
            "reduce_mean_cls_loss =  0.743010879\n",
            "\n",
            "regression loss =  9.52851295\n",
            " 3/34 [=>............................] - ETA: 17:54 - loss: 11.7326 - custom_mse: 135.5580 - recall: 0.4938\n",
            "reduce_confident_loss =  0.247699767\n",
            "\n",
            "reduce_mean_cls_loss =  0.459870607\n",
            "\n",
            "regression loss =  8.37383\n",
            " 4/34 [==>...........................] - ETA: 17:28 - loss: 11.0698 - custom_mse: 167.1129 - recall: 0.3704\n",
            "reduce_confident_loss =  0.133100554\n",
            "\n",
            "reduce_mean_cls_loss =  0.105575971\n",
            "\n",
            "regression loss =  4.98310661\n",
            " 5/34 [===>..........................] - ETA: 17:40 - loss: 9.9002 - custom_mse: 196.0107 - recall: 0.2963 \n",
            "reduce_confident_loss =  0.157700464\n",
            "\n",
            "reduce_mean_cls_loss =  0.0469934754\n",
            "\n",
            "regression loss =  5.16576195\n",
            " 6/34 [====>.........................] - ETA: 17:11 - loss: 9.1452 - custom_mse: 166.6271 - recall: 0.2469\n",
            "reduce_confident_loss =  0.10911148\n",
            "\n",
            "reduce_mean_cls_loss =  0.0845008269\n",
            "\n",
            "regression loss =  7.5076046\n",
            " 7/34 [=====>........................] - ETA: 16:30 - loss: 8.9390 - custom_mse: 139.1385 - recall: 0.2116\n",
            "reduce_confident_loss =  0.103335313\n",
            "\n",
            "reduce_mean_cls_loss =  0.232230186\n",
            "\n",
            "regression loss =  15.2936335\n",
            " 8/34 [======>.......................] - ETA: 15:48 - loss: 9.7752 - custom_mse: 127.4702 - recall: 0.1852\n",
            "reduce_confident_loss =  0.438807\n",
            "\n",
            "reduce_mean_cls_loss =  0.158631593\n",
            "\n",
            "regression loss =  13.5674715\n",
            " 9/34 [======>.......................] - ETA: 15:10 - loss: 10.2630 - custom_mse: 187.3016 - recall: 0.1646\n",
            "reduce_confident_loss =  0.516867042\n",
            "\n",
            "reduce_mean_cls_loss =  0.802841723\n",
            "\n",
            "regression loss =  8.80597\n",
            "10/34 [=======>......................] - ETA: 15:43 - loss: 10.2492 - custom_mse: 231.8645 - recall: 0.1482\n",
            "reduce_confident_loss =  0.130652785\n",
            "\n",
            "reduce_mean_cls_loss =  1.35716736\n",
            "\n",
            "regression loss =  12.1262693\n",
            "11/34 [========>.....................] - ETA: 14:55 - loss: 10.5551 - custom_mse: 22.7469 - recall: 0.1347 \n",
            "reduce_confident_loss =  0.555208206\n",
            "\n",
            "reduce_mean_cls_loss =  2.25387955\n",
            "\n",
            "regression loss =  17.7464638\n",
            "12/34 [=========>....................] - ETA: 14:09 - loss: 11.3885 - custom_mse: 39.9044 - recall: 0.1367\n",
            "reduce_confident_loss =  0.278921753\n",
            "\n",
            "reduce_mean_cls_loss =  2.69644022\n",
            "\n",
            "regression loss =  13.9653873\n",
            "13/34 [==========>...................] - ETA: 13:25 - loss: 11.8156 - custom_mse: 167.3227 - recall: 0.1262\n",
            "reduce_confident_loss =  0.581624448\n",
            "\n",
            "reduce_mean_cls_loss =  1.45002615\n",
            "\n",
            "regression loss =  7.93069363\n",
            "14/34 [===========>..................] - ETA: 12:42 - loss: 11.6832 - custom_mse: 156.8540 - recall: 0.1502\n",
            "reduce_confident_loss =  0.307509482\n",
            "\n",
            "reduce_mean_cls_loss =  1.36576712\n",
            "\n",
            "regression loss =  10.1866922\n",
            "15/34 [============>.................] - ETA: 12:01 - loss: 11.6950 - custom_mse: 160.3491 - recall: 0.1649\n",
            "reduce_confident_loss =  0.455478519\n",
            "\n",
            "reduce_mean_cls_loss =  1.25617683\n",
            "\n",
            "regression loss =  8.31133366\n",
            "16/34 [=============>................] - ETA: 11:20 - loss: 11.5905 - custom_mse: 168.6809 - recall: 0.1637\n",
            "reduce_confident_loss =  0.329036057\n",
            "\n",
            "reduce_mean_cls_loss =  0.922980726\n",
            "\n",
            "regression loss =  5.96631908\n",
            "17/34 [==============>...............] - ETA: 10:41 - loss: 11.3333 - custom_mse: 177.1676 - recall: 0.1540\n",
            "reduce_confident_loss =  0.285047591\n",
            "\n",
            "reduce_mean_cls_loss =  0.241943762\n",
            "\n",
            "regression loss =  5.97004032\n",
            "18/34 [==============>...............] - ETA: 10:01 - loss: 11.0646 - custom_mse: 177.3358 - recall: 0.1455\n",
            "reduce_confident_loss =  0.206778795\n",
            "\n",
            "reduce_mean_cls_loss =  0.345152318\n",
            "\n",
            "regression loss =  7.20174742\n",
            "19/34 [===============>..............] - ETA: 9:21 - loss: 10.8904 - custom_mse: 200.9280 - recall: 0.1378 \n",
            "reduce_confident_loss =  0.19007659\n",
            "\n",
            "reduce_mean_cls_loss =  0.402110308\n",
            "\n",
            "regression loss =  8.28850651\n",
            "20/34 [================>.............] - ETA: 8:43 - loss: 10.7899 - custom_mse: 201.1543 - recall: 0.1434\n",
            "reduce_confident_loss =  0.185570791\n",
            "\n",
            "reduce_mean_cls_loss =  0.192017794\n",
            "\n",
            "regression loss =  5.36765242\n",
            "21/34 [=================>............] - ETA: 8:07 - loss: 10.5497 - custom_mse: 144.5861 - recall: 0.1366\n",
            "reduce_confident_loss =  0.114289045\n",
            "\n",
            "reduce_mean_cls_loss =  0.193978414\n",
            "\n",
            "regression loss =  7.28814793\n",
            "22/34 [==================>...........] - ETA: 7:28 - loss: 10.4154 - custom_mse: 126.3483 - recall: 0.1304\n",
            "reduce_confident_loss =  0.412515432\n",
            "\n",
            "reduce_mean_cls_loss =  0.575200617\n",
            "\n",
            "regression loss =  9.00471306\n",
            "23/34 [===================>..........] - ETA: 6:50 - loss: 10.3970 - custom_mse: 111.6221 - recall: 0.1433\n",
            "reduce_confident_loss =  0.268426031\n",
            "\n",
            "reduce_mean_cls_loss =  0.422944963\n",
            "\n",
            "regression loss =  7.76392651\n",
            "24/34 [====================>.........] - ETA: 6:12 - loss: 10.3161 - custom_mse: 89.7099 - recall: 0.1478 \n",
            "reduce_confident_loss =  0.591079116\n",
            "\n",
            "reduce_mean_cls_loss =  0.360319287\n",
            "\n",
            "regression loss =  8.04334164\n",
            "25/34 [=====================>........] - ETA: 5:35 - loss: 10.2633 - custom_mse: 50.9955 - recall: 0.1712\n",
            "reduce_confident_loss =  1.10538614\n",
            "\n",
            "reduce_mean_cls_loss =  0.394841105\n",
            "\n",
            "regression loss =  9.07527256\n",
            "26/34 [=====================>........] - ETA: 4:57 - loss: 10.2753 - custom_mse: 34.8603 - recall: 0.2019\n",
            "reduce_confident_loss =  0.859906137\n",
            "\n",
            "reduce_mean_cls_loss =  1.07444191\n",
            "\n",
            "regression loss =  10.2741117\n",
            "27/34 [======================>.......] - ETA: 4:19 - loss: 10.3469 - custom_mse: 85.0006 - recall: 0.2237\n",
            "reduce_confident_loss =  0.469644\n",
            "\n",
            "reduce_mean_cls_loss =  0.732511103\n",
            "\n",
            "regression loss =  11.0678921\n",
            "28/34 [=======================>......] - ETA: 3:42 - loss: 10.4156 - custom_mse: 131.5315 - recall: 0.2335\n",
            "reduce_confident_loss =  0.323283\n",
            "\n",
            "reduce_mean_cls_loss =  0.247122929\n",
            "\n",
            "regression loss =  12.8953209\n",
            "29/34 [========================>.....] - ETA: 3:06 - loss: 10.5208 - custom_mse: 176.2888 - recall: 0.2255\n",
            "reduce_confident_loss =  0.276516587\n",
            "\n",
            "reduce_mean_cls_loss =  0.517234445\n",
            "\n",
            "regression loss =  10.4124384\n",
            "30/34 [=========================>....] - ETA: 2:28 - loss: 10.5436 - custom_mse: 133.8992 - recall: 0.2380\n",
            "reduce_confident_loss =  0.3730748\n",
            "\n",
            "reduce_mean_cls_loss =  0.314076096\n",
            "\n",
            "regression loss =  7.33110666\n",
            "31/34 [==========================>...] - ETA: 1:51 - loss: 10.4621 - custom_mse: 92.6569 - recall: 0.2626 \n",
            "reduce_confident_loss =  0.275581777\n",
            "\n",
            "reduce_mean_cls_loss =  2.26269817\n",
            "\n",
            "regression loss =  10.7712975\n",
            "32/34 [===========================>..] - ETA: 1:14 - loss: 10.5511 - custom_mse: 273.2633 - recall: 0.2544\n",
            "reduce_confident_loss =  0.237885609\n",
            "\n",
            "reduce_mean_cls_loss =  1.66011071\n",
            "\n",
            "regression loss =  11.9101915\n",
            "33/34 [============================>.] - ETA: 36s - loss: 10.6498 - custom_mse: 225.5341 - recall: 0.2466 \n",
            "reduce_confident_loss =  0.251357883\n",
            "\n",
            "reduce_mean_cls_loss =  1.16643226\n",
            "\n",
            "regression loss =  9.34468269\n",
            "34/34 [==============================] - ETA: 0s - loss: 10.6519 - custom_mse: 180.7562 - recall: 0.2394 \n",
            "reduce_confident_loss =  0.901568711\n",
            "\n",
            "reduce_mean_cls_loss =  6.45102692\n",
            "\n",
            "regression loss =  16.7764816\n",
            "\n",
            "reduce_confident_loss =  0.354184955\n",
            "\n",
            "reduce_mean_cls_loss =  0.803662598\n",
            "\n",
            "regression loss =  11.2353668\n",
            "\n",
            "reduce_confident_loss =  0.895881772\n",
            "\n",
            "reduce_mean_cls_loss =  6.39092\n",
            "\n",
            "regression loss =  20.2978725\n",
            "\n",
            "reduce_confident_loss =  0.894789398\n",
            "\n",
            "reduce_mean_cls_loss =  0.00175401627\n",
            "\n",
            "regression loss =  14.5906353\n",
            "\n",
            "reduce_confident_loss =  0.510841608\n",
            "\n",
            "reduce_mean_cls_loss =  2.38191366\n",
            "\n",
            "regression loss =  18.8341389\n",
            "\n",
            "reduce_confident_loss =  0.892287135\n",
            "\n",
            "reduce_mean_cls_loss =  6.34517908\n",
            "\n",
            "regression loss =  11.7145948\n",
            "\n",
            "reduce_confident_loss =  0.891282082\n",
            "\n",
            "reduce_mean_cls_loss =  6.33864594\n",
            "\n",
            "regression loss =  13.174839\n",
            "\n",
            "reduce_confident_loss =  0.8906039\n",
            "\n",
            "reduce_mean_cls_loss =  6.36095095\n",
            "\n",
            "regression loss =  12.7465754\n",
            "\n",
            "reduce_confident_loss =  0.738347828\n",
            "\n",
            "reduce_mean_cls_loss =  4.76196718\n",
            "\n",
            "regression loss =  16.7169266\n",
            "\n",
            "reduce_confident_loss =  0.32345739\n",
            "\n",
            "reduce_mean_cls_loss =  0.834958494\n",
            "\n",
            "regression loss =  18.0525227\n",
            "\n",
            "reduce_confident_loss =  0.293915272\n",
            "\n",
            "reduce_mean_cls_loss =  0.735757828\n",
            "\n",
            "regression loss =  12.7318\n",
            "\n",
            "reduce_confident_loss =  0.894379735\n",
            "\n",
            "reduce_mean_cls_loss =  6.36780882\n",
            "\n",
            "regression loss =  20.1284\n",
            "\n",
            "reduce_confident_loss =  0.89823854\n",
            "\n",
            "reduce_mean_cls_loss =  0.00172575703\n",
            "\n",
            "regression loss =  16.903019\n",
            "34/34 [==============================] - 1445s 43s/step - loss: 10.6519 - custom_mse: 180.7562 - recall: 0.2394 - val_loss: 20.1845 - val_custom_mse: 8.4359 - val_recall: 0.2260\n",
            "Epoch 3/10\n",
            "\n",
            "reduce_confident_loss =  0.217336848\n",
            "\n",
            "reduce_mean_cls_loss =  0.884679794\n",
            "\n",
            "regression loss =  11.6711597\n",
            " 1/34 [..............................] - ETA: 20:07 - loss: 12.7732 - custom_mse: 230.8148 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.380588144\n",
            "\n",
            "reduce_mean_cls_loss =  0.91848594\n",
            "\n",
            "regression loss =  9.62559605\n",
            " 2/34 [>.............................] - ETA: 19:02 - loss: 11.8489 - custom_mse: 101.7562 - recall: 0.1363    \n",
            "reduce_confident_loss =  0.357358545\n",
            "\n",
            "reduce_mean_cls_loss =  1.16072369\n",
            "\n",
            "regression loss =  9.03336811\n",
            " 3/34 [=>............................] - ETA: 18:35 - loss: 11.4164 - custom_mse: 155.9510 - recall: 0.1075\n",
            "reduce_confident_loss =  0.141405031\n",
            "\n",
            "reduce_mean_cls_loss =  0.395826\n",
            "\n",
            "regression loss =  6.68207073\n",
            " 4/34 [==>...........................] - ETA: 17:44 - loss: 10.3672 - custom_mse: 202.1716 - recall: 0.0806\n",
            "reduce_confident_loss =  0.324368328\n",
            "\n",
            "reduce_mean_cls_loss =  0.371698856\n",
            "\n",
            "regression loss =  3.82295299\n",
            " 5/34 [===>..........................] - ETA: 17:04 - loss: 9.1975 - custom_mse: 142.6696 - recall: 0.0645 \n",
            "reduce_confident_loss =  0.105687313\n",
            "\n",
            "reduce_mean_cls_loss =  0.256454736\n",
            "\n",
            "regression loss =  4.61397457\n",
            " 6/34 [====>.........................] - ETA: 16:25 - loss: 8.4940 - custom_mse: 148.2454 - recall: 0.0538\n",
            "reduce_confident_loss =  0.251328558\n",
            "\n",
            "reduce_mean_cls_loss =  0.155388936\n",
            "\n",
            "regression loss =  6.37331438\n",
            " 7/34 [=====>........................] - ETA: 15:48 - loss: 8.2491 - custom_mse: 115.1220 - recall: 0.0461\n",
            "reduce_confident_loss =  0.208531708\n",
            "\n",
            "reduce_mean_cls_loss =  0.241335705\n",
            "\n",
            "regression loss =  13.3320484\n",
            " 8/34 [======>.......................] - ETA: 15:09 - loss: 8.9407 - custom_mse: 97.0678 - recall: 0.0403 \n",
            "reduce_confident_loss =  0.277582556\n",
            "\n",
            "reduce_mean_cls_loss =  0.559357166\n",
            "\n",
            "regression loss =  13.4881887\n",
            " 9/34 [======>.......................] - ETA: 14:30 - loss: 9.5390 - custom_mse: 132.8477 - recall: 0.0358\n",
            "reduce_confident_loss =  0.315658092\n",
            "\n",
            "reduce_mean_cls_loss =  0.505894482\n",
            "\n",
            "regression loss =  9.73149586\n",
            "10/34 [=======>......................] - ETA: 15:08 - loss: 9.6404 - custom_mse: 139.1759 - recall: 0.0767\n",
            "reduce_confident_loss =  0.279625267\n",
            "\n",
            "reduce_mean_cls_loss =  0.0242631827\n",
            "\n",
            "regression loss =  10.1035948\n",
            "11/34 [========>.....................] - ETA: 14:25 - loss: 9.7101 - custom_mse: 12.7792 - recall: 0.1606 \n",
            "reduce_confident_loss =  0.343372881\n",
            "\n",
            "reduce_mean_cls_loss =  0.111436687\n",
            "\n",
            "regression loss =  15.5065269\n",
            "12/34 [=========>....................] - ETA: 13:44 - loss: 10.2311 - custom_mse: 102.7330 - recall: 0.2306\n",
            "reduce_confident_loss =  0.492762536\n",
            "\n",
            "reduce_mean_cls_loss =  0.0195269641\n",
            "\n",
            "regression loss =  12.6756115\n",
            "13/34 [==========>...................] - ETA: 13:04 - loss: 10.4585 - custom_mse: 110.6490 - recall: 0.2749\n",
            "reduce_confident_loss =  0.30730623\n",
            "\n",
            "reduce_mean_cls_loss =  0.712447047\n",
            "\n",
            "regression loss =  8.26214218\n",
            "14/34 [===========>..................] - ETA: 12:26 - loss: 10.3745 - custom_mse: 151.7162 - recall: 0.2898\n",
            "reduce_confident_loss =  0.29494229\n",
            "\n",
            "reduce_mean_cls_loss =  0.943141162\n",
            "\n",
            "regression loss =  10.1642399\n",
            "15/34 [============>.................] - ETA: 11:56 - loss: 10.4430 - custom_mse: 130.6023 - recall: 0.3224\n",
            "reduce_confident_loss =  0.16719164\n",
            "\n",
            "reduce_mean_cls_loss =  0.394795746\n",
            "\n",
            "regression loss =  7.81830454\n",
            "16/34 [=============>................] - ETA: 11:16 - loss: 10.3141 - custom_mse: 105.3966 - recall: 0.3444\n",
            "reduce_confident_loss =  0.608734548\n",
            "\n",
            "reduce_mean_cls_loss =  0.0347034447\n",
            "\n",
            "regression loss =  8.61928177\n",
            "17/34 [==============>...............] - ETA: 10:36 - loss: 10.2522 - custom_mse: 97.7361 - recall: 0.3241 \n",
            "reduce_confident_loss =  0.490575731\n",
            "\n",
            "reduce_mean_cls_loss =  0.0698180646\n",
            "\n",
            "regression loss =  7.79101515\n",
            "18/34 [==============>...............] - ETA: 10:00 - loss: 10.1466 - custom_mse: 136.1111 - recall: 0.3061\n",
            "reduce_confident_loss =  0.448581487\n",
            "\n",
            "reduce_mean_cls_loss =  0.0255909823\n",
            "\n",
            "regression loss =  9.45544\n",
            "19/34 [===============>..............] - ETA: 9:21 - loss: 10.1352 - custom_mse: 154.9531 - recall: 0.2900 \n",
            "reduce_confident_loss =  0.234587699\n",
            "\n",
            "reduce_mean_cls_loss =  0.0657799914\n",
            "\n",
            "regression loss =  9.66227245\n",
            "20/34 [================>.............] - ETA: 8:43 - loss: 10.1266 - custom_mse: 208.6889 - recall: 0.3061\n",
            "reduce_confident_loss =  0.212987453\n",
            "\n",
            "reduce_mean_cls_loss =  0.0286101606\n",
            "\n",
            "regression loss =  4.80454779\n",
            "21/34 [=================>............] - ETA: 8:04 - loss: 9.8846 - custom_mse: 135.5036 - recall: 0.2915 \n",
            "reduce_confident_loss =  0.24034895\n",
            "\n",
            "reduce_mean_cls_loss =  0.0457117483\n",
            "\n",
            "regression loss =  6.60225248\n",
            "22/34 [==================>...........] - ETA: 7:26 - loss: 9.7485 - custom_mse: 134.4214 - recall: 0.2782\n",
            "reduce_confident_loss =  0.553455532\n",
            "\n",
            "reduce_mean_cls_loss =  0.406667173\n",
            "\n",
            "regression loss =  7.29035282\n",
            "23/34 [===================>..........] - ETA: 6:48 - loss: 9.6833 - custom_mse: 129.1628 - recall: 0.3096\n",
            "reduce_confident_loss =  0.208827183\n",
            "\n",
            "reduce_mean_cls_loss =  0.313248903\n",
            "\n",
            "regression loss =  5.86813\n",
            "24/34 [====================>.........] - ETA: 6:10 - loss: 9.5461 - custom_mse: 115.7894 - recall: 0.3033\n",
            "reduce_confident_loss =  0.504281938\n",
            "\n",
            "reduce_mean_cls_loss =  0.609901845\n",
            "\n",
            "regression loss =  6.46264839\n",
            "25/34 [=====================>........] - ETA: 5:32 - loss: 9.4673 - custom_mse: 54.8952 - recall: 0.3312 \n",
            "reduce_confident_loss =  0.986059368\n",
            "\n",
            "reduce_mean_cls_loss =  0.635303497\n",
            "\n",
            "regression loss =  7.75770807\n",
            "26/34 [=====================>........] - ETA: 4:54 - loss: 9.4639 - custom_mse: 35.9541 - recall: 0.3565\n",
            "reduce_confident_loss =  1.18166602\n",
            "\n",
            "reduce_mean_cls_loss =  1.30944693\n",
            "\n",
            "regression loss =  9.29635811\n",
            "27/34 [======================>.......] - ETA: 4:16 - loss: 9.5500 - custom_mse: 95.9969 - recall: 0.3785\n",
            "reduce_confident_loss =  0.557813942\n",
            "\n",
            "reduce_mean_cls_loss =  0.383825451\n",
            "\n",
            "regression loss =  11.0696774\n",
            "28/34 [=======================>......] - ETA: 3:40 - loss: 9.6379 - custom_mse: 167.2730 - recall: 0.3650\n",
            "reduce_confident_loss =  0.0952196941\n",
            "\n",
            "reduce_mean_cls_loss =  2.25413346\n",
            "\n",
            "regression loss =  13.7606163\n",
            "29/34 [========================>.....] - ETA: 3:03 - loss: 9.8611 - custom_mse: 286.6759 - recall: 0.3524\n",
            "reduce_confident_loss =  0.40825209\n",
            "\n",
            "reduce_mean_cls_loss =  2.82774854\n",
            "\n",
            "regression loss =  8.95428753\n",
            "30/34 [=========================>....] - ETA: 2:26 - loss: 9.9387 - custom_mse: 162.4505 - recall: 0.3407\n",
            "reduce_confident_loss =  0.341595709\n",
            "\n",
            "reduce_mean_cls_loss =  2.46654677\n",
            "\n",
            "regression loss =  5.27525759\n",
            "31/34 [==========================>...] - ETA: 1:49 - loss: 9.8789 - custom_mse: 222.0307 - recall: 0.3297\n",
            "reduce_confident_loss =  0.245098665\n",
            "\n",
            "reduce_mean_cls_loss =  0.0497591\n",
            "\n",
            "regression loss =  10.5759659\n",
            "32/34 [===========================>..] - ETA: 1:13 - loss: 9.9099 - custom_mse: 465.9917 - recall: 0.3194\n",
            "reduce_confident_loss =  0.249063045\n",
            "\n",
            "reduce_mean_cls_loss =  0.0891587585\n",
            "\n",
            "regression loss =  12.965374\n",
            "33/34 [============================>.] - ETA: 36s - loss: 10.0127 - custom_mse: 364.4167 - recall: 0.3097\n",
            "reduce_confident_loss =  0.147369936\n",
            "\n",
            "reduce_mean_cls_loss =  0.455124766\n",
            "\n",
            "regression loss =  11.317482\n",
            "34/34 [==============================] - ETA: 0s - loss: 10.0482 - custom_mse: 276.8445 - recall: 0.3006 \n",
            "reduce_confident_loss =  0.592344165\n",
            "\n",
            "reduce_mean_cls_loss =  5.29819822\n",
            "\n",
            "regression loss =  16.179224\n",
            "\n",
            "reduce_confident_loss =  0.110086538\n",
            "\n",
            "reduce_mean_cls_loss =  0.977572858\n",
            "\n",
            "regression loss =  9.33003139\n",
            "\n",
            "reduce_confident_loss =  0.593509257\n",
            "\n",
            "reduce_mean_cls_loss =  5.25399876\n",
            "\n",
            "regression loss =  20.2904797\n",
            "\n",
            "reduce_confident_loss =  0.593729436\n",
            "\n",
            "reduce_mean_cls_loss =  0.00551347854\n",
            "\n",
            "regression loss =  16.6476212\n",
            "\n",
            "reduce_confident_loss =  0.251776159\n",
            "\n",
            "reduce_mean_cls_loss =  2.17656612\n",
            "\n",
            "regression loss =  18.2864532\n",
            "\n",
            "reduce_confident_loss =  0.592779696\n",
            "\n",
            "reduce_mean_cls_loss =  5.2119174\n",
            "\n",
            "regression loss =  9.72439\n",
            "\n",
            "reduce_confident_loss =  0.592166364\n",
            "\n",
            "reduce_mean_cls_loss =  5.19983339\n",
            "\n",
            "regression loss =  11.6400051\n",
            "\n",
            "reduce_confident_loss =  0.588573158\n",
            "\n",
            "reduce_mean_cls_loss =  5.19997931\n",
            "\n",
            "regression loss =  11.1510382\n",
            "\n",
            "reduce_confident_loss =  0.456777424\n",
            "\n",
            "reduce_mean_cls_loss =  3.98656845\n",
            "\n",
            "regression loss =  15.31392\n",
            "\n",
            "reduce_confident_loss =  0.115755558\n",
            "\n",
            "reduce_mean_cls_loss =  0.981491506\n",
            "\n",
            "regression loss =  17.6192646\n",
            "\n",
            "reduce_confident_loss =  0.118367039\n",
            "\n",
            "reduce_mean_cls_loss =  0.800169528\n",
            "\n",
            "regression loss =  12.6194983\n",
            "\n",
            "reduce_confident_loss =  0.592135847\n",
            "\n",
            "reduce_mean_cls_loss =  5.22017336\n",
            "\n",
            "regression loss =  21.9517\n",
            "\n",
            "reduce_confident_loss =  0.596123576\n",
            "\n",
            "reduce_mean_cls_loss =  0.00541100139\n",
            "\n",
            "regression loss =  15.7206039\n",
            "34/34 [==============================] - 1400s 41s/step - loss: 10.0482 - custom_mse: 276.8445 - recall: 0.3006 - val_loss: 18.7662 - val_custom_mse: 113.1054 - val_recall: 0.0769\n",
            "Epoch 4/10\n",
            "\n",
            "reduce_confident_loss =  0.0945282131\n",
            "\n",
            "reduce_mean_cls_loss =  0.0182873141\n",
            "\n",
            "regression loss =  7.05221\n",
            " 1/34 [..............................] - ETA: 18:46 - loss: 7.1650 - custom_mse: 348.1669 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.487013578\n",
            "\n",
            "reduce_mean_cls_loss =  1.22565293\n",
            "\n",
            "regression loss =  7.98687744\n",
            " 2/34 [>.............................] - ETA: 17:40 - loss: 8.4323 - custom_mse: 81.5199 - recall: 0.0401     \n",
            "reduce_confident_loss =  0.489873737\n",
            "\n",
            "reduce_mean_cls_loss =  1.49942768\n",
            "\n",
            "regression loss =  6.70305681\n",
            " 3/34 [=>............................] - ETA: 16:56 - loss: 8.5190 - custom_mse: 209.9407 - recall: 0.3601\n",
            "reduce_confident_loss =  0.257934153\n",
            "\n",
            "reduce_mean_cls_loss =  0.439297259\n",
            "\n",
            "regression loss =  3.16580081\n",
            " 4/34 [==>...........................] - ETA: 16:19 - loss: 7.3550 - custom_mse: 262.1471 - recall: 0.2700\n",
            "reduce_confident_loss =  0.207924604\n",
            "\n",
            "reduce_mean_cls_loss =  0.137635931\n",
            "\n",
            "regression loss =  2.40856767\n",
            " 5/34 [===>..........................] - ETA: 15:44 - loss: 6.4348 - custom_mse: 191.5491 - recall: 0.2160\n",
            "reduce_confident_loss =  0.229237765\n",
            "\n",
            "reduce_mean_cls_loss =  0.17492716\n",
            "\n",
            "regression loss =  4.71275711\n",
            " 6/34 [====>.........................] - ETA: 15:11 - loss: 6.2152 - custom_mse: 164.6349 - recall: 0.1800\n",
            "reduce_confident_loss =  0.234021395\n",
            "\n",
            "reduce_mean_cls_loss =  0.320561677\n",
            "\n",
            "regression loss =  6.52857494\n",
            " 7/34 [=====>........................] - ETA: 14:39 - loss: 6.3392 - custom_mse: 117.8597 - recall: 0.1543\n",
            "reduce_confident_loss =  0.113861836\n",
            "\n",
            "reduce_mean_cls_loss =  0.167292729\n",
            "\n",
            "regression loss =  13.2537842\n",
            " 8/34 [======>.......................] - ETA: 14:07 - loss: 7.2386 - custom_mse: 111.6210 - recall: 0.1350\n",
            "reduce_confident_loss =  0.134305939\n",
            "\n",
            "reduce_mean_cls_loss =  1.04020655\n",
            "\n",
            "regression loss =  12.9556952\n",
            " 9/34 [======>.......................] - ETA: 13:37 - loss: 8.0044 - custom_mse: 132.0397 - recall: 0.1200\n",
            "reduce_confident_loss =  0.27765277\n",
            "\n",
            "reduce_mean_cls_loss =  0.121036276\n",
            "\n",
            "regression loss =  9.17577362\n",
            "10/34 [=======>......................] - ETA: 14:27 - loss: 8.1614 - custom_mse: 180.7603 - recall: 0.1080\n",
            "reduce_confident_loss =  0.428154796\n",
            "\n",
            "reduce_mean_cls_loss =  0.00566846877\n",
            "\n",
            "regression loss =  10.3951063\n",
            "11/34 [========>.....................] - ETA: 13:47 - loss: 8.4039 - custom_mse: 18.7776 - recall: 0.1891 \n",
            "reduce_confident_loss =  0.106170669\n",
            "\n",
            "reduce_mean_cls_loss =  0.02778841\n",
            "\n",
            "regression loss =  16.682806\n",
            "12/34 [=========>....................] - ETA: 13:05 - loss: 9.1050 - custom_mse: 153.4014 - recall: 0.2567\n",
            "reduce_confident_loss =  0.718200564\n",
            "\n",
            "reduce_mean_cls_loss =  0.0301889628\n",
            "\n",
            "regression loss =  13.7449732\n",
            "13/34 [==========>...................] - ETA: 12:29 - loss: 9.5194 - custom_mse: 156.6927 - recall: 0.3093\n",
            "reduce_confident_loss =  0.534482598\n",
            "\n",
            "reduce_mean_cls_loss =  0.765783191\n",
            "\n",
            "regression loss =  7.25803041\n",
            "14/34 [===========>..................] - ETA: 11:50 - loss: 9.4508 - custom_mse: 193.5706 - recall: 0.3552\n",
            "reduce_confident_loss =  0.223753557\n",
            "\n",
            "reduce_mean_cls_loss =  0.732626379\n",
            "\n",
            "regression loss =  8.81039\n",
            "15/34 [============>.................] - ETA: 11:12 - loss: 9.4719 - custom_mse: 193.1624 - recall: 0.3859\n",
            "reduce_confident_loss =  0.244877502\n",
            "\n",
            "reduce_mean_cls_loss =  0.279034525\n",
            "\n",
            "regression loss =  5.93519545\n",
            "16/34 [=============>................] - ETA: 10:35 - loss: 9.2836 - custom_mse: 124.5451 - recall: 0.4025\n",
            "reduce_confident_loss =  0.744084775\n",
            "\n",
            "reduce_mean_cls_loss =  0.000542959315\n",
            "\n",
            "regression loss =  5.19107914\n",
            "17/34 [==============>...............] - ETA: 9:59 - loss: 9.0866 - custom_mse: 140.2678 - recall: 0.3789 \n",
            "reduce_confident_loss =  0.606797874\n",
            "\n",
            "reduce_mean_cls_loss =  0.00219247513\n",
            "\n",
            "regression loss =  4.46770954\n",
            "18/34 [==============>...............] - ETA: 9:22 - loss: 8.8639 - custom_mse: 141.6641 - recall: 0.3578\n",
            "reduce_confident_loss =  0.653201401\n",
            "\n",
            "reduce_mean_cls_loss =  0.00936372764\n",
            "\n",
            "regression loss =  6.31519461\n",
            "19/34 [===============>..............] - ETA: 8:46 - loss: 8.7646 - custom_mse: 185.2593 - recall: 0.3390\n",
            "reduce_confident_loss =  0.0811470374\n",
            "\n",
            "reduce_mean_cls_loss =  0.0280898176\n",
            "\n",
            "regression loss =  6.79366636\n",
            "20/34 [================>.............] - ETA: 8:12 - loss: 8.6715 - custom_mse: 276.7704 - recall: 0.3595\n",
            "reduce_confident_loss =  0.200495929\n",
            "\n",
            "reduce_mean_cls_loss =  0.0429250933\n",
            "\n",
            "regression loss =  2.78726602\n",
            "21/34 [=================>............] - ETA: 7:36 - loss: 8.4029 - custom_mse: 155.7205 - recall: 0.3847\n",
            "reduce_confident_loss =  0.134269953\n",
            "\n",
            "reduce_mean_cls_loss =  0.0389363356\n",
            "\n",
            "regression loss =  5.22746897\n",
            "22/34 [==================>...........] - ETA: 7:00 - loss: 8.2664 - custom_mse: 153.6285 - recall: 0.3672\n",
            "reduce_confident_loss =  0.229693964\n",
            "\n",
            "reduce_mean_cls_loss =  0.222105876\n",
            "\n",
            "regression loss =  6.62348413\n",
            "23/34 [===================>..........] - ETA: 6:24 - loss: 8.2146 - custom_mse: 147.5566 - recall: 0.3592\n",
            "reduce_confident_loss =  0.221202955\n",
            "\n",
            "reduce_mean_cls_loss =  0.185887918\n",
            "\n",
            "regression loss =  6.03633261\n",
            "24/34 [====================>.........] - ETA: 5:49 - loss: 8.1408 - custom_mse: 122.8305 - recall: 0.3781\n",
            "reduce_confident_loss =  0.374685019\n",
            "\n",
            "reduce_mean_cls_loss =  0.264445156\n",
            "\n",
            "regression loss =  5.28011\n",
            "25/34 [=====================>........] - ETA: 5:13 - loss: 8.0520 - custom_mse: 62.1015 - recall: 0.3914 \n",
            "reduce_confident_loss =  0.728953302\n",
            "\n",
            "reduce_mean_cls_loss =  0.704067647\n",
            "\n",
            "regression loss =  7.22771549\n",
            "26/34 [=====================>........] - ETA: 4:38 - loss: 8.0754 - custom_mse: 42.2333 - recall: 0.4144\n",
            "reduce_confident_loss =  0.700228035\n",
            "\n",
            "reduce_mean_cls_loss =  1.03879702\n",
            "\n",
            "regression loss =  8.40083122\n",
            "27/34 [======================>.......] - ETA: 4:03 - loss: 8.1519 - custom_mse: 100.6588 - recall: 0.4248\n",
            "reduce_confident_loss =  0.364681721\n",
            "\n",
            "reduce_mean_cls_loss =  0.339655876\n",
            "\n",
            "regression loss =  8.51007175\n",
            "28/34 [=======================>......] - ETA: 3:30 - loss: 8.1898 - custom_mse: 148.3537 - recall: 0.4108\n",
            "reduce_confident_loss =  0.236319542\n",
            "\n",
            "reduce_mean_cls_loss =  0.377022\n",
            "\n",
            "regression loss =  11.7592564\n",
            "29/34 [========================>.....] - ETA: 2:55 - loss: 8.3340 - custom_mse: 212.3036 - recall: 0.4197\n",
            "reduce_confident_loss =  0.24019368\n",
            "\n",
            "reduce_mean_cls_loss =  0.576032\n",
            "\n",
            "regression loss =  9.22475338\n",
            "30/34 [=========================>....] - ETA: 2:20 - loss: 8.3909 - custom_mse: 162.6213 - recall: 0.4353\n",
            "reduce_confident_loss =  0.30010286\n",
            "\n",
            "reduce_mean_cls_loss =  1.10041618\n",
            "\n",
            "regression loss =  8.53261089\n",
            "31/34 [==========================>...] - ETA: 1:45 - loss: 8.4407 - custom_mse: 181.0465 - recall: 0.4213\n",
            "reduce_confident_loss =  0.300701797\n",
            "\n",
            "reduce_mean_cls_loss =  0.152016431\n",
            "\n",
            "regression loss =  13.4036255\n",
            "32/34 [===========================>..] - ETA: 1:10 - loss: 8.6099 - custom_mse: 312.0071 - recall: 0.4081\n",
            "reduce_confident_loss =  0.154785052\n",
            "\n",
            "reduce_mean_cls_loss =  0.342860848\n",
            "\n",
            "regression loss =  10.6075392\n",
            "33/34 [============================>.] - ETA: 35s - loss: 8.6855 - custom_mse: 272.4886 - recall: 0.3957 \n",
            "reduce_confident_loss =  0.163529\n",
            "\n",
            "reduce_mean_cls_loss =  0.515147805\n",
            "\n",
            "regression loss =  6.92978525\n",
            "34/34 [==============================] - ETA: 0s - loss: 8.6655 - custom_mse: 172.1760 - recall: 0.3841 \n",
            "reduce_confident_loss =  0.585848212\n",
            "\n",
            "reduce_mean_cls_loss =  4.76490259\n",
            "\n",
            "regression loss =  15.3063297\n",
            "\n",
            "reduce_confident_loss =  0.181825504\n",
            "\n",
            "reduce_mean_cls_loss =  1.02364755\n",
            "\n",
            "regression loss =  11.5676727\n",
            "\n",
            "reduce_confident_loss =  0.591422737\n",
            "\n",
            "reduce_mean_cls_loss =  4.7445\n",
            "\n",
            "regression loss =  20.5284424\n",
            "\n",
            "reduce_confident_loss =  0.603354454\n",
            "\n",
            "reduce_mean_cls_loss =  0.00885245204\n",
            "\n",
            "regression loss =  15.6378517\n",
            "\n",
            "reduce_confident_loss =  0.311347187\n",
            "\n",
            "reduce_mean_cls_loss =  2.06522298\n",
            "\n",
            "regression loss =  17.1079369\n",
            "\n",
            "reduce_confident_loss =  0.604254782\n",
            "\n",
            "reduce_mean_cls_loss =  4.72569942\n",
            "\n",
            "regression loss =  11.004324\n",
            "\n",
            "reduce_confident_loss =  0.600387\n",
            "\n",
            "reduce_mean_cls_loss =  4.71702242\n",
            "\n",
            "regression loss =  12.197032\n",
            "\n",
            "reduce_confident_loss =  0.596707404\n",
            "\n",
            "reduce_mean_cls_loss =  4.72924042\n",
            "\n",
            "regression loss =  11.8231516\n",
            "\n",
            "reduce_confident_loss =  0.477292478\n",
            "\n",
            "reduce_mean_cls_loss =  3.6384294\n",
            "\n",
            "regression loss =  14.4134321\n",
            "\n",
            "reduce_confident_loss =  0.174639434\n",
            "\n",
            "reduce_mean_cls_loss =  1.04482877\n",
            "\n",
            "regression loss =  17.9120789\n",
            "\n",
            "reduce_confident_loss =  0.158136949\n",
            "\n",
            "reduce_mean_cls_loss =  0.872258067\n",
            "\n",
            "regression loss =  12.571393\n",
            "\n",
            "reduce_confident_loss =  0.591959953\n",
            "\n",
            "reduce_mean_cls_loss =  4.75394\n",
            "\n",
            "regression loss =  22.9716015\n",
            "\n",
            "reduce_confident_loss =  0.597561479\n",
            "\n",
            "reduce_mean_cls_loss =  0.00852582324\n",
            "\n",
            "regression loss =  18.1579628\n",
            "34/34 [==============================] - 1354s 40s/step - loss: 8.6655 - custom_mse: 172.1760 - recall: 0.3841 - val_loss: 18.7993 - val_custom_mse: 66.8504 - val_recall: 0.1298\n",
            "Epoch 5/10\n",
            "\n",
            "reduce_confident_loss =  0.0792053416\n",
            "\n",
            "reduce_mean_cls_loss =  0.00999295246\n",
            "\n",
            "regression loss =  7.19828176\n",
            " 1/34 [..............................] - ETA: 19:45 - loss: 7.2875 - custom_mse: 413.6808 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.650356174\n",
            "\n",
            "reduce_mean_cls_loss =  1.50588691\n",
            "\n",
            "regression loss =  7.78509569\n",
            " 2/34 [>.............................] - ETA: 18:18 - loss: 8.6144 - custom_mse: 66.0542 - recall: 0.0000e+00 \n",
            "reduce_confident_loss =  0.599669874\n",
            "\n",
            "reduce_mean_cls_loss =  1.60259116\n",
            "\n",
            "regression loss =  6.02851391\n",
            " 3/34 [=>............................] - ETA: 17:36 - loss: 8.4865 - custom_mse: 255.0211 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.122359753\n",
            "\n",
            "reduce_mean_cls_loss =  0.0174101274\n",
            "\n",
            "regression loss =  2.72499204\n",
            " 4/34 [==>...........................] - ETA: 16:59 - loss: 7.0811 - custom_mse: 304.6910 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.118239693\n",
            "\n",
            "reduce_mean_cls_loss =  0.0667537302\n",
            "\n",
            "regression loss =  3.27759552\n",
            " 5/34 [===>..........................] - ETA: 16:58 - loss: 6.3574 - custom_mse: 250.4896 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.147977218\n",
            "\n",
            "reduce_mean_cls_loss =  0.223609611\n",
            "\n",
            "regression loss =  3.4463706\n",
            " 6/34 [====>.........................] - ETA: 16:27 - loss: 5.9342 - custom_mse: 170.3836 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.115101747\n",
            "\n",
            "reduce_mean_cls_loss =  0.0989623442\n",
            "\n",
            "regression loss =  4.56593\n",
            " 7/34 [=====>........................] - ETA: 15:51 - loss: 5.7693 - custom_mse: 149.5036 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.165662646\n",
            "\n",
            "reduce_mean_cls_loss =  0.504401326\n",
            "\n",
            "regression loss =  11.630681\n",
            " 8/34 [======>.......................] - ETA: 15:16 - loss: 6.5857 - custom_mse: 109.9731 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.146209314\n",
            "\n",
            "reduce_mean_cls_loss =  0.657354772\n",
            "\n",
            "regression loss =  10.7737694\n",
            " 9/34 [======>.......................] - ETA: 14:38 - loss: 7.1403 - custom_mse: 167.6481 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.350547522\n",
            "\n",
            "reduce_mean_cls_loss =  0.260558903\n",
            "\n",
            "regression loss =  7.88299036\n",
            "10/34 [=======>......................] - ETA: 15:13 - loss: 7.2757 - custom_mse: 193.3851 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.478659838\n",
            "\n",
            "reduce_mean_cls_loss =  0.020844752\n",
            "\n",
            "regression loss =  9.3431015\n",
            "11/34 [========>.....................] - ETA: 14:27 - loss: 7.5091 - custom_mse: 18.8481 - recall: 0.0000e+00 \n",
            "reduce_confident_loss =  0.135445431\n",
            "\n",
            "reduce_mean_cls_loss =  0.278925329\n",
            "\n",
            "regression loss =  13.6837444\n",
            "12/34 [=========>....................] - ETA: 13:45 - loss: 8.0581 - custom_mse: 142.7318 - recall: 0.0833   \n",
            "reduce_confident_loss =  0.516178071\n",
            "\n",
            "reduce_mean_cls_loss =  0.011220376\n",
            "\n",
            "regression loss =  12.6839371\n",
            "13/34 [==========>...................] - ETA: 13:07 - loss: 8.4545 - custom_mse: 143.7483 - recall: 0.1517\n",
            "reduce_confident_loss =  0.471789896\n",
            "\n",
            "reduce_mean_cls_loss =  0.322727174\n",
            "\n",
            "regression loss =  7.14709282\n",
            "14/34 [===========>..................] - ETA: 12:28 - loss: 8.4179 - custom_mse: 196.3681 - recall: 0.2068\n",
            "reduce_confident_loss =  0.196138725\n",
            "\n",
            "reduce_mean_cls_loss =  0.761597693\n",
            "\n",
            "regression loss =  8.8105011\n",
            "15/34 [============>.................] - ETA: 11:53 - loss: 8.5079 - custom_mse: 220.2570 - recall: 0.2450\n",
            "reduce_confident_loss =  0.303462237\n",
            "\n",
            "reduce_mean_cls_loss =  0.15242222\n",
            "\n",
            "regression loss =  6.98921967\n",
            "16/34 [=============>................] - ETA: 11:13 - loss: 8.4415 - custom_mse: 166.7176 - recall: 0.2841\n",
            "reduce_confident_loss =  0.530872345\n",
            "\n",
            "reduce_mean_cls_loss =  0.000498502399\n",
            "\n",
            "regression loss =  4.76929665\n",
            "17/34 [==============>...............] - ETA: 10:34 - loss: 8.2568 - custom_mse: 147.7799 - recall: 0.2674\n",
            "reduce_confident_loss =  0.534866869\n",
            "\n",
            "reduce_mean_cls_loss =  0.00286708656\n",
            "\n",
            "regression loss =  4.23936462\n",
            "18/34 [==============>...............] - ETA: 9:55 - loss: 8.0634 - custom_mse: 167.9121 - recall: 0.2525 \n",
            "reduce_confident_loss =  0.645906031\n",
            "\n",
            "reduce_mean_cls_loss =  0.00127773068\n",
            "\n",
            "regression loss =  7.58684921\n",
            "19/34 [===============>..............] - ETA: 9:17 - loss: 8.0724 - custom_mse: 152.0987 - recall: 0.2392\n",
            "reduce_confident_loss =  0.217369765\n",
            "\n",
            "reduce_mean_cls_loss =  0.0800475\n",
            "\n",
            "regression loss =  8.11353779\n",
            "20/34 [================>.............] - ETA: 8:38 - loss: 8.0893 - custom_mse: 247.1149 - recall: 0.2273\n",
            "reduce_confident_loss =  0.19303669\n",
            "\n",
            "reduce_mean_cls_loss =  0.0092073055\n",
            "\n",
            "regression loss =  2.74541497\n",
            "21/34 [=================>............] - ETA: 8:02 - loss: 7.8445 - custom_mse: 144.9303 - recall: 0.2456\n",
            "reduce_confident_loss =  0.238482073\n",
            "\n",
            "reduce_mean_cls_loss =  0.0126048373\n",
            "\n",
            "regression loss =  4.34330702\n",
            "22/34 [==================>...........] - ETA: 7:25 - loss: 7.6968 - custom_mse: 132.0418 - recall: 0.2344\n",
            "reduce_confident_loss =  0.238697216\n",
            "\n",
            "reduce_mean_cls_loss =  0.272000581\n",
            "\n",
            "regression loss =  5.67934179\n",
            "23/34 [===================>..........] - ETA: 6:47 - loss: 7.6313 - custom_mse: 147.8629 - recall: 0.2251\n",
            "reduce_confident_loss =  0.206310436\n",
            "\n",
            "reduce_mean_cls_loss =  0.250010908\n",
            "\n",
            "regression loss =  4.63384914\n",
            "24/34 [====================>.........] - ETA: 6:09 - loss: 7.5254 - custom_mse: 135.3076 - recall: 0.2241\n",
            "reduce_confident_loss =  0.286439\n",
            "\n",
            "reduce_mean_cls_loss =  0.268492609\n",
            "\n",
            "regression loss =  4.00370216\n",
            "25/34 [=====================>........] - ETA: 5:32 - loss: 7.4067 - custom_mse: 58.1095 - recall: 0.2393 \n",
            "reduce_confident_loss =  0.625256598\n",
            "\n",
            "reduce_mean_cls_loss =  0.307965696\n",
            "\n",
            "regression loss =  6.06573725\n",
            "26/34 [=====================>........] - ETA: 4:55 - loss: 7.3910 - custom_mse: 46.1595 - recall: 0.2645\n",
            "reduce_confident_loss =  0.596412361\n",
            "\n",
            "reduce_mean_cls_loss =  0.808685303\n",
            "\n",
            "regression loss =  7.21561384\n",
            "27/34 [======================>.......] - ETA: 4:18 - loss: 7.4366 - custom_mse: 118.9613 - recall: 0.2804\n",
            "reduce_confident_loss =  0.275145561\n",
            "\n",
            "reduce_mean_cls_loss =  0.318379462\n",
            "\n",
            "regression loss =  8.03087807\n",
            "28/34 [=======================>......] - ETA: 3:43 - loss: 7.4790 - custom_mse: 178.7273 - recall: 0.2730\n",
            "reduce_confident_loss =  0.299275428\n",
            "\n",
            "reduce_mean_cls_loss =  0.265709877\n",
            "\n",
            "regression loss =  11.69382\n",
            "29/34 [========================>.....] - ETA: 3:06 - loss: 7.6438 - custom_mse: 248.8517 - recall: 0.2635\n",
            "reduce_confident_loss =  0.0675928518\n",
            "\n",
            "reduce_mean_cls_loss =  0.444498062\n",
            "\n",
            "regression loss =  8.16673851\n",
            "30/34 [=========================>....] - ETA: 2:29 - loss: 7.6783 - custom_mse: 196.4305 - recall: 0.2548\n",
            "reduce_confident_loss =  0.0553230457\n",
            "\n",
            "reduce_mean_cls_loss =  0.319422096\n",
            "\n",
            "regression loss =  5.91199303\n",
            "31/34 [==========================>...] - ETA: 1:52 - loss: 7.6334 - custom_mse: 162.3330 - recall: 0.2465\n",
            "reduce_confident_loss =  0.29583472\n",
            "\n",
            "reduce_mean_cls_loss =  0.575290501\n",
            "\n",
            "regression loss =  8.60881519\n",
            "32/34 [===========================>..] - ETA: 1:14 - loss: 7.6911 - custom_mse: 298.2131 - recall: 0.2388\n",
            "reduce_confident_loss =  0.160150021\n",
            "\n",
            "reduce_mean_cls_loss =  0.0767946914\n",
            "\n",
            "regression loss =  11.1903791\n",
            "33/34 [============================>.] - ETA: 37s - loss: 7.8043 - custom_mse: 300.3505 - recall: 0.2316 \n",
            "reduce_confident_loss =  0.188070402\n",
            "\n",
            "reduce_mean_cls_loss =  0.179581657\n",
            "\n",
            "regression loss =  8.9737978\n",
            "34/34 [==============================] - ETA: 0s - loss: 7.8329 - custom_mse: 240.5310 - recall: 0.2248 \n",
            "reduce_confident_loss =  0.571448207\n",
            "\n",
            "reduce_mean_cls_loss =  5.03544283\n",
            "\n",
            "regression loss =  16.7095203\n",
            "\n",
            "reduce_confident_loss =  0.220560893\n",
            "\n",
            "reduce_mean_cls_loss =  0.688409686\n",
            "\n",
            "regression loss =  9.97681141\n",
            "\n",
            "reduce_confident_loss =  0.570770562\n",
            "\n",
            "reduce_mean_cls_loss =  5.04699612\n",
            "\n",
            "regression loss =  20.9167042\n",
            "\n",
            "reduce_confident_loss =  0.577931941\n",
            "\n",
            "reduce_mean_cls_loss =  0.00640399335\n",
            "\n",
            "regression loss =  15.7799311\n",
            "\n",
            "reduce_confident_loss =  0.330239236\n",
            "\n",
            "reduce_mean_cls_loss =  1.92112839\n",
            "\n",
            "regression loss =  18.7376728\n",
            "\n",
            "reduce_confident_loss =  0.579802632\n",
            "\n",
            "reduce_mean_cls_loss =  5.04809809\n",
            "\n",
            "regression loss =  13.9376287\n",
            "\n",
            "reduce_confident_loss =  0.577154815\n",
            "\n",
            "reduce_mean_cls_loss =  5.03563261\n",
            "\n",
            "regression loss =  12.7832975\n",
            "\n",
            "reduce_confident_loss =  0.579833329\n",
            "\n",
            "reduce_mean_cls_loss =  5.09030914\n",
            "\n",
            "regression loss =  12.4151669\n",
            "\n",
            "reduce_confident_loss =  0.472836703\n",
            "\n",
            "reduce_mean_cls_loss =  3.79252982\n",
            "\n",
            "regression loss =  15.8586283\n",
            "\n",
            "reduce_confident_loss =  0.208456919\n",
            "\n",
            "reduce_mean_cls_loss =  0.695192873\n",
            "\n",
            "regression loss =  16.4603043\n",
            "\n",
            "reduce_confident_loss =  0.186425313\n",
            "\n",
            "reduce_mean_cls_loss =  0.600126266\n",
            "\n",
            "regression loss =  12.27526\n",
            "\n",
            "reduce_confident_loss =  0.569246352\n",
            "\n",
            "reduce_mean_cls_loss =  5.06655645\n",
            "\n",
            "regression loss =  19.7041607\n",
            "\n",
            "reduce_confident_loss =  0.57342732\n",
            "\n",
            "reduce_mean_cls_loss =  0.00636885362\n",
            "\n",
            "regression loss =  12.7705507\n",
            "34/34 [==============================] - 1458s 43s/step - loss: 7.8329 - custom_mse: 240.5310 - recall: 0.2248 - val_loss: 18.8838 - val_custom_mse: 69.5739 - val_recall: 0.1490\n",
            "Epoch 6/10\n",
            "\n",
            "reduce_confident_loss =  0.0872263089\n",
            "\n",
            "reduce_mean_cls_loss =  0.0540582351\n",
            "\n",
            "regression loss =  8.59353352\n",
            " 1/34 [..............................] - ETA: 20:26 - loss: 8.7348 - custom_mse: 457.8046 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.428859383\n",
            "\n",
            "reduce_mean_cls_loss =  1.28531945\n",
            "\n",
            "regression loss =  9.0901041\n",
            " 2/34 [>.............................] - ETA: 19:28 - loss: 9.7696 - custom_mse: 66.0829 - recall: 0.1373     \n",
            "reduce_confident_loss =  0.517847\n",
            "\n",
            "reduce_mean_cls_loss =  1.58672452\n",
            "\n",
            "regression loss =  7.81236887\n",
            " 3/34 [=>............................] - ETA: 18:48 - loss: 9.8187 - custom_mse: 234.3880 - recall: 0.1749\n",
            "reduce_confident_loss =  0.108308367\n",
            "\n",
            "reduce_mean_cls_loss =  0.0193085708\n",
            "\n",
            "regression loss =  5.07571173\n",
            " 4/34 [==>...........................] - ETA: 18:05 - loss: 8.6648 - custom_mse: 275.8362 - recall: 0.1311\n",
            "reduce_confident_loss =  0.0858030841\n",
            "\n",
            "reduce_mean_cls_loss =  0.0182983447\n",
            "\n",
            "regression loss =  3.71873426\n",
            " 5/34 [===>..........................] - ETA: 17:31 - loss: 7.6964 - custom_mse: 288.3551 - recall: 0.1049\n",
            "reduce_confident_loss =  0.204979733\n",
            "\n",
            "reduce_mean_cls_loss =  0.102089129\n",
            "\n",
            "regression loss =  3.33382821\n",
            " 6/34 [====>.........................] - ETA: 16:44 - loss: 7.0205 - custom_mse: 199.3185 - recall: 0.0874\n",
            "reduce_confident_loss =  0.0798262\n",
            "\n",
            "reduce_mean_cls_loss =  0.158380479\n",
            "\n",
            "regression loss =  4.26485682\n",
            " 7/34 [=====>........................] - ETA: 16:02 - loss: 6.6609 - custom_mse: 153.8227 - recall: 0.0749\n",
            "reduce_confident_loss =  0.153003141\n",
            "\n",
            "reduce_mean_cls_loss =  0.200922862\n",
            "\n",
            "regression loss =  12.2356596\n",
            " 8/34 [======>.......................] - ETA: 15:25 - loss: 7.4020 - custom_mse: 129.1290 - recall: 0.0656\n",
            "reduce_confident_loss =  0.187246799\n",
            "\n",
            "reduce_mean_cls_loss =  0.599139094\n",
            "\n",
            "regression loss =  10.4001913\n",
            " 9/34 [======>.......................] - ETA: 14:50 - loss: 7.8225 - custom_mse: 157.7453 - recall: 0.0583\n",
            "reduce_confident_loss =  0.338035852\n",
            "\n",
            "reduce_mean_cls_loss =  0.206079721\n",
            "\n",
            "regression loss =  7.75433159\n",
            "10/34 [=======>......................] - ETA: 15:29 - loss: 7.8701 - custom_mse: 217.6935 - recall: 0.0525\n",
            "reduce_confident_loss =  0.361069351\n",
            "\n",
            "reduce_mean_cls_loss =  0.0545434169\n",
            "\n",
            "regression loss =  9.21127415\n",
            "11/34 [========>.....................] - ETA: 14:41 - loss: 8.0298 - custom_mse: 20.8901 - recall: 0.0477 \n",
            "reduce_confident_loss =  0.177697912\n",
            "\n",
            "reduce_mean_cls_loss =  0.102135949\n",
            "\n",
            "regression loss =  15.6710424\n",
            "12/34 [=========>....................] - ETA: 13:55 - loss: 8.6899 - custom_mse: 128.9911 - recall: 0.1270\n",
            "reduce_confident_loss =  0.421095878\n",
            "\n",
            "reduce_mean_cls_loss =  0.0410867743\n",
            "\n",
            "regression loss =  11.9196739\n",
            "13/34 [==========>...................] - ETA: 13:23 - loss: 8.9739 - custom_mse: 158.5317 - recall: 0.1942\n",
            "reduce_confident_loss =  0.52693665\n",
            "\n",
            "reduce_mean_cls_loss =  0.303188086\n",
            "\n",
            "regression loss =  8.0333786\n",
            "14/34 [===========>..................] - ETA: 12:40 - loss: 8.9660 - custom_mse: 219.0587 - recall: 0.2493\n",
            "reduce_confident_loss =  0.170165539\n",
            "\n",
            "reduce_mean_cls_loss =  0.802512586\n",
            "\n",
            "regression loss =  9.7513485\n",
            "15/34 [============>.................] - ETA: 11:57 - loss: 9.0832 - custom_mse: 235.3273 - recall: 0.2993\n",
            "reduce_confident_loss =  0.260804474\n",
            "\n",
            "reduce_mean_cls_loss =  0.257677138\n",
            "\n",
            "regression loss =  7.45702648\n",
            "16/34 [=============>................] - ETA: 11:15 - loss: 9.0140 - custom_mse: 157.3845 - recall: 0.3282\n",
            "reduce_confident_loss =  0.332746357\n",
            "\n",
            "reduce_mean_cls_loss =  0.100191288\n",
            "\n",
            "regression loss =  6.24020672\n",
            "17/34 [==============>...............] - ETA: 10:34 - loss: 8.8763 - custom_mse: 187.2303 - recall: 0.3089\n",
            "reduce_confident_loss =  0.353424281\n",
            "\n",
            "reduce_mean_cls_loss =  0.0250266232\n",
            "\n",
            "regression loss =  6.9015975\n",
            "18/34 [==============>...............] - ETA: 9:54 - loss: 8.7876 - custom_mse: 183.9115 - recall: 0.2918 \n",
            "reduce_confident_loss =  0.422343\n",
            "\n",
            "reduce_mean_cls_loss =  0.00819386542\n",
            "\n",
            "regression loss =  5.73776627\n",
            "19/34 [===============>..............] - ETA: 9:15 - loss: 8.6497 - custom_mse: 197.5005 - recall: 0.2764\n",
            "reduce_confident_loss =  0.134630039\n",
            "\n",
            "reduce_mean_cls_loss =  0.0150108272\n",
            "\n",
            "regression loss =  6.0787096\n",
            "20/34 [================>.............] - ETA: 8:37 - loss: 8.5287 - custom_mse: 301.4357 - recall: 0.2626\n",
            "reduce_confident_loss =  0.165128499\n",
            "\n",
            "reduce_mean_cls_loss =  0.0334269442\n",
            "\n",
            "regression loss =  3.1824162\n",
            "21/34 [=================>............] - ETA: 7:58 - loss: 8.2835 - custom_mse: 161.3890 - recall: 0.2792\n",
            "reduce_confident_loss =  0.244424552\n",
            "\n",
            "reduce_mean_cls_loss =  0.042462714\n",
            "\n",
            "regression loss =  4.24655437\n",
            "22/34 [==================>...........] - ETA: 7:20 - loss: 8.1131 - custom_mse: 142.5630 - recall: 0.2791\n",
            "reduce_confident_loss =  0.170838639\n",
            "\n",
            "reduce_mean_cls_loss =  0.259417772\n",
            "\n",
            "regression loss =  5.24364\n",
            "23/34 [===================>..........] - ETA: 6:43 - loss: 8.0070 - custom_mse: 145.3993 - recall: 0.2850\n",
            "reduce_confident_loss =  0.124318786\n",
            "\n",
            "reduce_mean_cls_loss =  0.214934245\n",
            "\n",
            "regression loss =  3.82858968\n",
            "24/34 [====================>.........] - ETA: 6:05 - loss: 7.8471 - custom_mse: 131.3007 - recall: 0.3003\n",
            "reduce_confident_loss =  0.330863744\n",
            "\n",
            "reduce_mean_cls_loss =  0.198703483\n",
            "\n",
            "regression loss =  3.74597859\n",
            "25/34 [=====================>........] - ETA: 5:28 - loss: 7.7042 - custom_mse: 57.3079 - recall: 0.3059 \n",
            "reduce_confident_loss =  0.486005932\n",
            "\n",
            "reduce_mean_cls_loss =  0.345915318\n",
            "\n",
            "regression loss =  6.26429892\n",
            "26/34 [=====================>........] - ETA: 4:51 - loss: 7.6808 - custom_mse: 45.6184 - recall: 0.3241\n",
            "reduce_confident_loss =  0.738350868\n",
            "\n",
            "reduce_mean_cls_loss =  0.844692528\n",
            "\n",
            "regression loss =  7.93582344\n",
            "27/34 [======================>.......] - ETA: 4:14 - loss: 7.7489 - custom_mse: 122.4542 - recall: 0.3304\n",
            "reduce_confident_loss =  0.343015641\n",
            "\n",
            "reduce_mean_cls_loss =  0.330970436\n",
            "\n",
            "regression loss =  8.37811279\n",
            "28/34 [=======================>......] - ETA: 3:40 - loss: 7.7954 - custom_mse: 203.3415 - recall: 0.3301\n",
            "reduce_confident_loss =  0.296479225\n",
            "\n",
            "reduce_mean_cls_loss =  0.089319177\n",
            "\n",
            "regression loss =  10.1772785\n",
            "29/34 [========================>.....] - ETA: 3:04 - loss: 7.8909 - custom_mse: 278.1140 - recall: 0.3187\n",
            "reduce_confident_loss =  0.101289339\n",
            "\n",
            "reduce_mean_cls_loss =  0.718687356\n",
            "\n",
            "regression loss =  7.84873295\n",
            "30/34 [=========================>....] - ETA: 2:28 - loss: 7.9168 - custom_mse: 198.9748 - recall: 0.3081\n",
            "reduce_confident_loss =  0.0840202048\n",
            "\n",
            "reduce_mean_cls_loss =  1.19398093\n",
            "\n",
            "regression loss =  5.88775682\n",
            "31/34 [==========================>...] - ETA: 1:51 - loss: 7.8926 - custom_mse: 194.4349 - recall: 0.2981\n",
            "reduce_confident_loss =  0.191749483\n",
            "\n",
            "reduce_mean_cls_loss =  0.0845304281\n",
            "\n",
            "regression loss =  6.69434547\n",
            "32/34 [===========================>..] - ETA: 1:14 - loss: 7.8638 - custom_mse: 348.4565 - recall: 0.2888\n",
            "reduce_confident_loss =  0.0733497962\n",
            "\n",
            "reduce_mean_cls_loss =  0.313845634\n",
            "\n",
            "regression loss =  8.81451797\n",
            "33/34 [============================>.] - ETA: 37s - loss: 7.9043 - custom_mse: 262.7253 - recall: 0.2801 \n",
            "reduce_confident_loss =  0.123163208\n",
            "\n",
            "reduce_mean_cls_loss =  0.102985539\n",
            "\n",
            "regression loss =  6.27444172\n",
            "34/34 [==============================] - ETA: 0s - loss: 7.8782 - custom_mse: 257.3602 - recall: 0.2718 \n",
            "reduce_confident_loss =  0.492040366\n",
            "\n",
            "reduce_mean_cls_loss =  5.37130976\n",
            "\n",
            "regression loss =  15.5829697\n",
            "\n",
            "reduce_confident_loss =  0.122037105\n",
            "\n",
            "reduce_mean_cls_loss =  0.902075887\n",
            "\n",
            "regression loss =  10.3903379\n",
            "\n",
            "reduce_confident_loss =  0.498664588\n",
            "\n",
            "reduce_mean_cls_loss =  5.36253595\n",
            "\n",
            "regression loss =  24.3447647\n",
            "\n",
            "reduce_confident_loss =  0.49442026\n",
            "\n",
            "reduce_mean_cls_loss =  0.00438994775\n",
            "\n",
            "regression loss =  18.2482395\n",
            "\n",
            "reduce_confident_loss =  0.233305216\n",
            "\n",
            "reduce_mean_cls_loss =  2.17269635\n",
            "\n",
            "regression loss =  20.2431774\n",
            "\n",
            "reduce_confident_loss =  0.488242298\n",
            "\n",
            "reduce_mean_cls_loss =  5.380898\n",
            "\n",
            "regression loss =  15.4423428\n",
            "\n",
            "reduce_confident_loss =  0.487633198\n",
            "\n",
            "reduce_mean_cls_loss =  5.40177822\n",
            "\n",
            "regression loss =  11.5893707\n",
            "\n",
            "reduce_confident_loss =  0.504872382\n",
            "\n",
            "reduce_mean_cls_loss =  5.44226933\n",
            "\n",
            "regression loss =  11.2541418\n",
            "\n",
            "reduce_confident_loss =  0.385284364\n",
            "\n",
            "reduce_mean_cls_loss =  4.02543211\n",
            "\n",
            "regression loss =  16.2004623\n",
            "\n",
            "reduce_confident_loss =  0.125468969\n",
            "\n",
            "reduce_mean_cls_loss =  0.876094818\n",
            "\n",
            "regression loss =  15.4152117\n",
            "\n",
            "reduce_confident_loss =  0.125994876\n",
            "\n",
            "reduce_mean_cls_loss =  0.773566902\n",
            "\n",
            "regression loss =  12.3697166\n",
            "\n",
            "reduce_confident_loss =  0.49148792\n",
            "\n",
            "reduce_mean_cls_loss =  5.30589056\n",
            "\n",
            "regression loss =  20.6421528\n",
            "\n",
            "reduce_confident_loss =  0.482468545\n",
            "\n",
            "reduce_mean_cls_loss =  0.00490006479\n",
            "\n",
            "regression loss =  14.3509092\n",
            "34/34 [==============================] - 1450s 43s/step - loss: 7.8782 - custom_mse: 257.3602 - recall: 0.2718 - val_loss: 19.5926 - val_custom_mse: 118.7207 - val_recall: 0.0721\n",
            "Epoch 7/10\n",
            "\n",
            "reduce_confident_loss =  0.0931577161\n",
            "\n",
            "reduce_mean_cls_loss =  0.0884736776\n",
            "\n",
            "regression loss =  5.24813271\n",
            " 1/34 [..............................] - ETA: 20:34 - loss: 5.4298 - custom_mse: 384.2621 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.489999771\n",
            "\n",
            "reduce_mean_cls_loss =  0.94128567\n",
            "\n",
            "regression loss =  7.23299837\n",
            " 2/34 [>.............................] - ETA: 19:28 - loss: 7.0470 - custom_mse: 63.8746 - recall: 0.2499     \n",
            "reduce_confident_loss =  0.401029497\n",
            "\n",
            "reduce_mean_cls_loss =  1.37951279\n",
            "\n",
            "regression loss =  7.0019908\n",
            " 3/34 [=>............................] - ETA: 18:55 - loss: 7.6255 - custom_mse: 223.7126 - recall: 0.1666\n",
            "reduce_confident_loss =  0.105989262\n",
            "\n",
            "reduce_mean_cls_loss =  0.0287413765\n",
            "\n",
            "regression loss =  3.05377722\n",
            " 4/34 [==>...........................] - ETA: 18:18 - loss: 6.5163 - custom_mse: 295.5190 - recall: 0.1249\n",
            "reduce_confident_loss =  0.128576502\n",
            "\n",
            "reduce_mean_cls_loss =  0.129856452\n",
            "\n",
            "regression loss =  2.65973163\n",
            " 5/34 [===>..........................] - ETA: 17:41 - loss: 5.7967 - custom_mse: 239.3601 - recall: 0.1000\n",
            "reduce_confident_loss =  0.107944779\n",
            "\n",
            "reduce_mean_cls_loss =  0.0825305\n",
            "\n",
            "regression loss =  2.31031179\n",
            " 6/34 [====>.........................] - ETA: 17:04 - loss: 5.2473 - custom_mse: 235.4710 - recall: 0.0833\n",
            "reduce_confident_loss =  0.0713043213\n",
            "\n",
            "reduce_mean_cls_loss =  0.149139196\n",
            "\n",
            "regression loss =  2.56078434\n",
            " 7/34 [=====>........................] - ETA: 16:55 - loss: 4.8950 - custom_mse: 173.8201 - recall: 0.0714\n",
            "reduce_confident_loss =  0.119327344\n",
            "\n",
            "reduce_mean_cls_loss =  0.167207211\n",
            "\n",
            "regression loss =  7.82719851\n",
            " 8/34 [======>.......................] - ETA: 16:16 - loss: 5.2974 - custom_mse: 106.7138 - recall: 0.0625\n",
            "reduce_confident_loss =  0.217339888\n",
            "\n",
            "reduce_mean_cls_loss =  0.264950305\n",
            "\n",
            "regression loss =  10.1153021\n",
            " 9/34 [======>.......................] - ETA: 15:37 - loss: 5.8863 - custom_mse: 158.2135 - recall: 0.0555\n",
            "reduce_confident_loss =  0.258294314\n",
            "\n",
            "reduce_mean_cls_loss =  0.0756024644\n",
            "\n",
            "regression loss =  8.1168642\n",
            "10/34 [=======>......................] - ETA: 16:06 - loss: 6.1427 - custom_mse: 211.8783 - recall: 0.0500\n",
            "reduce_confident_loss =  0.340080917\n",
            "\n",
            "reduce_mean_cls_loss =  0.00321855652\n",
            "\n",
            "regression loss =  11.005909\n",
            "11/34 [========>.....................] - ETA: 15:13 - loss: 6.6161 - custom_mse: 22.8498 - recall: 0.0454 \n",
            "reduce_confident_loss =  0.209049463\n",
            "\n",
            "reduce_mean_cls_loss =  0.269699961\n",
            "\n",
            "regression loss =  12.9689732\n",
            "12/34 [=========>....................] - ETA: 14:23 - loss: 7.1854 - custom_mse: 115.6631 - recall: 0.1250\n",
            "reduce_confident_loss =  0.38058266\n",
            "\n",
            "reduce_mean_cls_loss =  0.0513337664\n",
            "\n",
            "regression loss =  12.7458792\n",
            "13/34 [==========>...................] - ETA: 13:37 - loss: 7.6463 - custom_mse: 171.7462 - recall: 0.1897\n",
            "reduce_confident_loss =  0.673447907\n",
            "\n",
            "reduce_mean_cls_loss =  0.467727453\n",
            "\n",
            "regression loss =  7.31295729\n",
            "14/34 [===========>..................] - ETA: 12:52 - loss: 7.7040 - custom_mse: 241.9592 - recall: 0.2414\n",
            "reduce_confident_loss =  0.222879276\n",
            "\n",
            "reduce_mean_cls_loss =  0.791025043\n",
            "\n",
            "regression loss =  8.87022591\n",
            "15/34 [============>.................] - ETA: 12:08 - loss: 7.8494 - custom_mse: 230.6864 - recall: 0.2920\n",
            "reduce_confident_loss =  0.351379931\n",
            "\n",
            "reduce_mean_cls_loss =  0.0735783949\n",
            "\n",
            "regression loss =  6.72062874\n",
            "16/34 [=============>................] - ETA: 11:25 - loss: 7.8054 - custom_mse: 187.4577 - recall: 0.3293\n",
            "reduce_confident_loss =  0.398539126\n",
            "\n",
            "reduce_mean_cls_loss =  0.00477241306\n",
            "\n",
            "regression loss =  4.80225039\n",
            "17/34 [==============>...............] - ETA: 10:44 - loss: 7.6524 - custom_mse: 196.8853 - recall: 0.3100\n",
            "reduce_confident_loss =  0.299726903\n",
            "\n",
            "reduce_mean_cls_loss =  0.0538856052\n",
            "\n",
            "regression loss =  4.34716034\n",
            "18/34 [==============>...............] - ETA: 10:02 - loss: 7.4885 - custom_mse: 242.3351 - recall: 0.2927\n",
            "reduce_confident_loss =  0.283484221\n",
            "\n",
            "reduce_mean_cls_loss =  0.00449411059\n",
            "\n",
            "regression loss =  5.18727303\n",
            "19/34 [===============>..............] - ETA: 9:24 - loss: 7.3825 - custom_mse: 253.2218 - recall: 0.2773 \n",
            "reduce_confident_loss =  0.0984319597\n",
            "\n",
            "reduce_mean_cls_loss =  0.0401460268\n",
            "\n",
            "regression loss =  5.18070078\n",
            "20/34 [================>.............] - ETA: 8:44 - loss: 7.2793 - custom_mse: 339.0917 - recall: 0.2635\n",
            "reduce_confident_loss =  0.107381478\n",
            "\n",
            "reduce_mean_cls_loss =  0.018392954\n",
            "\n",
            "regression loss =  3.07793522\n",
            "21/34 [=================>............] - ETA: 8:04 - loss: 7.0853 - custom_mse: 198.1880 - recall: 0.2509\n",
            "reduce_confident_loss =  0.12835367\n",
            "\n",
            "reduce_mean_cls_loss =  0.11614202\n",
            "\n",
            "regression loss =  4.19205666\n",
            "22/34 [==================>...........] - ETA: 7:25 - loss: 6.9649 - custom_mse: 180.1707 - recall: 0.2395\n",
            "reduce_confident_loss =  0.176074982\n",
            "\n",
            "reduce_mean_cls_loss =  0.0761413574\n",
            "\n",
            "regression loss =  4.96723795\n",
            "23/34 [===================>..........] - ETA: 6:47 - loss: 6.8890 - custom_mse: 172.9561 - recall: 0.2298\n",
            "reduce_confident_loss =  0.16286622\n",
            "\n",
            "reduce_mean_cls_loss =  0.151702881\n",
            "\n",
            "regression loss =  3.94076157\n",
            "24/34 [====================>.........] - ETA: 6:12 - loss: 6.7792 - custom_mse: 152.1720 - recall: 0.2202\n",
            "reduce_confident_loss =  0.33867082\n",
            "\n",
            "reduce_mean_cls_loss =  0.509854555\n",
            "\n",
            "regression loss =  3.87755513\n",
            "25/34 [=====================>........] - ETA: 5:34 - loss: 6.6971 - custom_mse: 80.2532 - recall: 0.2304 \n",
            "reduce_confident_loss =  0.534339607\n",
            "\n",
            "reduce_mean_cls_loss =  1.12904096\n",
            "\n",
            "regression loss =  6.22159958\n",
            "26/34 [=====================>........] - ETA: 4:56 - loss: 6.7428 - custom_mse: 66.0131 - recall: 0.2501\n",
            "reduce_confident_loss =  0.633059084\n",
            "\n",
            "reduce_mean_cls_loss =  0.789992929\n",
            "\n",
            "regression loss =  6.8428731\n",
            "27/34 [======================>.......] - ETA: 4:18 - loss: 6.7992 - custom_mse: 134.5782 - recall: 0.2586\n",
            "reduce_confident_loss =  0.217054367\n",
            "\n",
            "reduce_mean_cls_loss =  0.542868495\n",
            "\n",
            "regression loss =  7.77166367\n",
            "28/34 [=======================>......] - ETA: 3:41 - loss: 6.8611 - custom_mse: 212.3856 - recall: 0.2493\n",
            "reduce_confident_loss =  0.337322444\n",
            "\n",
            "reduce_mean_cls_loss =  0.0211034119\n",
            "\n",
            "regression loss =  7.98704672\n",
            "29/34 [========================>.....] - ETA: 3:05 - loss: 6.9123 - custom_mse: 256.5380 - recall: 0.2407\n",
            "reduce_confident_loss =  0.0332897492\n",
            "\n",
            "reduce_mean_cls_loss =  0.281354\n",
            "\n",
            "regression loss =  6.85264683\n",
            "30/34 [=========================>....] - ETA: 2:28 - loss: 6.9208 - custom_mse: 198.7485 - recall: 0.2327\n",
            "reduce_confident_loss =  0.0617821217\n",
            "\n",
            "reduce_mean_cls_loss =  0.658389509\n",
            "\n",
            "regression loss =  4.35485029\n",
            "31/34 [==========================>...] - ETA: 1:50 - loss: 6.8612 - custom_mse: 181.1600 - recall: 0.2252\n",
            "reduce_confident_loss =  0.197871\n",
            "\n",
            "reduce_mean_cls_loss =  0.773795962\n",
            "\n",
            "regression loss =  6.86056471\n",
            "32/34 [===========================>..] - ETA: 1:13 - loss: 6.8916 - custom_mse: 361.8666 - recall: 0.2182\n",
            "reduce_confident_loss =  0.0777813196\n",
            "\n",
            "reduce_mean_cls_loss =  0.0670375\n",
            "\n",
            "regression loss =  8.80227757\n",
            "33/34 [============================>.] - ETA: 36s - loss: 6.9539 - custom_mse: 292.8003 - recall: 0.2116 \n",
            "reduce_confident_loss =  0.0970145762\n",
            "\n",
            "reduce_mean_cls_loss =  0.105855271\n",
            "\n",
            "regression loss =  6.08197689\n",
            "34/34 [==============================] - ETA: 0s - loss: 6.9414 - custom_mse: 202.5954 - recall: 0.2053 \n",
            "reduce_confident_loss =  0.514966547\n",
            "\n",
            "reduce_mean_cls_loss =  4.92837954\n",
            "\n",
            "regression loss =  15.7043934\n",
            "\n",
            "reduce_confident_loss =  0.143294364\n",
            "\n",
            "reduce_mean_cls_loss =  0.887877047\n",
            "\n",
            "regression loss =  9.6495266\n",
            "\n",
            "reduce_confident_loss =  0.501760423\n",
            "\n",
            "reduce_mean_cls_loss =  4.91522551\n",
            "\n",
            "regression loss =  22.0030804\n",
            "\n",
            "reduce_confident_loss =  0.507784665\n",
            "\n",
            "reduce_mean_cls_loss =  0.00730107306\n",
            "\n",
            "regression loss =  16.6558037\n",
            "\n",
            "reduce_confident_loss =  0.24690187\n",
            "\n",
            "reduce_mean_cls_loss =  1.97471023\n",
            "\n",
            "regression loss =  19.8161888\n",
            "\n",
            "reduce_confident_loss =  0.497808218\n",
            "\n",
            "reduce_mean_cls_loss =  4.85278749\n",
            "\n",
            "regression loss =  11.0018358\n",
            "\n",
            "reduce_confident_loss =  0.493858963\n",
            "\n",
            "reduce_mean_cls_loss =  4.94056034\n",
            "\n",
            "regression loss =  12.0861607\n",
            "\n",
            "reduce_confident_loss =  0.510462165\n",
            "\n",
            "reduce_mean_cls_loss =  5.01915836\n",
            "\n",
            "regression loss =  11.6232023\n",
            "\n",
            "reduce_confident_loss =  0.40627709\n",
            "\n",
            "reduce_mean_cls_loss =  3.69332314\n",
            "\n",
            "regression loss =  15.4488487\n",
            "\n",
            "reduce_confident_loss =  0.137528554\n",
            "\n",
            "reduce_mean_cls_loss =  0.876257062\n",
            "\n",
            "regression loss =  16.0186329\n",
            "\n",
            "reduce_confident_loss =  0.141077012\n",
            "\n",
            "reduce_mean_cls_loss =  0.779379845\n",
            "\n",
            "regression loss =  11.3859425\n",
            "\n",
            "reduce_confident_loss =  0.509382188\n",
            "\n",
            "reduce_mean_cls_loss =  4.9639411\n",
            "\n",
            "regression loss =  19.996954\n",
            "\n",
            "reduce_confident_loss =  0.492079258\n",
            "\n",
            "reduce_mean_cls_loss =  0.00774699915\n",
            "\n",
            "regression loss =  13.5235176\n",
            "34/34 [==============================] - 1424s 42s/step - loss: 6.9414 - custom_mse: 202.5954 - recall: 0.2053 - val_loss: 18.4905 - val_custom_mse: 112.6415 - val_recall: 0.0721\n",
            "Epoch 8/10\n",
            "\n",
            "reduce_confident_loss =  0.0677906275\n",
            "\n",
            "reduce_mean_cls_loss =  0.0327711403\n",
            "\n",
            "regression loss =  4.36676073\n",
            " 1/34 [..............................] - ETA: 22:11 - loss: 4.4673 - custom_mse: 441.6354 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.375953734\n",
            "\n",
            "reduce_mean_cls_loss =  0.669628263\n",
            "\n",
            "regression loss =  6.68897486\n",
            " 2/34 [>.............................] - ETA: 19:35 - loss: 6.1009 - custom_mse: 75.2190 - recall: 0.1721     \n",
            "reduce_confident_loss =  0.440099984\n",
            "\n",
            "reduce_mean_cls_loss =  1.3360641\n",
            "\n",
            "regression loss =  7.4168\n",
            " 3/34 [=>............................] - ETA: 18:23 - loss: 7.1316 - custom_mse: 275.9717 - recall: 0.1148\n",
            "reduce_confident_loss =  0.133463621\n",
            "\n",
            "reduce_mean_cls_loss =  0.0763436928\n",
            "\n",
            "regression loss =  2.14157867\n",
            " 4/34 [==>...........................] - ETA: 17:39 - loss: 5.9366 - custom_mse: 313.4792 - recall: 0.0861\n",
            "reduce_confident_loss =  0.137891158\n",
            "\n",
            "reduce_mean_cls_loss =  0.0295029841\n",
            "\n",
            "regression loss =  3.17461634\n",
            " 5/34 [===>..........................] - ETA: 16:59 - loss: 5.4176 - custom_mse: 250.9097 - recall: 0.0689\n",
            "reduce_confident_loss =  0.0778092\n",
            "\n",
            "reduce_mean_cls_loss =  0.0183872245\n",
            "\n",
            "regression loss =  2.32879257\n",
            " 6/34 [====>.........................] - ETA: 16:30 - loss: 4.9189 - custom_mse: 221.1280 - recall: 0.0574\n",
            "reduce_confident_loss =  0.117778145\n",
            "\n",
            "reduce_mean_cls_loss =  0.122688226\n",
            "\n",
            "regression loss =  2.67765021\n",
            " 7/34 [=====>........................] - ETA: 15:50 - loss: 4.6330 - custom_mse: 153.4882 - recall: 0.0492\n",
            "reduce_confident_loss =  0.141914383\n",
            "\n",
            "reduce_mean_cls_loss =  0.0877850205\n",
            "\n",
            "regression loss =  7.8941741\n",
            " 8/34 [======>.......................] - ETA: 15:13 - loss: 5.0694 - custom_mse: 116.7496 - recall: 0.0430\n",
            "reduce_confident_loss =  0.140941143\n",
            "\n",
            "reduce_mean_cls_loss =  0.56552273\n",
            "\n",
            "regression loss =  9.09374619\n",
            " 9/34 [======>.......................] - ETA: 14:36 - loss: 5.5950 - custom_mse: 154.2874 - recall: 0.0383\n",
            "reduce_confident_loss =  0.356063575\n",
            "\n",
            "reduce_mean_cls_loss =  0.104647689\n",
            "\n",
            "regression loss =  8.00958061\n",
            "10/34 [=======>......................] - ETA: 15:13 - loss: 5.8826 - custom_mse: 201.5745 - recall: 0.0844\n",
            "reduce_confident_loss =  0.137770697\n",
            "\n",
            "reduce_mean_cls_loss =  0.121120416\n",
            "\n",
            "regression loss =  7.09989309\n",
            "11/34 [========>.....................] - ETA: 14:28 - loss: 6.0168 - custom_mse: 18.8153 - recall: 0.0768 \n",
            "reduce_confident_loss =  0.190457717\n",
            "\n",
            "reduce_mean_cls_loss =  0.187050343\n",
            "\n",
            "regression loss =  12.8296413\n",
            "12/34 [=========>....................] - ETA: 13:46 - loss: 6.6160 - custom_mse: 98.1555 - recall: 0.1537\n",
            "reduce_confident_loss =  0.228926033\n",
            "\n",
            "reduce_mean_cls_loss =  0.0500127599\n",
            "\n",
            "regression loss =  12.8502369\n",
            "13/34 [==========>...................] - ETA: 13:03 - loss: 7.1170 - custom_mse: 187.0335 - recall: 0.1930\n",
            "reduce_confident_loss =  0.664681852\n",
            "\n",
            "reduce_mean_cls_loss =  0.387403816\n",
            "\n",
            "regression loss =  6.37024689\n",
            "14/34 [===========>..................] - ETA: 12:30 - loss: 7.1388 - custom_mse: 224.9987 - recall: 0.2449\n",
            "reduce_confident_loss =  0.234349623\n",
            "\n",
            "reduce_mean_cls_loss =  0.888546944\n",
            "\n",
            "regression loss =  8.28576756\n",
            "15/34 [============>.................] - ETA: 11:48 - loss: 7.2901 - custom_mse: 209.2266 - recall: 0.2952\n",
            "reduce_confident_loss =  0.253449708\n",
            "\n",
            "reduce_mean_cls_loss =  0.280176848\n",
            "\n",
            "regression loss =  6.50971174\n",
            "16/34 [=============>................] - ETA: 11:07 - loss: 7.2747 - custom_mse: 171.1941 - recall: 0.3284\n",
            "reduce_confident_loss =  0.354908884\n",
            "\n",
            "reduce_mean_cls_loss =  0.00531562511\n",
            "\n",
            "regression loss =  3.14647269\n",
            "17/34 [==============>...............] - ETA: 10:27 - loss: 7.0531 - custom_mse: 197.7960 - recall: 0.3091\n",
            "reduce_confident_loss =  0.302134037\n",
            "\n",
            "reduce_mean_cls_loss =  0.00547510898\n",
            "\n",
            "regression loss =  2.8798995\n",
            "18/34 [==============>...............] - ETA: 9:53 - loss: 6.8383 - custom_mse: 200.5995 - recall: 0.2919 \n",
            "reduce_confident_loss =  0.248628274\n",
            "\n",
            "reduce_mean_cls_loss =  0.0187869109\n",
            "\n",
            "regression loss =  5.48002672\n",
            "19/34 [===============>..............] - ETA: 9:14 - loss: 6.7809 - custom_mse: 248.1788 - recall: 0.2766\n",
            "reduce_confident_loss =  0.0746426061\n",
            "\n",
            "reduce_mean_cls_loss =  0.0079165455\n",
            "\n",
            "regression loss =  5.15414953\n",
            "20/34 [================>.............] - ETA: 8:35 - loss: 6.7037 - custom_mse: 335.8805 - recall: 0.2627\n",
            "reduce_confident_loss =  0.17927587\n",
            "\n",
            "reduce_mean_cls_loss =  0.00525662722\n",
            "\n",
            "regression loss =  2.8138814\n",
            "21/34 [=================>............] - ETA: 7:56 - loss: 6.5272 - custom_mse: 179.5523 - recall: 0.2572\n",
            "reduce_confident_loss =  0.158927\n",
            "\n",
            "reduce_mean_cls_loss =  0.0543652661\n",
            "\n",
            "regression loss =  4.53711653\n",
            "22/34 [==================>...........] - ETA: 7:18 - loss: 6.4465 - custom_mse: 162.1195 - recall: 0.2483\n",
            "reduce_confident_loss =  0.19225046\n",
            "\n",
            "reduce_mean_cls_loss =  0.177283973\n",
            "\n",
            "regression loss =  4.77564096\n",
            "23/34 [===================>..........] - ETA: 6:40 - loss: 6.3899 - custom_mse: 167.1167 - recall: 0.2485\n",
            "reduce_confident_loss =  0.200690374\n",
            "\n",
            "reduce_mean_cls_loss =  0.146233797\n",
            "\n",
            "regression loss =  3.51858974\n",
            "24/34 [====================>.........] - ETA: 6:03 - loss: 6.2847 - custom_mse: 148.4716 - recall: 0.2413\n",
            "reduce_confident_loss =  0.339136124\n",
            "\n",
            "reduce_mean_cls_loss =  0.259649247\n",
            "\n",
            "regression loss =  3.24133325\n",
            "25/34 [=====================>........] - ETA: 5:26 - loss: 6.1869 - custom_mse: 67.6277 - recall: 0.2468 \n",
            "reduce_confident_loss =  0.43945536\n",
            "\n",
            "reduce_mean_cls_loss =  0.737727463\n",
            "\n",
            "regression loss =  5.76640415\n",
            "26/34 [=====================>........] - ETA: 4:49 - loss: 6.2160 - custom_mse: 64.7871 - recall: 0.2722\n",
            "reduce_confident_loss =  0.583672822\n",
            "\n",
            "reduce_mean_cls_loss =  0.827723801\n",
            "\n",
            "regression loss =  6.66420841\n",
            "27/34 [======================>.......] - ETA: 4:12 - loss: 6.2849 - custom_mse: 137.5565 - recall: 0.2827\n",
            "reduce_confident_loss =  0.332029283\n",
            "\n",
            "reduce_mean_cls_loss =  0.383645\n",
            "\n",
            "regression loss =  7.45340252\n",
            "28/34 [=======================>......] - ETA: 3:37 - loss: 6.3522 - custom_mse: 195.3943 - recall: 0.2787\n",
            "reduce_confident_loss =  0.282482\n",
            "\n",
            "reduce_mean_cls_loss =  0.00946471747\n",
            "\n",
            "regression loss =  9.03086\n",
            "29/34 [========================>.....] - ETA: 3:01 - loss: 6.4546 - custom_mse: 264.0236 - recall: 0.2691\n",
            "reduce_confident_loss =  0.0431047678\n",
            "\n",
            "reduce_mean_cls_loss =  0.430662721\n",
            "\n",
            "regression loss =  6.49247265\n",
            "30/34 [=========================>....] - ETA: 2:24 - loss: 6.4717 - custom_mse: 197.9148 - recall: 0.2601\n",
            "reduce_confident_loss =  0.0357682109\n",
            "\n",
            "reduce_mean_cls_loss =  1.12292969\n",
            "\n",
            "regression loss =  4.55399799\n",
            "31/34 [==========================>...] - ETA: 1:48 - loss: 6.4472 - custom_mse: 193.3876 - recall: 0.2517\n",
            "reduce_confident_loss =  0.172832459\n",
            "\n",
            "reduce_mean_cls_loss =  0.184130922\n",
            "\n",
            "regression loss =  6.62278891\n",
            "32/34 [===========================>..] - ETA: 1:12 - loss: 6.4638 - custom_mse: 330.5718 - recall: 0.2438\n",
            "reduce_confident_loss =  0.0373412259\n",
            "\n",
            "reduce_mean_cls_loss =  0.214900866\n",
            "\n",
            "regression loss =  8.90773678\n",
            "33/34 [============================>.] - ETA: 36s - loss: 6.5455 - custom_mse: 275.5861 - recall: 0.2364 \n",
            "reduce_confident_loss =  0.0737557411\n",
            "\n",
            "reduce_mean_cls_loss =  0.388716489\n",
            "\n",
            "regression loss =  6.95454836\n",
            "34/34 [==============================] - ETA: 0s - loss: 6.5617 - custom_mse: 227.4833 - recall: 0.2295 \n",
            "reduce_confident_loss =  0.53675127\n",
            "\n",
            "reduce_mean_cls_loss =  5.4993329\n",
            "\n",
            "regression loss =  15.486352\n",
            "\n",
            "reduce_confident_loss =  0.15220353\n",
            "\n",
            "reduce_mean_cls_loss =  0.892435074\n",
            "\n",
            "regression loss =  9.69422531\n",
            "\n",
            "reduce_confident_loss =  0.495450497\n",
            "\n",
            "reduce_mean_cls_loss =  4.8037\n",
            "\n",
            "regression loss =  23.317873\n",
            "\n",
            "reduce_confident_loss =  0.515067101\n",
            "\n",
            "reduce_mean_cls_loss =  0.00670696469\n",
            "\n",
            "regression loss =  17.0624695\n",
            "\n",
            "reduce_confident_loss =  0.255454719\n",
            "\n",
            "reduce_mean_cls_loss =  2.01194572\n",
            "\n",
            "regression loss =  20.8413296\n",
            "\n",
            "reduce_confident_loss =  0.491946489\n",
            "\n",
            "reduce_mean_cls_loss =  4.82254028\n",
            "\n",
            "regression loss =  14.0464382\n",
            "\n",
            "reduce_confident_loss =  0.496105403\n",
            "\n",
            "reduce_mean_cls_loss =  4.95229769\n",
            "\n",
            "regression loss =  11.8286047\n",
            "\n",
            "reduce_confident_loss =  0.504703045\n",
            "\n",
            "reduce_mean_cls_loss =  4.95734501\n",
            "\n",
            "regression loss =  11.5047026\n",
            "\n",
            "reduce_confident_loss =  0.416998774\n",
            "\n",
            "reduce_mean_cls_loss =  3.58499503\n",
            "\n",
            "regression loss =  15.734396\n",
            "\n",
            "reduce_confident_loss =  0.146684319\n",
            "\n",
            "reduce_mean_cls_loss =  0.901836336\n",
            "\n",
            "regression loss =  15.7185783\n",
            "\n",
            "reduce_confident_loss =  0.143898726\n",
            "\n",
            "reduce_mean_cls_loss =  0.818429887\n",
            "\n",
            "regression loss =  12.1224518\n",
            "\n",
            "reduce_confident_loss =  0.50285393\n",
            "\n",
            "reduce_mean_cls_loss =  4.92646122\n",
            "\n",
            "regression loss =  20.41469\n",
            "\n",
            "reduce_confident_loss =  0.491875\n",
            "\n",
            "reduce_mean_cls_loss =  0.00817842782\n",
            "\n",
            "regression loss =  13.3604717\n",
            "34/34 [==============================] - 1404s 41s/step - loss: 6.5617 - custom_mse: 227.4833 - recall: 0.2295 - val_loss: 19.0289 - val_custom_mse: 106.0533 - val_recall: 0.0721\n",
            "Epoch 9/10\n",
            "\n",
            "reduce_confident_loss =  0.0404873416\n",
            "\n",
            "reduce_mean_cls_loss =  0.0457693674\n",
            "\n",
            "regression loss =  3.51932645\n",
            " 1/34 [..............................] - ETA: 19:46 - loss: 3.6056 - custom_mse: 430.1937 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.560913622\n",
            "\n",
            "reduce_mean_cls_loss =  1.21432436\n",
            "\n",
            "regression loss =  6.15193033\n",
            " 2/34 [>.............................] - ETA: 17:56 - loss: 5.7664 - custom_mse: 67.1235 - recall: 0.0000e+00 \n",
            "reduce_confident_loss =  0.46270439\n",
            "\n",
            "reduce_mean_cls_loss =  1.10665965\n",
            "\n",
            "regression loss =  6.203897\n",
            " 3/34 [=>............................] - ETA: 17:59 - loss: 6.4353 - custom_mse: 282.9343 - recall: 0.0370   \n",
            "reduce_confident_loss =  0.172105148\n",
            "\n",
            "reduce_mean_cls_loss =  0.0348028727\n",
            "\n",
            "regression loss =  2.81041408\n",
            " 4/34 [==>...........................] - ETA: 18:13 - loss: 5.5808 - custom_mse: 303.4876 - recall: 0.0278\n",
            "reduce_confident_loss =  0.110453539\n",
            "\n",
            "reduce_mean_cls_loss =  0.0742772818\n",
            "\n",
            "regression loss =  2.60094333\n",
            " 5/34 [===>..........................] - ETA: 17:32 - loss: 5.0218 - custom_mse: 284.1337 - recall: 0.0222\n",
            "reduce_confident_loss =  0.0479906388\n",
            "\n",
            "reduce_mean_cls_loss =  0.0650404543\n",
            "\n",
            "regression loss =  2.44248366\n",
            " 6/34 [====>.........................] - ETA: 16:48 - loss: 4.6108 - custom_mse: 219.2880 - recall: 0.0185\n",
            "reduce_confident_loss =  0.0523102954\n",
            "\n",
            "reduce_mean_cls_loss =  0.0230081193\n",
            "\n",
            "regression loss =  2.8538847\n",
            " 7/34 [=====>........................] - ETA: 16:10 - loss: 4.3705 - custom_mse: 183.1037 - recall: 0.0159\n",
            "reduce_confident_loss =  0.0889466181\n",
            "\n",
            "reduce_mean_cls_loss =  0.028130237\n",
            "\n",
            "regression loss =  9.06144905\n",
            " 8/34 [======>.......................] - ETA: 15:33 - loss: 4.9715 - custom_mse: 135.4638 - recall: 0.0139\n",
            "reduce_confident_loss =  0.32977131\n",
            "\n",
            "reduce_mean_cls_loss =  0.142736048\n",
            "\n",
            "regression loss =  8.97227859\n",
            " 9/34 [======>.......................] - ETA: 14:58 - loss: 5.4686 - custom_mse: 168.9397 - recall: 0.0123\n",
            "reduce_confident_loss =  0.284591883\n",
            "\n",
            "reduce_mean_cls_loss =  0.12858507\n",
            "\n",
            "regression loss =  7.03109264\n",
            "10/34 [=======>......................] - ETA: 15:36 - loss: 5.6661 - custom_mse: 231.1210 - recall: 0.1111\n",
            "reduce_confident_loss =  0.186569378\n",
            "\n",
            "reduce_mean_cls_loss =  0.00599495554\n",
            "\n",
            "regression loss =  8.7539711\n",
            "11/34 [========>.....................] - ETA: 15:16 - loss: 5.9643 - custom_mse: 20.5856 - recall: 0.1010 \n",
            "reduce_confident_loss =  0.224641532\n",
            "\n",
            "reduce_mean_cls_loss =  0.236172989\n",
            "\n",
            "regression loss =  12.8605995\n",
            "12/34 [=========>....................] - ETA: 14:32 - loss: 6.5774 - custom_mse: 111.6237 - recall: 0.1759\n",
            "reduce_confident_loss =  0.410952628\n",
            "\n",
            "reduce_mean_cls_loss =  0.0871482864\n",
            "\n",
            "regression loss =  12.7695198\n",
            "13/34 [==========>...................] - ETA: 13:46 - loss: 7.0921 - custom_mse: 161.6125 - recall: 0.2377\n",
            "reduce_confident_loss =  0.465589374\n",
            "\n",
            "reduce_mean_cls_loss =  0.367199153\n",
            "\n",
            "regression loss =  6.75534916\n",
            "14/34 [===========>..................] - ETA: 13:02 - loss: 7.1275 - custom_mse: 204.1592 - recall: 0.2766\n",
            "reduce_confident_loss =  0.221796274\n",
            "\n",
            "reduce_mean_cls_loss =  0.842135549\n",
            "\n",
            "regression loss =  8.32586384\n",
            "15/34 [============>.................] - ETA: 12:18 - loss: 7.2783 - custom_mse: 222.9339 - recall: 0.3248\n",
            "reduce_confident_loss =  0.316142708\n",
            "\n",
            "reduce_mean_cls_loss =  0.0694737583\n",
            "\n",
            "regression loss =  6.024261\n",
            "16/34 [=============>................] - ETA: 11:36 - loss: 7.2240 - custom_mse: 175.6732 - recall: 0.3457\n",
            "reduce_confident_loss =  0.456864625\n",
            "\n",
            "reduce_mean_cls_loss =  0.000876860053\n",
            "\n",
            "regression loss =  2.56272888\n",
            "17/34 [==============>...............] - ETA: 10:55 - loss: 6.9768 - custom_mse: 172.4469 - recall: 0.3253\n",
            "reduce_confident_loss =  0.412855774\n",
            "\n",
            "reduce_mean_cls_loss =  0.00536160823\n",
            "\n",
            "regression loss =  2.06482482\n",
            "18/34 [==============>...............] - ETA: 10:13 - loss: 6.7271 - custom_mse: 173.6815 - recall: 0.3073\n",
            "reduce_confident_loss =  0.258185327\n",
            "\n",
            "reduce_mean_cls_loss =  0.00188514218\n",
            "\n",
            "regression loss =  4.18210888\n",
            "19/34 [===============>..............] - ETA: 9:33 - loss: 6.6069 - custom_mse: 215.1472 - recall: 0.2911 \n",
            "reduce_confident_loss =  0.0723751709\n",
            "\n",
            "reduce_mean_cls_loss =  0.0113685261\n",
            "\n",
            "regression loss =  5.28283405\n",
            "20/34 [================>.............] - ETA: 8:53 - loss: 6.5448 - custom_mse: 321.7834 - recall: 0.3265\n",
            "reduce_confident_loss =  0.149772882\n",
            "\n",
            "reduce_mean_cls_loss =  0.0124456035\n",
            "\n",
            "regression loss =  2.39761353\n",
            "21/34 [=================>............] - ETA: 8:14 - loss: 6.3551 - custom_mse: 180.6066 - recall: 0.3110\n",
            "reduce_confident_loss =  0.129791066\n",
            "\n",
            "reduce_mean_cls_loss =  0.0210456811\n",
            "\n",
            "regression loss =  4.06926441\n",
            "22/34 [==================>...........] - ETA: 7:34 - loss: 6.2580 - custom_mse: 167.5401 - recall: 0.3226\n",
            "reduce_confident_loss =  0.182449237\n",
            "\n",
            "reduce_mean_cls_loss =  0.0645274594\n",
            "\n",
            "regression loss =  4.35569954\n",
            "23/34 [===================>..........] - ETA: 6:56 - loss: 6.1861 - custom_mse: 167.6277 - recall: 0.3101\n",
            "reduce_confident_loss =  0.100001745\n",
            "\n",
            "reduce_mean_cls_loss =  0.114796214\n",
            "\n",
            "regression loss =  3.32055783\n",
            "24/34 [====================>.........] - ETA: 6:18 - loss: 6.0756 - custom_mse: 140.6461 - recall: 0.2972\n",
            "reduce_confident_loss =  0.169105187\n",
            "\n",
            "reduce_mean_cls_loss =  0.581016421\n",
            "\n",
            "regression loss =  2.89309406\n",
            "25/34 [=====================>........] - ETA: 5:39 - loss: 5.9783 - custom_mse: 66.6364 - recall: 0.3036 \n",
            "reduce_confident_loss =  0.31612736\n",
            "\n",
            "reduce_mean_cls_loss =  0.814715087\n",
            "\n",
            "regression loss =  5.91836357\n",
            "26/34 [=====================>........] - ETA: 5:01 - loss: 6.0195 - custom_mse: 59.3051 - recall: 0.3186\n",
            "reduce_confident_loss =  0.531718731\n",
            "\n",
            "reduce_mean_cls_loss =  0.530841291\n",
            "\n",
            "regression loss =  6.56578922\n",
            "27/34 [======================>.......] - ETA: 4:22 - loss: 6.0791 - custom_mse: 128.7227 - recall: 0.3300\n",
            "reduce_confident_loss =  0.293693215\n",
            "\n",
            "reduce_mean_cls_loss =  0.369274527\n",
            "\n",
            "regression loss =  7.5302887\n",
            "28/34 [=======================>......] - ETA: 3:47 - loss: 6.1546 - custom_mse: 179.1137 - recall: 0.3182\n",
            "reduce_confident_loss =  0.207833216\n",
            "\n",
            "reduce_mean_cls_loss =  0.117550969\n",
            "\n",
            "regression loss =  8.57983685\n",
            "29/34 [========================>.....] - ETA: 3:10 - loss: 6.2495 - custom_mse: 277.6724 - recall: 0.3072\n",
            "reduce_confident_loss =  0.0402901955\n",
            "\n",
            "reduce_mean_cls_loss =  0.287421435\n",
            "\n",
            "regression loss =  6.6735673\n",
            "30/34 [=========================>....] - ETA: 2:32 - loss: 6.2745 - custom_mse: 190.0710 - recall: 0.2970\n",
            "reduce_confident_loss =  0.046430178\n",
            "\n",
            "reduce_mean_cls_loss =  0.716538131\n",
            "\n",
            "regression loss =  3.92978454\n",
            "31/34 [==========================>...] - ETA: 1:54 - loss: 6.2235 - custom_mse: 163.7571 - recall: 0.2874\n",
            "reduce_confident_loss =  0.187035397\n",
            "\n",
            "reduce_mean_cls_loss =  0.130595148\n",
            "\n",
            "regression loss =  5.23700476\n",
            "32/34 [===========================>..] - ETA: 1:15 - loss: 6.2026 - custom_mse: 319.5893 - recall: 0.2784\n",
            "reduce_confident_loss =  0.0301476549\n",
            "\n",
            "reduce_mean_cls_loss =  0.260884076\n",
            "\n",
            "regression loss =  7.35712624\n",
            "33/34 [============================>.] - ETA: 37s - loss: 6.2464 - custom_mse: 256.2777 - recall: 0.2700 \n",
            "reduce_confident_loss =  0.0740545392\n",
            "\n",
            "reduce_mean_cls_loss =  0.0931940898\n",
            "\n",
            "regression loss =  7.05255175\n",
            "34/34 [==============================] - ETA: 0s - loss: 6.2645 - custom_mse: 225.1251 - recall: 0.2620 \n",
            "reduce_confident_loss =  0.599718928\n",
            "\n",
            "reduce_mean_cls_loss =  6.83329439\n",
            "\n",
            "regression loss =  15.3307266\n",
            "\n",
            "reduce_confident_loss =  0.173996791\n",
            "\n",
            "reduce_mean_cls_loss =  0.892599523\n",
            "\n",
            "regression loss =  9.77393341\n",
            "\n",
            "reduce_confident_loss =  0.51943177\n",
            "\n",
            "reduce_mean_cls_loss =  4.8074441\n",
            "\n",
            "regression loss =  22.9945526\n",
            "\n",
            "reduce_confident_loss =  0.552177489\n",
            "\n",
            "reduce_mean_cls_loss =  0.00607881788\n",
            "\n",
            "regression loss =  16.9772472\n",
            "\n",
            "reduce_confident_loss =  0.282126725\n",
            "\n",
            "reduce_mean_cls_loss =  2.14751387\n",
            "\n",
            "regression loss =  20.7488098\n",
            "\n",
            "reduce_confident_loss =  0.555913389\n",
            "\n",
            "reduce_mean_cls_loss =  5.10220861\n",
            "\n",
            "regression loss =  13.984437\n",
            "\n",
            "reduce_confident_loss =  0.56750828\n",
            "\n",
            "reduce_mean_cls_loss =  5.12607813\n",
            "\n",
            "regression loss =  11.6411495\n",
            "\n",
            "reduce_confident_loss =  0.558882892\n",
            "\n",
            "reduce_mean_cls_loss =  5.07307148\n",
            "\n",
            "regression loss =  11.4402952\n",
            "\n",
            "reduce_confident_loss =  0.446743071\n",
            "\n",
            "reduce_mean_cls_loss =  3.57998896\n",
            "\n",
            "regression loss =  15.7537279\n",
            "\n",
            "reduce_confident_loss =  0.14074643\n",
            "\n",
            "reduce_mean_cls_loss =  0.851804256\n",
            "\n",
            "regression loss =  14.7862482\n",
            "\n",
            "reduce_confident_loss =  0.156905174\n",
            "\n",
            "reduce_mean_cls_loss =  0.891184926\n",
            "\n",
            "regression loss =  11.6419439\n",
            "\n",
            "reduce_confident_loss =  0.523685277\n",
            "\n",
            "reduce_mean_cls_loss =  4.81686926\n",
            "\n",
            "regression loss =  20.0672169\n",
            "\n",
            "reduce_confident_loss =  0.520986497\n",
            "\n",
            "reduce_mean_cls_loss =  0.00728167035\n",
            "\n",
            "regression loss =  13.1844263\n",
            "34/34 [==============================] - 1476s 44s/step - loss: 6.2645 - custom_mse: 225.1251 - recall: 0.2620 - val_loss: 19.0027 - val_custom_mse: 85.3909 - val_recall: 0.0721\n",
            "Epoch 10/10\n",
            "\n",
            "reduce_confident_loss =  0.124223456\n",
            "\n",
            "reduce_mean_cls_loss =  0.0725261644\n",
            "\n",
            "regression loss =  4.17048502\n",
            " 1/34 [..............................] - ETA: 21:10 - loss: 4.3672 - custom_mse: 396.2488 - recall: 0.0000e+00\n",
            "reduce_confident_loss =  0.506216466\n",
            "\n",
            "reduce_mean_cls_loss =  1.04332387\n",
            "\n",
            "regression loss =  6.17410755\n",
            " 2/34 [>.............................] - ETA: 19:41 - loss: 6.0454 - custom_mse: 64.3083 - recall: 0.0248     \n",
            "reduce_confident_loss =  0.359595507\n",
            "\n",
            "reduce_mean_cls_loss =  1.56756651\n",
            "\n",
            "regression loss =  6.14233589\n",
            " 3/34 [=>............................] - ETA: 19:05 - loss: 6.7201 - custom_mse: 277.6021 - recall: 0.0707\n",
            "reduce_confident_loss =  0.0454311296\n",
            "\n",
            "reduce_mean_cls_loss =  0.0542577766\n",
            "\n",
            "regression loss =  2.12401891\n",
            " 4/34 [==>...........................] - ETA: 18:55 - loss: 5.5960 - custom_mse: 321.8754 - recall: 0.0530\n",
            "reduce_confident_loss =  0.0635698214\n",
            "\n",
            "reduce_mean_cls_loss =  0.0325389206\n",
            "\n",
            "regression loss =  1.99720728\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-7f348dcb3daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_resnet_backbone_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'use_generator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-ce5ccfa1a98b>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, history_)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     shuffle=True, callbacks = callbacks )\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhistory_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'use_fit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_rgb_depth_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    851\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;34m\"\"\"Runs a training execution with one step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    844\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1284\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1285\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1286\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2847\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2849\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3630\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3631\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3632\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    789\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[1;32m    790\u001b[0m     \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# Collect metrics to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \"\"\"\n\u001b[1;32m    520\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 521\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1384\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     gx = array_ops.reshape(\n\u001b[0;32m-> 1386\u001b[0;31m         math_ops.reduce_sum(gen_math_ops.mul(grad, y), rx), sx)\n\u001b[0m\u001b[1;32m   1387\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6228\u001b[0m   \u001b[0m_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6229\u001b[0m   \u001b[0mtld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6230\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6231\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6232\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t-tcsWVvN_Z"
      },
      "source": [
        "modified recall metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3-EKw8DvMbq"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ONRQ_TVvL4j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9vx43_Rv-Uh"
      },
      "source": [
        "## training with loss, modified mse, recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyJ7b_fGQiiF",
        "outputId": "2032e15e-46c5-4bdf-e561-ad3f47e927e0"
      },
      "source": [
        "# Modify mse metric: only calculate dist between confident scores > 0.5\n",
        "history_modified_mse_metric = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mKết quả truyền trực tuyến bị cắt bớt đến 5000 dòng cuối.\u001b[0m\n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.492466152 0.861984849 0.825798094 ... 0.969612718 0.962777197 8.20878486e-05]\n",
            " [0.688673258 0.633979559 0.236955732 ... 0.991401553 0.859568357 0.000533849]\n",
            " [0.550403297 0.959114194 0.445172548 ... 0.969572544 0.92289412 0.000226169825]\n",
            " [0.80535 0.791642427 0.947777033 ... 0.996895969 0.997538447 7.44925228e-06]\n",
            " [0.635962188 0.843631804 0.806946516 ... 0.998679698 0.962911606 3.27010434e-06]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.492466152 0.861984849 0.825798094 ... 0.969612718 0.962777197 8.20878486e-05]\n",
            " [0.688673258 0.633979559 0.236955732 ... 0.991401553 0.859568357 0.000533849]\n",
            " [0.550403297 0.959114194 0.445172548 ... 0.969572544 0.92289412 0.000226169825]\n",
            " [0.80535 0.791642427 0.947777033 ... 0.996895969 0.997538447 7.44925228e-06]\n",
            " [0.635962188 0.843631804 0.806946516 ... 0.998679698 0.962911606 3.27010434e-06]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.492466152 0.861984849 0.825798094 ... 0.969612718 0.962777197 8.20878486e-05]\n",
            " [0.688673258 0.633979559 0.236955732 ... 0.991401553 0.859568357 0.000533849]\n",
            " [0.550403297 0.959114194 0.445172548 ... 0.969572544 0.92289412 0.000226169825]\n",
            " [0.80535 0.791642427 0.947777033 ... 0.996895969 0.997538447 7.44925228e-06]\n",
            " [0.635962188 0.843631804 0.806946516 ... 0.998679698 0.962911606 3.27010434e-06]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.492466152 0.861984849 0.825798094 ... 0.969612718 0.962777197 8.20878486e-05]\n",
            " [0.688673258 0.633979559 0.236955732 ... 0.991401553 0.859568357 0.000533849]\n",
            " [0.550403297 0.959114194 0.445172548 ... 0.969572544 0.92289412 0.000226169825]\n",
            " [0.80535 0.791642427 0.947777033 ... 0.996895969 0.997538447 7.44925228e-06]\n",
            " [0.635962188 0.843631804 0.806946516 ... 0.998679698 0.962911606 3.27010434e-06]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.492466152 0.861984849 0.825798094 ... 0.969612718 0.962777197 8.20878486e-05]\n",
            " [0.688673258 0.633979559 0.236955732 ... 0.991401553 0.859568357 0.000533849]\n",
            " [0.550403297 0.959114194 0.445172548 ... 0.969572544 0.92289412 0.000226169825]\n",
            " [0.80535 0.791642427 0.947777033 ... 0.996895969 0.997538447 7.44925228e-06]\n",
            " [0.635962188 0.843631804 0.806946516 ... 0.998679698 0.962911606 3.27010434e-06]]\n",
            "\n",
            "reduce_confident_loss =  0.418207347\n",
            "\n",
            "reduce_mean_cls_loss =  1.50929976\n",
            "\n",
            "regression loss =  11.6104908\n",
            "     41/Unknown - 1343s 33s/step - loss: 11.9081 - custom_mse: 129.2973 - recall: 0.5393\n",
            "reduce_confident_loss =  0.083875604\n",
            "\n",
            "reduce_mean_cls_loss =  0.513972878\n",
            "\n",
            "regression loss =  12.2379389\n",
            "\n",
            "reduce_confident_loss =  0.0869238377\n",
            "\n",
            "reduce_mean_cls_loss =  0.51190716\n",
            "\n",
            "regression loss =  10.39641\n",
            "\n",
            "reduce_confident_loss =  0.929262161\n",
            "\n",
            "reduce_mean_cls_loss =  3.21576333\n",
            "\n",
            "regression loss =  15.1733866\n",
            "\n",
            "reduce_confident_loss =  0.468153805\n",
            "\n",
            "reduce_mean_cls_loss =  0.567950845\n",
            "\n",
            "regression loss =  11.447958\n",
            "\n",
            "reduce_confident_loss =  0.930951238\n",
            "\n",
            "reduce_mean_cls_loss =  3.24525118\n",
            "\n",
            "regression loss =  7.22332954\n",
            "\n",
            "reduce_confident_loss =  0.924638212\n",
            "\n",
            "reduce_mean_cls_loss =  0.95419544\n",
            "\n",
            "regression loss =  17.3820744\n",
            "\n",
            "reduce_confident_loss =  0.465181768\n",
            "\n",
            "reduce_mean_cls_loss =  0.567004085\n",
            "\n",
            "regression loss =  11.8249054\n",
            "\n",
            "reduce_confident_loss =  0.924320519\n",
            "\n",
            "reduce_mean_cls_loss =  3.22778058\n",
            "\n",
            "regression loss =  5.90807533\n",
            "\n",
            "reduce_confident_loss =  0.924650848\n",
            "\n",
            "reduce_mean_cls_loss =  3.23196292\n",
            "\n",
            "regression loss =  6.79139233\n",
            "\n",
            "reduce_confident_loss =  0.926990926\n",
            "\n",
            "reduce_mean_cls_loss =  3.23387194\n",
            "\n",
            "regression loss =  8.50018787\n",
            "\n",
            "reduce_confident_loss =  0.920616806\n",
            "\n",
            "reduce_mean_cls_loss =  3.21328354\n",
            "\n",
            "regression loss =  9.82154751\n",
            "\n",
            "reduce_confident_loss =  0.409864753\n",
            "\n",
            "reduce_mean_cls_loss =  0.558410347\n",
            "\n",
            "regression loss =  16.3376675\n",
            "\n",
            "reduce_confident_loss =  0.142423049\n",
            "\n",
            "reduce_mean_cls_loss =  0.308548063\n",
            "\n",
            "regression loss =  12.3718729\n",
            "\n",
            "reduce_confident_loss =  0.920260727\n",
            "\n",
            "reduce_mean_cls_loss =  3.21435928\n",
            "\n",
            "regression loss =  14.2205658\n",
            "\n",
            "reduce_confident_loss =  0.922275186\n",
            "\n",
            "reduce_mean_cls_loss =  0.0405005142\n",
            "\n",
            "regression loss =  27.2197933\n",
            "41/41 [==============================] - 1545s 38s/step - loss: 11.9081 - custom_mse: 129.2973 - recall: 0.5393 - val_loss: 14.6710 - val_custom_mse: 123.0629 - val_recall: 0.5750\n",
            "Epoch 2/10\n",
            "\n",
            "reduce_confident_loss =  0.557943344\n",
            "\n",
            "reduce_mean_cls_loss =  1.14162302\n",
            "\n",
            "regression loss =  15.7525139\n",
            " 1/41 [..............................] - ETA: 21:14 - loss: 17.4521 - custom_mse: 120.3058 - recall: 0.6940\n",
            "reduce_confident_loss =  0.132222518\n",
            "\n",
            "reduce_mean_cls_loss =  0.588226438\n",
            "\n",
            "regression loss =  12.1391993\n",
            " 2/41 [>.............................] - ETA: 19:14 - loss: 15.1559 - custom_mse: 74.2652 - recall: 0.6913 \n",
            "reduce_confident_loss =  0.693506777\n",
            "\n",
            "reduce_mean_cls_loss =  1.52453172\n",
            "\n",
            "regression loss =  11.5528049\n",
            " 3/41 [=>............................] - ETA: 19:51 - loss: 14.6942 - custom_mse: 109.5140 - recall: 0.5730\n",
            "reduce_confident_loss =  0.506481707\n",
            "\n",
            "reduce_mean_cls_loss =  0.797010839\n",
            "\n",
            "regression loss =  10.1571102\n",
            " 4/41 [=>............................] - ETA: 19:08 - loss: 13.8858 - custom_mse: 161.2677 - recall: 0.5979\n",
            "reduce_confident_loss =  0.248831347\n",
            "\n",
            "reduce_mean_cls_loss =  0.596023381\n",
            "\n",
            "regression loss =  10.1175613\n",
            " 5/41 [==>...........................] - ETA: 18:21 - loss: 13.3011 - custom_mse: 85.1761 - recall: 0.6565 \n",
            "reduce_confident_loss =  0.709850967\n",
            "\n",
            "reduce_mean_cls_loss =  0.722092092\n",
            "\n",
            "regression loss =  13.0256138\n",
            " 6/41 [===>..........................] - ETA: 17:53 - loss: 13.4939 - custom_mse: 79.4051 - recall: 0.5912Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.704599261\n",
            "\n",
            "reduce_mean_cls_loss =  0.494233757\n",
            "\n",
            "regression loss =  12.2881384\n",
            " 7/41 [====>.........................] - ETA: 18:18 - loss: 13.4929 - custom_mse: 116.5760 - recall: 0.6253\n",
            "reduce_confident_loss =  0.716067731\n",
            "\n",
            "reduce_mean_cls_loss =  0.0864127874\n",
            "\n",
            "regression loss =  12.2758446\n",
            " 8/41 [====>.........................] - ETA: 17:43 - loss: 13.4411 - custom_mse: 175.7837 - recall: 0.5509\n",
            "reduce_confident_loss =  0.257891\n",
            "\n",
            "reduce_mean_cls_loss =  1.32108796\n",
            "\n",
            "regression loss =  8.72777843\n",
            " 9/41 [=====>........................] - ETA: 17:09 - loss: 13.0928 - custom_mse: 142.0367 - recall: 0.5067\n",
            "reduce_confident_loss =  0.544043481\n",
            "\n",
            "reduce_mean_cls_loss =  0.517568648\n",
            "\n",
            "regression loss =  7.23764515\n",
            "10/41 [======>.......................] - ETA: 16:37 - loss: 12.6134 - custom_mse: 154.2390 - recall: 0.4560\n",
            "reduce_confident_loss =  0.427195966\n",
            "\n",
            "reduce_mean_cls_loss =  0.322752148\n",
            "\n",
            "regression loss =  4.70816898\n",
            "11/41 [=======>......................] - ETA: 16:04 - loss: 11.9630 - custom_mse: 120.8791 - recall: 0.4146\n",
            "reduce_confident_loss =  0.31090185\n",
            "\n",
            "reduce_mean_cls_loss =  0.253195733\n",
            "\n",
            "regression loss =  4.63763189\n",
            "12/41 [=======>......................] - ETA: 15:31 - loss: 11.3995 - custom_mse: 101.5085 - recall: 0.3800\n",
            "reduce_confident_loss =  0.575474441\n",
            "\n",
            "reduce_mean_cls_loss =  0.157281265\n",
            "\n",
            "regression loss =  5.6863327\n",
            "13/41 [========>.....................] - ETA: 14:58 - loss: 11.0164 - custom_mse: 85.9619 - recall: 0.3508 \n",
            "reduce_confident_loss =  0.339537084\n",
            "\n",
            "reduce_mean_cls_loss =  0.88987571\n",
            "\n",
            "regression loss =  8.52887249\n",
            "14/41 [=========>....................] - ETA: 14:31 - loss: 10.9265 - custom_mse: 77.7452 - recall: 0.3257\n",
            "reduce_confident_loss =  0.44903177\n",
            "\n",
            "reduce_mean_cls_loss =  0.682457\n",
            "\n",
            "regression loss =  12.346302\n",
            "15/41 [=========>....................] - ETA: 13:57 - loss: 11.0966 - custom_mse: 107.3400 - recall: 0.3040\n",
            "reduce_confident_loss =  0.171978369\n",
            "\n",
            "reduce_mean_cls_loss =  0.503988922\n",
            "\n",
            "regression loss =  12.6919546\n",
            "16/41 [==========>...................] - ETA: 13:24 - loss: 11.2386 - custom_mse: 172.2001 - recall: 0.2935\n",
            "reduce_confident_loss =  0.284989774\n",
            "\n",
            "reduce_mean_cls_loss =  0.509670794\n",
            "\n",
            "regression loss =  9.86601925\n",
            "17/41 [===========>..................] - ETA: 13:29 - loss: 11.2046 - custom_mse: 90.2114 - recall: 0.3008 \n",
            "reduce_confident_loss =  0.273688823\n",
            "\n",
            "reduce_mean_cls_loss =  0.395695627\n",
            "\n",
            "regression loss =  10.792428\n",
            "18/41 [============>.................] - ETA: 12:52 - loss: 11.2189 - custom_mse: 61.5735 - recall: 0.3336\n",
            "reduce_confident_loss =  0.218608111\n",
            "\n",
            "reduce_mean_cls_loss =  0.584668159\n",
            "\n",
            "regression loss =  12.5112047\n",
            "19/41 [============>.................] - ETA: 12:16 - loss: 11.3292 - custom_mse: 25.8358 - recall: 0.3493\n",
            "reduce_confident_loss =  0.374512881\n",
            "\n",
            "reduce_mean_cls_loss =  0.735191882\n",
            "\n",
            "regression loss =  13.4214764\n",
            "20/41 [=============>................] - ETA: 11:42 - loss: 11.4893 - custom_mse: 161.4164 - recall: 0.3640\n",
            "reduce_confident_loss =  0.89948982\n",
            "\n",
            "reduce_mean_cls_loss =  2.87565184\n",
            "\n",
            "regression loss =  8.63917\n",
            "21/41 [==============>...............] - ETA: 11:12 - loss: 11.5333 - custom_mse: 108.7633 - recall: 0.3545\n",
            "reduce_confident_loss =  0.354601711\n",
            "\n",
            "reduce_mean_cls_loss =  0.493029445\n",
            "\n",
            "regression loss =  9.27480221\n",
            "22/41 [===============>..............] - ETA: 10:38 - loss: 11.4692 - custom_mse: 211.5345 - recall: 0.3758\n",
            "reduce_confident_loss =  0.723807633\n",
            "\n",
            "reduce_mean_cls_loss =  1.02032375\n",
            "\n",
            "regression loss =  7.61860323\n",
            "23/41 [===============>..............] - ETA: 10:03 - loss: 11.3776 - custom_mse: 184.0517 - recall: 0.3798\n",
            "reduce_confident_loss =  0.24239485\n",
            "\n",
            "reduce_mean_cls_loss =  0.228371307\n",
            "\n",
            "regression loss =  6.22286892\n",
            "24/41 [================>.............] - ETA: 9:28 - loss: 11.1824 - custom_mse: 154.2523 - recall: 0.4019 \n",
            "reduce_confident_loss =  0.320867598\n",
            "\n",
            "reduce_mean_cls_loss =  0.212122917\n",
            "\n",
            "regression loss =  6.37444353\n",
            "25/41 [=================>............] - ETA: 8:54 - loss: 11.0114 - custom_mse: 188.3671 - recall: 0.4240\n",
            "reduce_confident_loss =  0.317540497\n",
            "\n",
            "reduce_mean_cls_loss =  0.0786094\n",
            "\n",
            "regression loss =  10.7463102\n",
            "26/41 [==================>...........] - ETA: 8:19 - loss: 11.0165 - custom_mse: 200.3431 - recall: 0.4426\n",
            "reduce_confident_loss =  0.347487032\n",
            "\n",
            "reduce_mean_cls_loss =  0.541894257\n",
            "\n",
            "regression loss =  6.95605755\n",
            "27/41 [==================>...........] - ETA: 7:45 - loss: 10.8990 - custom_mse: 174.0720 - recall: 0.4549\n",
            "reduce_confident_loss =  0.356668055\n",
            "\n",
            "reduce_mean_cls_loss =  0.63525784\n",
            "\n",
            "regression loss =  3.60300827\n",
            "28/41 [===================>..........] - ETA: 7:11 - loss: 10.6739 - custom_mse: 77.3175 - recall: 0.4590 \n",
            "reduce_confident_loss =  0.318011582\n",
            "\n",
            "reduce_mean_cls_loss =  0.867455304\n",
            "\n",
            "regression loss =  6.06714296\n",
            "29/41 [====================>.........] - ETA: 6:37 - loss: 10.5559 - custom_mse: 96.7317 - recall: 0.4660\n",
            "reduce_confident_loss =  0.326373249\n",
            "\n",
            "reduce_mean_cls_loss =  0.501073301\n",
            "\n",
            "regression loss =  6.50672913\n",
            "30/41 [====================>.........] - ETA: 6:04 - loss: 10.4485 - custom_mse: 97.7981 - recall: 0.4732\n",
            "reduce_confident_loss =  0.380060107\n",
            "\n",
            "reduce_mean_cls_loss =  0.50056833\n",
            "\n",
            "regression loss =  6.46633291\n",
            "31/41 [=====================>........] - ETA: 5:30 - loss: 10.3485 - custom_mse: 84.4338 - recall: 0.4850\n",
            "reduce_confident_loss =  0.753262639\n",
            "\n",
            "reduce_mean_cls_loss =  0.452030599\n",
            "\n",
            "regression loss =  7.86672211\n",
            "32/41 [======================>.......] - ETA: 4:57 - loss: 10.3086 - custom_mse: 44.0956 - recall: 0.4930\n",
            "reduce_confident_loss =  1.75654054\n",
            "\n",
            "reduce_mean_cls_loss =  0.968982577\n",
            "\n",
            "regression loss =  10.2740993\n",
            "33/41 [=======================>......] - ETA: 4:23 - loss: 10.3901 - custom_mse: 42.4093 - recall: 0.4994\n",
            "reduce_confident_loss =  0.247211248\n",
            "\n",
            "reduce_mean_cls_loss =  0.655946076\n",
            "\n",
            "regression loss =  7.90631151\n",
            "34/41 [=======================>......] - ETA: 3:51 - loss: 10.3436 - custom_mse: 75.6501 - recall: 0.5103\n",
            "reduce_confident_loss =  0.341493189\n",
            "\n",
            "reduce_mean_cls_loss =  0.401070058\n",
            "\n",
            "regression loss =  11.7344313\n",
            "35/41 [========================>.....] - ETA: 3:18 - loss: 10.4046 - custom_mse: 222.7425 - recall: 0.5188\n",
            "reduce_confident_loss =  0.173248172\n",
            "\n",
            "reduce_mean_cls_loss =  0.995371\n",
            "\n",
            "regression loss =  13.6664944\n",
            "36/41 [=========================>....] - ETA: 2:46 - loss: 10.5277 - custom_mse: 262.8522 - recall: 0.5256\n",
            "reduce_confident_loss =  0.0855444819\n",
            "\n",
            "reduce_mean_cls_loss =  0.771376073\n",
            "\n",
            "regression loss =  6.87911701\n",
            "37/41 [==========================>...] - ETA: 2:12 - loss: 10.4522 - custom_mse: 104.9588 - recall: 0.5292\n",
            "reduce_confident_loss =  0.268499166\n",
            "\n",
            "reduce_mean_cls_loss =  1.01346552\n",
            "\n",
            "regression loss =  4.73009825\n",
            "38/41 [==========================>...] - ETA: 1:39 - loss: 10.3354 - custom_mse: 149.4285 - recall: 0.5276\n",
            "reduce_confident_loss =  0.124848671\n",
            "\n",
            "reduce_mean_cls_loss =  1.20199668\n",
            "\n",
            "regression loss =  13.8663406\n",
            "39/41 [===========================>..] - ETA: 1:06 - loss: 10.4599 - custom_mse: 328.9447 - recall: 0.5141\n",
            "reduce_confident_loss =  0.0778354481\n",
            "\n",
            "reduce_mean_cls_loss =  1.33486044\n",
            "\n",
            "regression loss =  15.3008099\n",
            "40/41 [============================>.] - ETA: 33s - loss: 10.6163 - custom_mse: 256.1479 - recall: 0.5012 indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 1\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.323693514 0.439520359 0.181572855 ... 0.994214118 0.933698714 0.000236362219]\n",
            " [0.968749404 0.0308884978 0.835012555 ... 0.991021752 0.998397231 7.94613788e-06]\n",
            " [0.712009907 0.164143354 0.0706700385 ... 0.991369367 0.997414768 6.77489952e-05]\n",
            " [0.81631887 0.0198926628 0.0506059825 ... 0.987929463 0.992168665 0.000224351883]\n",
            " [0.194626868 0.756720781 0.0833980739 ... 0.996155858 0.987365 2.54840597e-05]]\n",
            "\n",
            "reduce_confident_loss =  0.320837\n",
            "\n",
            "reduce_mean_cls_loss =  1.01892138\n",
            "\n",
            "regression loss =  13.9648504\n",
            "41/41 [==============================] - ETA: 0s - loss: 10.6526 - custom_mse: 228.1378 - recall: 0.4890 \n",
            "reduce_confident_loss =  0.328611583\n",
            "\n",
            "reduce_mean_cls_loss =  0.609451175\n",
            "\n",
            "regression loss =  12.45786\n",
            "\n",
            "reduce_confident_loss =  0.317819387\n",
            "\n",
            "reduce_mean_cls_loss =  0.607443\n",
            "\n",
            "regression loss =  8.65239906\n",
            "\n",
            "reduce_confident_loss =  0.202830151\n",
            "\n",
            "reduce_mean_cls_loss =  5.34004879\n",
            "\n",
            "regression loss =  16.0276394\n",
            "\n",
            "reduce_confident_loss =  0.135587603\n",
            "\n",
            "reduce_mean_cls_loss =  0.768050611\n",
            "\n",
            "regression loss =  12.4902563\n",
            "\n",
            "reduce_confident_loss =  0.196764916\n",
            "\n",
            "reduce_mean_cls_loss =  5.41275263\n",
            "\n",
            "regression loss =  12.0873289\n",
            "\n",
            "reduce_confident_loss =  0.194803551\n",
            "\n",
            "reduce_mean_cls_loss =  1.54024541\n",
            "\n",
            "regression loss =  14.2883291\n",
            "\n",
            "reduce_confident_loss =  0.134871483\n",
            "\n",
            "reduce_mean_cls_loss =  0.757427633\n",
            "\n",
            "regression loss =  13.7833691\n",
            "\n",
            "reduce_confident_loss =  0.193827495\n",
            "\n",
            "reduce_mean_cls_loss =  5.36267328\n",
            "\n",
            "regression loss =  8.90926933\n",
            "\n",
            "reduce_confident_loss =  0.194424897\n",
            "\n",
            "reduce_mean_cls_loss =  5.37134218\n",
            "\n",
            "regression loss =  9.62960911\n",
            "\n",
            "reduce_confident_loss =  0.195011616\n",
            "\n",
            "reduce_mean_cls_loss =  5.37226772\n",
            "\n",
            "regression loss =  11.1182966\n",
            "\n",
            "reduce_confident_loss =  0.194520921\n",
            "\n",
            "reduce_mean_cls_loss =  5.35095167\n",
            "\n",
            "regression loss =  12.7343388\n",
            "\n",
            "reduce_confident_loss =  0.161201164\n",
            "\n",
            "reduce_mean_cls_loss =  0.743696749\n",
            "\n",
            "regression loss =  20.7023773\n",
            "\n",
            "reduce_confident_loss =  0.311210215\n",
            "\n",
            "reduce_mean_cls_loss =  0.24878566\n",
            "\n",
            "regression loss =  14.7340498\n",
            "\n",
            "reduce_confident_loss =  0.191311702\n",
            "\n",
            "reduce_mean_cls_loss =  5.37244\n",
            "\n",
            "regression loss =  20.7863693\n",
            "\n",
            "reduce_confident_loss =  0.193297863\n",
            "\n",
            "reduce_mean_cls_loss =  0.004639911\n",
            "\n",
            "regression loss =  21.4741211\n",
            "41/41 [==============================] - 1532s 37s/step - loss: 10.6526 - custom_mse: 228.1378 - recall: 0.4890 - val_loss: 16.9807 - val_custom_mse: 304.3531 - val_recall: 0.5750\n",
            "Epoch 3/10\n",
            "\n",
            "reduce_confident_loss =  0.180949822\n",
            "\n",
            "reduce_mean_cls_loss =  1.88658583\n",
            "\n",
            "regression loss =  13.6136341\n",
            " 1/41 [..............................] - ETA: 22:06 - loss: 15.6812 - custom_mse: 175.9028 - recall: 0.8095\n",
            "reduce_confident_loss =  0.467413753\n",
            "\n",
            "reduce_mean_cls_loss =  0.187692508\n",
            "\n",
            "regression loss =  10.2325926\n",
            " 2/41 [>.............................] - ETA: 19:41 - loss: 13.2844 - custom_mse: 80.5713 - recall: 0.8525 \n",
            "reduce_confident_loss =  0.23001048\n",
            "\n",
            "reduce_mean_cls_loss =  1.32116616\n",
            "\n",
            "regression loss =  11.3018246\n",
            " 3/41 [=>............................] - ETA: 19:42 - loss: 13.1406 - custom_mse: 174.2019 - recall: 0.6555\n",
            "reduce_confident_loss =  0.338811249\n",
            "\n",
            "reduce_mean_cls_loss =  0.690172136\n",
            "\n",
            "regression loss =  10.7898579\n",
            " 4/41 [=>............................] - ETA: 18:58 - loss: 12.8102 - custom_mse: 237.6754 - recall: 0.7067\n",
            "reduce_confident_loss =  0.33902806\n",
            "\n",
            "reduce_mean_cls_loss =  0.538545728\n",
            "\n",
            "regression loss =  12.5540476\n",
            " 5/41 [==>...........................] - ETA: 18:14 - loss: 12.9345 - custom_mse: 117.6283 - recall: 0.7654\n",
            "reduce_confident_loss =  0.319479465\n",
            "\n",
            "reduce_mean_cls_loss =  0.0493808575\n",
            "\n",
            "regression loss =  15.3104534\n",
            " 6/41 [===>..........................] - ETA: 17:47 - loss: 13.3919 - custom_mse: 133.2192 - recall: 0.8045Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.42163223\n",
            "\n",
            "reduce_mean_cls_loss =  0.182030171\n",
            "\n",
            "regression loss =  11.8516426\n",
            " 7/41 [====>.........................] - ETA: 18:03 - loss: 13.2581 - custom_mse: 168.7204 - recall: 0.8014\n",
            "reduce_confident_loss =  0.169378385\n",
            "\n",
            "reduce_mean_cls_loss =  0.114487447\n",
            "\n",
            "regression loss =  13.7214727\n",
            " 8/41 [====>.........................] - ETA: 17:31 - loss: 13.3515 - custom_mse: 251.6490 - recall: 0.7103\n",
            "reduce_confident_loss =  0.800103724\n",
            "\n",
            "reduce_mean_cls_loss =  0.717167735\n",
            "\n",
            "regression loss =  9.31980133\n",
            " 9/41 [=====>........................] - ETA: 16:58 - loss: 13.0722 - custom_mse: 59.5285 - recall: 0.6919 \n",
            "reduce_confident_loss =  0.201020986\n",
            "\n",
            "reduce_mean_cls_loss =  0.0637516528\n",
            "\n",
            "regression loss =  7.34838772\n",
            "10/41 [======>.......................] - ETA: 16:29 - loss: 12.5263 - custom_mse: 267.6897 - recall: 0.6227\n",
            "reduce_confident_loss =  0.140854552\n",
            "\n",
            "reduce_mean_cls_loss =  0.048538439\n",
            "\n",
            "regression loss =  4.84843731\n",
            "11/41 [=======>......................] - ETA: 16:03 - loss: 11.8455 - custom_mse: 218.7749 - recall: 0.5661\n",
            "reduce_confident_loss =  0.206709936\n",
            "\n",
            "reduce_mean_cls_loss =  0.0212932676\n",
            "\n",
            "regression loss =  2.32380199\n",
            "12/41 [=======>......................] - ETA: 15:46 - loss: 11.0710 - custom_mse: 192.1461 - recall: 0.5189\n",
            "reduce_confident_loss =  0.200328976\n",
            "\n",
            "reduce_mean_cls_loss =  0.0755479\n",
            "\n",
            "regression loss =  3.51395273\n",
            "13/41 [========>.....................] - ETA: 15:16 - loss: 10.5109 - custom_mse: 149.7950 - recall: 0.4790\n",
            "reduce_confident_loss =  0.23167263\n",
            "\n",
            "reduce_mean_cls_loss =  0.0270692874\n",
            "\n",
            "regression loss =  6.60174608\n",
            "14/41 [=========>....................] - ETA: 14:42 - loss: 10.2502 - custom_mse: 100.4019 - recall: 0.4448\n",
            "reduce_confident_loss =  0.133260459\n",
            "\n",
            "reduce_mean_cls_loss =  0.114069477\n",
            "\n",
            "regression loss =  11.4704657\n",
            "15/41 [=========>....................] - ETA: 14:08 - loss: 10.3480 - custom_mse: 137.6647 - recall: 0.4151\n",
            "reduce_confident_loss =  0.145528764\n",
            "\n",
            "reduce_mean_cls_loss =  0.808428347\n",
            "\n",
            "regression loss =  12.8864431\n",
            "16/41 [==========>...................] - ETA: 13:34 - loss: 10.5663 - custom_mse: 202.6110 - recall: 0.3892\n",
            "reduce_confident_loss =  0.346753448\n",
            "\n",
            "reduce_mean_cls_loss =  0.897924483\n",
            "\n",
            "regression loss =  12.6706886\n",
            "17/41 [===========>..................] - ETA: 13:39 - loss: 10.7633 - custom_mse: 93.8711 - recall: 0.3983 \n",
            "reduce_confident_loss =  0.317052424\n",
            "\n",
            "reduce_mean_cls_loss =  0.514326513\n",
            "\n",
            "regression loss =  16.4968872\n",
            "18/41 [============>.................] - ETA: 13:04 - loss: 11.1280 - custom_mse: 45.5701 - recall: 0.4136\n",
            "reduce_confident_loss =  0.133602381\n",
            "\n",
            "reduce_mean_cls_loss =  0.380906731\n",
            "\n",
            "regression loss =  9.84035873\n",
            "19/41 [============>.................] - ETA: 12:28 - loss: 11.0873 - custom_mse: 22.9122 - recall: 0.4356\n",
            "reduce_confident_loss =  0.434009492\n",
            "\n",
            "reduce_mean_cls_loss =  0.91448009\n",
            "\n",
            "regression loss =  14.2146196\n",
            "20/41 [=============>................] - ETA: 11:54 - loss: 11.3111 - custom_mse: 151.0157 - recall: 0.4444\n",
            "reduce_confident_loss =  0.487902492\n",
            "\n",
            "reduce_mean_cls_loss =  2.63172197\n",
            "\n",
            "regression loss =  9.37673378\n",
            "21/41 [==============>...............] - ETA: 11:19 - loss: 11.3676 - custom_mse: 136.2925 - recall: 0.4385\n",
            "reduce_confident_loss =  0.37250033\n",
            "\n",
            "reduce_mean_cls_loss =  0.380589157\n",
            "\n",
            "regression loss =  9.63707638\n",
            "22/41 [===============>..............] - ETA: 10:43 - loss: 11.3231 - custom_mse: 205.0519 - recall: 0.4587\n",
            "reduce_confident_loss =  0.243254021\n",
            "\n",
            "reduce_mean_cls_loss =  0.429633617\n",
            "\n",
            "regression loss =  10.1909084\n",
            "23/41 [===============>..............] - ETA: 10:07 - loss: 11.3032 - custom_mse: 157.4391 - recall: 0.4668\n",
            "reduce_confident_loss =  0.554449677\n",
            "\n",
            "reduce_mean_cls_loss =  0.527660966\n",
            "\n",
            "regression loss =  7.90409851\n",
            "24/41 [================>.............] - ETA: 9:32 - loss: 11.2066 - custom_mse: 129.8437 - recall: 0.4801 \n",
            "reduce_confident_loss =  0.548013866\n",
            "\n",
            "reduce_mean_cls_loss =  0.427994877\n",
            "\n",
            "regression loss =  9.71912861\n",
            "25/41 [=================>............] - ETA: 8:59 - loss: 11.1862 - custom_mse: 150.4155 - recall: 0.4924\n",
            "reduce_confident_loss =  0.423344821\n",
            "\n",
            "reduce_mean_cls_loss =  0.208431453\n",
            "\n",
            "regression loss =  12.8671808\n",
            "26/41 [==================>...........] - ETA: 8:25 - loss: 11.2751 - custom_mse: 169.4413 - recall: 0.5084\n",
            "reduce_confident_loss =  0.541873276\n",
            "\n",
            "reduce_mean_cls_loss =  0.34107542\n",
            "\n",
            "regression loss =  9.01514244\n",
            "27/41 [==================>...........] - ETA: 7:50 - loss: 11.2241 - custom_mse: 104.9029 - recall: 0.5173\n",
            "reduce_confident_loss =  0.266692817\n",
            "\n",
            "reduce_mean_cls_loss =  0.199027568\n",
            "\n",
            "regression loss =  5.0604434\n",
            "28/41 [===================>..........] - ETA: 7:16 - loss: 11.0206 - custom_mse: 75.7403 - recall: 0.5323 \n",
            "reduce_confident_loss =  0.258559912\n",
            "\n",
            "reduce_mean_cls_loss =  0.242612019\n",
            "\n",
            "regression loss =  5.76268148\n",
            "29/41 [====================>.........] - ETA: 6:42 - loss: 10.8566 - custom_mse: 88.2904 - recall: 0.5461\n",
            "reduce_confident_loss =  0.247993678\n",
            "\n",
            "reduce_mean_cls_loss =  0.305556983\n",
            "\n",
            "regression loss =  5.78565598\n",
            "30/41 [====================>.........] - ETA: 6:09 - loss: 10.7060 - custom_mse: 98.6097 - recall: 0.5582\n",
            "reduce_confident_loss =  0.281340122\n",
            "\n",
            "reduce_mean_cls_loss =  0.34639746\n",
            "\n",
            "regression loss =  5.21153069\n",
            "31/41 [=====================>........] - ETA: 5:35 - loss: 10.5490 - custom_mse: 75.8630 - recall: 0.5663\n",
            "reduce_confident_loss =  0.530335724\n",
            "\n",
            "reduce_mean_cls_loss =  0.349130094\n",
            "\n",
            "regression loss =  7.0955205\n",
            "32/41 [======================>.......] - ETA: 5:02 - loss: 10.4686 - custom_mse: 38.7077 - recall: 0.5717\n",
            "reduce_confident_loss =  1.23679137\n",
            "\n",
            "reduce_mean_cls_loss =  0.620506465\n",
            "\n",
            "regression loss =  8.27630901\n",
            "33/41 [=======================>......] - ETA: 4:28 - loss: 10.4584 - custom_mse: 46.1345 - recall: 0.5802\n",
            "reduce_confident_loss =  0.21522148\n",
            "\n",
            "reduce_mean_cls_loss =  0.545354843\n",
            "\n",
            "regression loss =  6.41978741\n",
            "34/41 [=======================>......] - ETA: 3:54 - loss: 10.3620 - custom_mse: 70.5439 - recall: 0.5920\n",
            "reduce_confident_loss =  0.379234642\n",
            "\n",
            "reduce_mean_cls_loss =  0.229895607\n",
            "\n",
            "regression loss =  10.5705948\n",
            "35/41 [========================>.....] - ETA: 3:21 - loss: 10.3854 - custom_mse: 194.6297 - recall: 0.5999\n",
            "reduce_confident_loss =  0.194686815\n",
            "\n",
            "reduce_mean_cls_loss =  0.333183974\n",
            "\n",
            "regression loss =  12.8847656\n",
            "36/41 [=========================>....] - ETA: 2:48 - loss: 10.4695 - custom_mse: 198.0904 - recall: 0.6045\n",
            "reduce_confident_loss =  0.323625833\n",
            "\n",
            "reduce_mean_cls_loss =  0.178040773\n",
            "\n",
            "regression loss =  8.24435234\n",
            "37/41 [==========================>...] - ETA: 2:14 - loss: 10.4229 - custom_mse: 80.3961 - recall: 0.6138 \n",
            "reduce_confident_loss =  0.317021191\n",
            "\n",
            "reduce_mean_cls_loss =  1.0768826\n",
            "\n",
            "regression loss =  7.42355061\n",
            "38/41 [==========================>...] - ETA: 1:40 - loss: 10.3806 - custom_mse: 155.4079 - recall: 0.6240\n",
            "reduce_confident_loss =  0.393598706\n",
            "\n",
            "reduce_mean_cls_loss =  0.881952286\n",
            "\n",
            "regression loss =  13.8680344\n",
            "39/41 [===========================>..] - ETA: 1:07 - loss: 10.5028 - custom_mse: 219.1049 - recall: 0.6080\n",
            "reduce_confident_loss =  0.209800407\n",
            "\n",
            "reduce_mean_cls_loss =  0.668579221\n",
            "\n",
            "regression loss =  11.9041233\n",
            "40/41 [============================>.] - ETA: 33s - loss: 10.5598 - custom_mse: 212.1976 - recall: 0.5928 indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 1\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0382458866 0.212100536 0.0725413859 ... 0.999223232 0.974157572 1.49100679e-05]\n",
            " [0.225472987 0.239958972 0.49586153 ... 0.98494 0.979532659 9.39054189e-06]\n",
            " [0.233719826 0.0367499888 0.142427474 ... 0.992926061 0.986726344 0.000229179859]\n",
            " [0.649498224 0.0563360751 0.500305772 ... 0.898177147 0.989108264 4.62601893e-05]\n",
            " [0.604417801 0.0178038776 0.0909016132 ... 0.998362601 0.989108324 0.00178039074]]\n",
            "\n",
            "reduce_confident_loss =  0.319774777\n",
            "\n",
            "reduce_mean_cls_loss =  0.348374307\n",
            "\n",
            "regression loss =  12.0008831\n",
            "41/41 [==============================] - ETA: 0s - loss: 10.5761 - custom_mse: 177.0923 - recall: 0.5783 \n",
            "reduce_confident_loss =  0.289332062\n",
            "\n",
            "reduce_mean_cls_loss =  0.577062488\n",
            "\n",
            "regression loss =  12.4001007\n",
            "\n",
            "reduce_confident_loss =  0.27878359\n",
            "\n",
            "reduce_mean_cls_loss =  0.575076222\n",
            "\n",
            "regression loss =  9.84466362\n",
            "\n",
            "reduce_confident_loss =  0.283658296\n",
            "\n",
            "reduce_mean_cls_loss =  5.32710171\n",
            "\n",
            "regression loss =  16.3320045\n",
            "\n",
            "reduce_confident_loss =  0.412137836\n",
            "\n",
            "reduce_mean_cls_loss =  0.661895692\n",
            "\n",
            "regression loss =  14.0917377\n",
            "\n",
            "reduce_confident_loss =  0.2882213\n",
            "\n",
            "reduce_mean_cls_loss =  5.44125652\n",
            "\n",
            "regression loss =  16.6712761\n",
            "\n",
            "reduce_confident_loss =  0.28303954\n",
            "\n",
            "reduce_mean_cls_loss =  1.53561532\n",
            "\n",
            "regression loss =  15.1362028\n",
            "\n",
            "reduce_confident_loss =  0.415605962\n",
            "\n",
            "reduce_mean_cls_loss =  0.643728137\n",
            "\n",
            "regression loss =  14.9612122\n",
            "\n",
            "reduce_confident_loss =  0.282154649\n",
            "\n",
            "reduce_mean_cls_loss =  5.33642817\n",
            "\n",
            "regression loss =  16.3756866\n",
            "\n",
            "reduce_confident_loss =  0.283023059\n",
            "\n",
            "reduce_mean_cls_loss =  5.31890774\n",
            "\n",
            "regression loss =  11.2164869\n",
            "\n",
            "reduce_confident_loss =  0.283644229\n",
            "\n",
            "reduce_mean_cls_loss =  5.35019684\n",
            "\n",
            "regression loss =  12.1550579\n",
            "\n",
            "reduce_confident_loss =  0.274524778\n",
            "\n",
            "reduce_mean_cls_loss =  5.34972525\n",
            "\n",
            "regression loss =  13.9754457\n",
            "\n",
            "reduce_confident_loss =  0.385840118\n",
            "\n",
            "reduce_mean_cls_loss =  0.650632858\n",
            "\n",
            "regression loss =  19.5775127\n",
            "\n",
            "reduce_confident_loss =  0.314574212\n",
            "\n",
            "reduce_mean_cls_loss =  0.285553932\n",
            "\n",
            "regression loss =  13.4906588\n",
            "\n",
            "reduce_confident_loss =  0.275240928\n",
            "\n",
            "reduce_mean_cls_loss =  5.33100557\n",
            "\n",
            "regression loss =  14.6397667\n",
            "\n",
            "reduce_confident_loss =  0.274171889\n",
            "\n",
            "reduce_mean_cls_loss =  0.00475974567\n",
            "\n",
            "regression loss =  26.9438133\n",
            "41/41 [==============================] - 1549s 38s/step - loss: 10.5761 - custom_mse: 177.0923 - recall: 0.5783 - val_loss: 18.1708 - val_custom_mse: 138.2636 - val_recall: 0.5750\n",
            "Epoch 4/10\n",
            "\n",
            "reduce_confident_loss =  0.208453774\n",
            "\n",
            "reduce_mean_cls_loss =  2.41667032\n",
            "\n",
            "regression loss =  13.0762787\n",
            " 1/41 [..............................] - ETA: 22:09 - loss: 15.7014 - custom_mse: 133.6678 - recall: 0.7096\n",
            "reduce_confident_loss =  0.213011712\n",
            "\n",
            "reduce_mean_cls_loss =  0.359332144\n",
            "\n",
            "regression loss =  9.76067543\n",
            " 2/41 [>.............................] - ETA: 23:48 - loss: 13.0172 - custom_mse: 91.8249 - recall: 0.8202 \n",
            "reduce_confident_loss =  0.128476381\n",
            "\n",
            "reduce_mean_cls_loss =  1.84676802\n",
            "\n",
            "regression loss =  10.3856421\n",
            " 3/41 [=>............................] - ETA: 21:13 - loss: 12.7984 - custom_mse: 201.0852 - recall: 0.6416\n",
            "reduce_confident_loss =  0.268463165\n",
            "\n",
            "reduce_mean_cls_loss =  0.55577451\n",
            "\n",
            "regression loss =  10.9723053\n",
            " 4/41 [=>............................] - ETA: 20:03 - loss: 12.5480 - custom_mse: 229.8754 - recall: 0.6906\n",
            "reduce_confident_loss =  0.367320478\n",
            "\n",
            "reduce_mean_cls_loss =  0.337410897\n",
            "\n",
            "regression loss =  13.0959282\n",
            " 5/41 [==>...........................] - ETA: 19:07 - loss: 12.7985 - custom_mse: 129.4630 - recall: 0.7285\n",
            "reduce_confident_loss =  0.342631608\n",
            "\n",
            "reduce_mean_cls_loss =  0.015822133\n",
            "\n",
            "regression loss =  16.1519489\n",
            " 6/41 [===>..........................] - ETA: 18:32 - loss: 13.4172 - custom_mse: 146.3242 - recall: 0.7737Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.339967877\n",
            "\n",
            "reduce_mean_cls_loss =  0.0540635586\n",
            "\n",
            "regression loss =  13.8258801\n",
            " 7/41 [====>.........................] - ETA: 18:42 - loss: 13.5318 - custom_mse: 202.6052 - recall: 0.8060\n",
            "reduce_confident_loss =  0.140661269\n",
            "\n",
            "reduce_mean_cls_loss =  0.0552542284\n",
            "\n",
            "regression loss =  10.3514471\n",
            " 8/41 [====>.........................] - ETA: 18:06 - loss: 13.1588 - custom_mse: 271.8632 - recall: 0.7117\n",
            "reduce_confident_loss =  0.568974\n",
            "\n",
            "reduce_mean_cls_loss =  1.23826516\n",
            "\n",
            "regression loss =  7.57926226\n",
            " 9/41 [=====>........................] - ETA: 17:30 - loss: 12.7396 - custom_mse: 49.9948 - recall: 0.6754 \n",
            "reduce_confident_loss =  0.161709756\n",
            "\n",
            "reduce_mean_cls_loss =  0.0632345304\n",
            "\n",
            "regression loss =  3.52179694\n",
            "10/41 [======>.......................] - ETA: 16:55 - loss: 11.8403 - custom_mse: 238.2295 - recall: 0.6078\n",
            "reduce_confident_loss =  0.131077677\n",
            "\n",
            "reduce_mean_cls_loss =  0.0125849461\n",
            "\n",
            "regression loss =  1.88567352\n",
            "11/41 [=======>......................] - ETA: 16:23 - loss: 10.9484 - custom_mse: 227.9511 - recall: 0.5526\n",
            "reduce_confident_loss =  0.253742784\n",
            "\n",
            "reduce_mean_cls_loss =  0.0161860734\n",
            "\n",
            "regression loss =  3.53314543\n",
            "12/41 [=======>......................] - ETA: 15:51 - loss: 10.3530 - custom_mse: 177.9406 - recall: 0.5065\n",
            "reduce_confident_loss =  0.0822232813\n",
            "\n",
            "reduce_mean_cls_loss =  0.0339788124\n",
            "\n",
            "regression loss =  4.72248888\n",
            "13/41 [========>.....................] - ETA: 15:17 - loss: 9.9288 - custom_mse: 173.3596 - recall: 0.4676 \n",
            "reduce_confident_loss =  0.113769226\n",
            "\n",
            "reduce_mean_cls_loss =  0.155674726\n",
            "\n",
            "regression loss =  5.42177057\n",
            "14/41 [=========>....................] - ETA: 14:49 - loss: 9.6261 - custom_mse: 99.7831 - recall: 0.4342 \n",
            "reduce_confident_loss =  0.142043784\n",
            "\n",
            "reduce_mean_cls_loss =  0.269748718\n",
            "\n",
            "regression loss =  9.95632553\n",
            "15/41 [=========>....................] - ETA: 14:14 - loss: 9.6756 - custom_mse: 115.6640 - recall: 0.4052\n",
            "reduce_confident_loss =  0.143646404\n",
            "\n",
            "reduce_mean_cls_loss =  0.870747149\n",
            "\n",
            "regression loss =  12.2086124\n",
            "16/41 [==========>...................] - ETA: 13:40 - loss: 9.8973 - custom_mse: 194.5636 - recall: 0.3836\n",
            "reduce_confident_loss =  0.374817222\n",
            "\n",
            "reduce_mean_cls_loss =  0.392900944\n",
            "\n",
            "regression loss =  15.1158352\n",
            "17/41 [===========>..................] - ETA: 13:44 - loss: 10.2494 - custom_mse: 93.9668 - recall: 0.4192\n",
            "reduce_confident_loss =  0.193811625\n",
            "\n",
            "reduce_mean_cls_loss =  0.0623343177\n",
            "\n",
            "regression loss =  17.1142502\n",
            "18/41 [============>.................] - ETA: 13:06 - loss: 10.6450 - custom_mse: 62.0144 - recall: 0.4514\n",
            "reduce_confident_loss =  0.106899925\n",
            "\n",
            "reduce_mean_cls_loss =  0.158278197\n",
            "\n",
            "regression loss =  11.1551085\n",
            "19/41 [============>.................] - ETA: 12:30 - loss: 10.6858 - custom_mse: 37.4691 - recall: 0.4788\n",
            "reduce_confident_loss =  0.299667746\n",
            "\n",
            "reduce_mean_cls_loss =  0.448806971\n",
            "\n",
            "regression loss =  13.65763\n",
            "20/41 [=============>................] - ETA: 12:00 - loss: 10.8719 - custom_mse: 141.6751 - recall: 0.4938\n",
            "reduce_confident_loss =  0.547945857\n",
            "\n",
            "reduce_mean_cls_loss =  1.64003432\n",
            "\n",
            "regression loss =  8.62425137\n",
            "21/41 [==============>...............] - ETA: 11:25 - loss: 10.8690 - custom_mse: 148.7969 - recall: 0.4879\n",
            "reduce_confident_loss =  0.286402881\n",
            "\n",
            "reduce_mean_cls_loss =  0.467243969\n",
            "\n",
            "regression loss =  9.08423805\n",
            "22/41 [===============>..............] - ETA: 10:50 - loss: 10.8222 - custom_mse: 215.2252 - recall: 0.5073\n",
            "reduce_confident_loss =  0.436189115\n",
            "\n",
            "reduce_mean_cls_loss =  0.24577117\n",
            "\n",
            "regression loss =  8.57336712\n",
            "23/41 [===============>..............] - ETA: 10:14 - loss: 10.7540 - custom_mse: 168.7162 - recall: 0.5198\n",
            "reduce_confident_loss =  0.260491103\n",
            "\n",
            "reduce_mean_cls_loss =  0.120878018\n",
            "\n",
            "regression loss =  5.95033073\n",
            "24/41 [================>.............] - ETA: 9:38 - loss: 10.5698 - custom_mse: 165.2205 - recall: 0.5398 \n",
            "reduce_confident_loss =  0.311709344\n",
            "\n",
            "reduce_mean_cls_loss =  0.185553521\n",
            "\n",
            "regression loss =  6.82424927\n",
            "25/41 [=================>............] - ETA: 9:03 - loss: 10.4398 - custom_mse: 169.8740 - recall: 0.5582\n",
            "reduce_confident_loss =  0.211968392\n",
            "\n",
            "reduce_mean_cls_loss =  0.0934372321\n",
            "\n",
            "regression loss =  11.5755243\n",
            "26/41 [==================>...........] - ETA: 8:28 - loss: 10.4953 - custom_mse: 186.0675 - recall: 0.5741\n",
            "reduce_confident_loss =  0.315484524\n",
            "\n",
            "reduce_mean_cls_loss =  0.203788832\n",
            "\n",
            "regression loss =  8.28042221\n",
            "27/41 [==================>...........] - ETA: 7:53 - loss: 10.4325 - custom_mse: 108.7104 - recall: 0.5886\n",
            "reduce_confident_loss =  0.156357497\n",
            "\n",
            "reduce_mean_cls_loss =  0.0393062644\n",
            "\n",
            "regression loss =  3.21629\n",
            "28/41 [===================>..........] - ETA: 7:19 - loss: 10.1817 - custom_mse: 88.6453 - recall: 0.6033 \n",
            "reduce_confident_loss =  0.303408086\n",
            "\n",
            "reduce_mean_cls_loss =  0.0889404938\n",
            "\n",
            "regression loss =  5.10007572\n",
            "29/41 [====================>.........] - ETA: 6:46 - loss: 10.0200 - custom_mse: 67.4284 - recall: 0.6170\n",
            "reduce_confident_loss =  0.253260434\n",
            "\n",
            "reduce_mean_cls_loss =  0.0836923495\n",
            "\n",
            "regression loss =  4.68426609\n",
            "30/41 [====================>.........] - ETA: 6:12 - loss: 9.8534 - custom_mse: 82.0715 - recall: 0.6297 \n",
            "reduce_confident_loss =  0.211483479\n",
            "\n",
            "reduce_mean_cls_loss =  0.23670657\n",
            "\n",
            "regression loss =  3.96935344\n",
            "31/41 [=====================>........] - ETA: 5:38 - loss: 9.6781 - custom_mse: 59.4941 - recall: 0.6405\n",
            "reduce_confident_loss =  0.544527113\n",
            "\n",
            "reduce_mean_cls_loss =  0.572271228\n",
            "\n",
            "regression loss =  5.66394329\n",
            "32/41 [======================>.......] - ETA: 5:03 - loss: 9.5875 - custom_mse: 55.1841 - recall: 0.6426\n",
            "reduce_confident_loss =  1.38193727\n",
            "\n",
            "reduce_mean_cls_loss =  1.02371514\n",
            "\n",
            "regression loss =  7.76423454\n",
            "33/41 [=======================>......] - ETA: 4:29 - loss: 9.6052 - custom_mse: 37.4829 - recall: 0.6426\n",
            "reduce_confident_loss =  0.222147301\n",
            "\n",
            "reduce_mean_cls_loss =  0.120073237\n",
            "\n",
            "regression loss =  6.29466105\n",
            "34/41 [=======================>......] - ETA: 3:55 - loss: 9.5179 - custom_mse: 62.0013 - recall: 0.6511\n",
            "reduce_confident_loss =  0.426076829\n",
            "\n",
            "reduce_mean_cls_loss =  0.583268\n",
            "\n",
            "regression loss =  8.36360168\n",
            "35/41 [========================>.....] - ETA: 3:22 - loss: 9.5137 - custom_mse: 228.2782 - recall: 0.6464\n",
            "reduce_confident_loss =  0.260025442\n",
            "\n",
            "reduce_mean_cls_loss =  2.66023636\n",
            "\n",
            "regression loss =  12.3864174\n",
            "36/41 [=========================>....] - ETA: 2:49 - loss: 9.6746 - custom_mse: 177.1821 - recall: 0.6307\n",
            "reduce_confident_loss =  0.27587834\n",
            "\n",
            "reduce_mean_cls_loss =  2.72626376\n",
            "\n",
            "regression loss =  7.78617668\n",
            "37/41 [==========================>...] - ETA: 2:15 - loss: 9.7047 - custom_mse: 75.0857 - recall: 0.6136 \n",
            "reduce_confident_loss =  0.50477314\n",
            "\n",
            "reduce_mean_cls_loss =  1.16688621\n",
            "\n",
            "regression loss =  10.1273584\n",
            "38/41 [==========================>...] - ETA: 1:42 - loss: 9.7598 - custom_mse: 109.2677 - recall: 0.5975\n",
            "reduce_confident_loss =  0.277355433\n",
            "\n",
            "reduce_mean_cls_loss =  0.0326352566\n",
            "\n",
            "regression loss =  10.8547182\n",
            "39/41 [===========================>..] - ETA: 1:08 - loss: 9.7959 - custom_mse: 216.7113 - recall: 0.5822\n",
            "reduce_confident_loss =  0.178206414\n",
            "\n",
            "reduce_mean_cls_loss =  0.190431044\n",
            "\n",
            "regression loss =  9.08248901\n",
            "40/41 [============================>.] - ETA: 34s - loss: 9.7872 - custom_mse: 203.0603 - recall: 0.5676 indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.000280737877 0.116731673 0.00981634855 ... 0.995871782 0.742464244 2.97202987e-05]\n",
            " [0.0133527517 0.121630013 0.0425127149 ... 0.998630762 0.92209065 0.00194492936]\n",
            " [0.00339642167 0.0543026626 0.0396491 ... 0.998611331 0.539048374 0.000372320414]\n",
            " [0.150093555 0.265348196 0.0459229946 ... 0.977315366 0.823223829 0.00199010968]\n",
            " [0.00881075859 0.109352052 0.0103780627 ... 0.999412656 0.64585197 7.16557406e-05]]\n",
            "\n",
            "reduce_confident_loss =  0.264734387\n",
            "\n",
            "reduce_mean_cls_loss =  0.0874609724\n",
            "\n",
            "regression loss =  7.86678934\n",
            "41/41 [==============================] - ETA: 0s - loss: 9.7751 - custom_mse: 115.6392 - recall: 0.5538 \n",
            "reduce_confident_loss =  0.317786\n",
            "\n",
            "reduce_mean_cls_loss =  0.557620108\n",
            "\n",
            "regression loss =  12.8669615\n",
            "\n",
            "reduce_confident_loss =  0.313933909\n",
            "\n",
            "reduce_mean_cls_loss =  0.542602956\n",
            "\n",
            "regression loss =  9.70137787\n",
            "\n",
            "reduce_confident_loss =  0.381932199\n",
            "\n",
            "reduce_mean_cls_loss =  4.27604771\n",
            "\n",
            "regression loss =  16.9767303\n",
            "\n",
            "reduce_confident_loss =  0.508170903\n",
            "\n",
            "reduce_mean_cls_loss =  0.666591763\n",
            "\n",
            "regression loss =  13.858942\n",
            "\n",
            "reduce_confident_loss =  0.395005703\n",
            "\n",
            "reduce_mean_cls_loss =  4.36185503\n",
            "\n",
            "regression loss =  17.0490799\n",
            "\n",
            "reduce_confident_loss =  0.3827779\n",
            "\n",
            "reduce_mean_cls_loss =  1.24041748\n",
            "\n",
            "regression loss =  13.1978407\n",
            "\n",
            "reduce_confident_loss =  0.506679714\n",
            "\n",
            "reduce_mean_cls_loss =  0.647793472\n",
            "\n",
            "regression loss =  15.1117163\n",
            "\n",
            "reduce_confident_loss =  0.386398107\n",
            "\n",
            "reduce_mean_cls_loss =  4.27824\n",
            "\n",
            "regression loss =  15.369031\n",
            "\n",
            "reduce_confident_loss =  0.384748459\n",
            "\n",
            "reduce_mean_cls_loss =  4.24989557\n",
            "\n",
            "regression loss =  12.9296846\n",
            "\n",
            "reduce_confident_loss =  0.384528577\n",
            "\n",
            "reduce_mean_cls_loss =  4.3022809\n",
            "\n",
            "regression loss =  13.764205\n",
            "\n",
            "reduce_confident_loss =  0.372586459\n",
            "\n",
            "reduce_mean_cls_loss =  4.30323744\n",
            "\n",
            "regression loss =  14.9748869\n",
            "\n",
            "reduce_confident_loss =  0.474064052\n",
            "\n",
            "reduce_mean_cls_loss =  0.647928655\n",
            "\n",
            "regression loss =  18.159462\n",
            "\n",
            "reduce_confident_loss =  0.365718633\n",
            "\n",
            "reduce_mean_cls_loss =  0.261459172\n",
            "\n",
            "regression loss =  12.5008192\n",
            "\n",
            "reduce_confident_loss =  0.385134429\n",
            "\n",
            "reduce_mean_cls_loss =  4.28905582\n",
            "\n",
            "regression loss =  10.6893921\n",
            "\n",
            "reduce_confident_loss =  0.375453591\n",
            "\n",
            "reduce_mean_cls_loss =  0.0140996417\n",
            "\n",
            "regression loss =  23.1369839\n",
            "41/41 [==============================] - 1580s 39s/step - loss: 9.7751 - custom_mse: 115.6392 - recall: 0.5538 - val_loss: 17.2867 - val_custom_mse: 123.1802 - val_recall: 0.5750\n",
            "Epoch 5/10\n",
            "\n",
            "reduce_confident_loss =  0.169590473\n",
            "\n",
            "reduce_mean_cls_loss =  1.20538795\n",
            "\n",
            "regression loss =  14.6228809\n",
            " 1/41 [..............................] - ETA: 23:09 - loss: 15.9979 - custom_mse: 131.0578 - recall: 0.5863\n",
            "reduce_confident_loss =  0.198156148\n",
            "\n",
            "reduce_mean_cls_loss =  0.639127254\n",
            "\n",
            "regression loss =  9.60845\n",
            " 2/41 [>.............................] - ETA: 20:08 - loss: 13.2218 - custom_mse: 101.8825 - recall: 0.6504\n",
            "reduce_confident_loss =  0.154918328\n",
            "\n",
            "reduce_mean_cls_loss =  1.89277673\n",
            "\n",
            "regression loss =  7.90634\n",
            " 3/41 [=>............................] - ETA: 19:21 - loss: 12.1325 - custom_mse: 209.4622 - recall: 0.5551\n",
            "reduce_confident_loss =  0.333341599\n",
            "\n",
            "reduce_mean_cls_loss =  0.774571419\n",
            "\n",
            "regression loss =  8.35991669\n",
            " 4/41 [=>............................] - ETA: 19:09 - loss: 11.4664 - custom_mse: 247.9309 - recall: 0.6528\n",
            "reduce_confident_loss =  0.375964344\n",
            "\n",
            "reduce_mean_cls_loss =  0.372256279\n",
            "\n",
            "regression loss =  8.4328928\n",
            " 5/41 [==>...........................] - ETA: 18:27 - loss: 11.0093 - custom_mse: 128.8931 - recall: 0.7097\n",
            "reduce_confident_loss =  0.264588535\n",
            "\n",
            "reduce_mean_cls_loss =  0.0409619734\n",
            "\n",
            "regression loss =  10.5761328\n",
            " 6/41 [===>..........................] - ETA: 18:01 - loss: 10.9880 - custom_mse: 127.1163 - recall: 0.7581Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.328610837\n",
            "\n",
            "reduce_mean_cls_loss =  0.0101987822\n",
            "\n",
            "regression loss =  11.7694674\n",
            " 7/41 [====>.........................] - ETA: 18:17 - loss: 11.1481 - custom_mse: 180.5748 - recall: 0.7926\n",
            "reduce_confident_loss =  0.105292745\n",
            "\n",
            "reduce_mean_cls_loss =  0.136760414\n",
            "\n",
            "regression loss =  12.4032135\n",
            " 8/41 [====>.........................] - ETA: 17:46 - loss: 11.3352 - custom_mse: 281.6928 - recall: 0.6947\n",
            "reduce_confident_loss =  0.613151491\n",
            "\n",
            "reduce_mean_cls_loss =  1.29608667\n",
            "\n",
            "regression loss =  8.99912262\n",
            " 9/41 [=====>........................] - ETA: 17:32 - loss: 11.2878 - custom_mse: 67.0174 - recall: 0.6227 \n",
            "reduce_confident_loss =  0.121795878\n",
            "\n",
            "reduce_mean_cls_loss =  0.112826027\n",
            "\n",
            "regression loss =  5.68649244\n",
            "10/41 [======>.......................] - ETA: 17:03 - loss: 10.7511 - custom_mse: 230.9774 - recall: 0.5605\n",
            "reduce_confident_loss =  0.0684850588\n",
            "\n",
            "reduce_mean_cls_loss =  0.106554165\n",
            "\n",
            "regression loss =  3.42139578\n",
            "11/41 [=======>......................] - ETA: 16:32 - loss: 10.1007 - custom_mse: 228.5895 - recall: 0.5095\n",
            "reduce_confident_loss =  0.0941037312\n",
            "\n",
            "reduce_mean_cls_loss =  0.111767396\n",
            "\n",
            "regression loss =  2.86804318\n",
            "12/41 [=======>......................] - ETA: 15:57 - loss: 9.5151 - custom_mse: 181.7618 - recall: 0.4670 \n",
            "reduce_confident_loss =  0.0991966203\n",
            "\n",
            "reduce_mean_cls_loss =  0.492358774\n",
            "\n",
            "regression loss =  3.14848208\n",
            "13/41 [========>.....................] - ETA: 15:22 - loss: 9.0709 - custom_mse: 118.8913 - recall: 0.4311\n",
            "reduce_confident_loss =  0.0754256099\n",
            "\n",
            "reduce_mean_cls_loss =  0.467120707\n",
            "\n",
            "regression loss =  4.26157379\n",
            "14/41 [=========>....................] - ETA: 14:48 - loss: 8.7661 - custom_mse: 104.0997 - recall: 0.4003\n",
            "reduce_confident_loss =  0.156055406\n",
            "\n",
            "reduce_mean_cls_loss =  0.120161951\n",
            "\n",
            "regression loss =  10.1031055\n",
            "15/41 [=========>....................] - ETA: 14:14 - loss: 8.8737 - custom_mse: 130.1423 - recall: 0.3736\n",
            "reduce_confident_loss =  0.103850536\n",
            "\n",
            "reduce_mean_cls_loss =  1.07572603\n",
            "\n",
            "regression loss =  11.9762554\n",
            "16/41 [==========>...................] - ETA: 13:40 - loss: 9.1413 - custom_mse: 166.3828 - recall: 0.3570\n",
            "reduce_confident_loss =  0.248799935\n",
            "\n",
            "reduce_mean_cls_loss =  0.236048818\n",
            "\n",
            "regression loss =  13.2515268\n",
            "17/41 [===========>..................] - ETA: 13:49 - loss: 9.4116 - custom_mse: 74.5916 - recall: 0.3948 \n",
            "reduce_confident_loss =  0.203716621\n",
            "\n",
            "reduce_mean_cls_loss =  0.0452420898\n",
            "\n",
            "regression loss =  18.1876\n",
            "18/41 [============>.................] - ETA: 13:14 - loss: 9.9130 - custom_mse: 71.4113 - recall: 0.4281\n",
            "reduce_confident_loss =  0.213609219\n",
            "\n",
            "reduce_mean_cls_loss =  0.371972084\n",
            "\n",
            "regression loss =  12.2190409\n",
            "19/41 [============>.................] - ETA: 12:37 - loss: 10.0652 - custom_mse: 31.0449 - recall: 0.4492\n",
            "reduce_confident_loss =  0.523171604\n",
            "\n",
            "reduce_mean_cls_loss =  0.310894579\n",
            "\n",
            "regression loss =  11.9939842\n",
            "20/41 [=============>................] - ETA: 12:00 - loss: 10.2033 - custom_mse: 136.0664 - recall: 0.4651\n",
            "reduce_confident_loss =  0.284244448\n",
            "\n",
            "reduce_mean_cls_loss =  2.05076599\n",
            "\n",
            "regression loss =  7.91870356\n",
            "21/41 [==============>...............] - ETA: 11:24 - loss: 10.2057 - custom_mse: 150.0184 - recall: 0.4600\n",
            "reduce_confident_loss =  0.219970435\n",
            "\n",
            "reduce_mean_cls_loss =  0.496727884\n",
            "\n",
            "regression loss =  9.28375912\n",
            "22/41 [===============>..............] - ETA: 10:48 - loss: 10.1964 - custom_mse: 216.9440 - recall: 0.4755\n",
            "reduce_confident_loss =  0.18286182\n",
            "\n",
            "reduce_mean_cls_loss =  0.54719919\n",
            "\n",
            "regression loss =  9.39874363\n",
            "23/41 [===============>..............] - ETA: 10:12 - loss: 10.1935 - custom_mse: 143.4146 - recall: 0.4805\n",
            "reduce_confident_loss =  0.481970042\n",
            "\n",
            "reduce_mean_cls_loss =  0.224426344\n",
            "\n",
            "regression loss =  4.29997301\n",
            "24/41 [================>.............] - ETA: 9:37 - loss: 9.9773 - custom_mse: 104.9647 - recall: 0.4964  \n",
            "reduce_confident_loss =  0.501718044\n",
            "\n",
            "reduce_mean_cls_loss =  0.0673977956\n",
            "\n",
            "regression loss =  4.63001776\n",
            "25/41 [=================>............] - ETA: 9:02 - loss: 9.7862 - custom_mse: 134.7384 - recall: 0.5156\n",
            "reduce_confident_loss =  0.544247329\n",
            "\n",
            "reduce_mean_cls_loss =  0.0438855961\n",
            "\n",
            "regression loss =  7.38080645\n",
            "26/41 [==================>...........] - ETA: 8:27 - loss: 9.7163 - custom_mse: 131.3714 - recall: 0.5339\n",
            "reduce_confident_loss =  0.262207985\n",
            "\n",
            "reduce_mean_cls_loss =  0.267485887\n",
            "\n",
            "regression loss =  6.17479753\n",
            "27/41 [==================>...........] - ETA: 7:56 - loss: 9.6048 - custom_mse: 112.4400 - recall: 0.5445\n",
            "reduce_confident_loss =  0.130303875\n",
            "\n",
            "reduce_mean_cls_loss =  0.121901765\n",
            "\n",
            "regression loss =  2.76082444\n",
            "28/41 [===================>..........] - ETA: 7:21 - loss: 9.3693 - custom_mse: 85.5036 - recall: 0.5568 \n",
            "reduce_confident_loss =  0.119371757\n",
            "\n",
            "reduce_mean_cls_loss =  0.0776190758\n",
            "\n",
            "regression loss =  5.58693171\n",
            "29/41 [====================>.........] - ETA: 6:47 - loss: 9.2457 - custom_mse: 72.5827 - recall: 0.5714\n",
            "reduce_confident_loss =  0.345752865\n",
            "\n",
            "reduce_mean_cls_loss =  0.45811674\n",
            "\n",
            "regression loss =  4.93096733\n",
            "30/41 [====================>.........] - ETA: 6:12 - loss: 9.1287 - custom_mse: 67.3973 - recall: 0.5857\n",
            "reduce_confident_loss =  0.314522058\n",
            "\n",
            "reduce_mean_cls_loss =  0.330593318\n",
            "\n",
            "regression loss =  3.70906043\n",
            "31/41 [=====================>........] - ETA: 5:38 - loss: 8.9747 - custom_mse: 60.7280 - recall: 0.5990\n",
            "reduce_confident_loss =  0.385705262\n",
            "\n",
            "reduce_mean_cls_loss =  0.570283\n",
            "\n",
            "regression loss =  6.05513954\n",
            "32/41 [======================>.......] - ETA: 5:04 - loss: 8.9133 - custom_mse: 55.7108 - recall: 0.6076\n",
            "reduce_confident_loss =  0.515442431\n",
            "\n",
            "reduce_mean_cls_loss =  0.588973463\n",
            "\n",
            "regression loss =  7.96453524\n",
            "33/41 [=======================>......] - ETA: 4:30 - loss: 8.9180 - custom_mse: 39.8188 - recall: 0.6178\n",
            "reduce_confident_loss =  0.312122762\n",
            "\n",
            "reduce_mean_cls_loss =  0.356921\n",
            "\n",
            "regression loss =  6.76042414\n",
            "34/41 [=======================>......] - ETA: 3:56 - loss: 8.8742 - custom_mse: 49.6083 - recall: 0.6289\n",
            "reduce_confident_loss =  0.227746725\n",
            "\n",
            "reduce_mean_cls_loss =  0.60415405\n",
            "\n",
            "regression loss =  9.28224754\n",
            "35/41 [========================>.....] - ETA: 3:23 - loss: 8.9097 - custom_mse: 209.9479 - recall: 0.6321\n",
            "reduce_confident_loss =  0.212815046\n",
            "\n",
            "reduce_mean_cls_loss =  0.66111815\n",
            "\n",
            "regression loss =  13.0271521\n",
            "36/41 [=========================>....] - ETA: 2:50 - loss: 9.0483 - custom_mse: 176.5327 - recall: 0.6236\n",
            "reduce_confident_loss =  0.113024577\n",
            "\n",
            "reduce_mean_cls_loss =  1.06653345\n",
            "\n",
            "regression loss =  8.05867863\n",
            "37/41 [==========================>...] - ETA: 2:16 - loss: 9.0534 - custom_mse: 76.0127 - recall: 0.6135 \n",
            "reduce_confident_loss =  0.182710245\n",
            "\n",
            "reduce_mean_cls_loss =  0.740709722\n",
            "\n",
            "regression loss =  6.53124428\n",
            "38/41 [==========================>...] - ETA: 1:41 - loss: 9.0114 - custom_mse: 115.7411 - recall: 0.6097\n",
            "reduce_confident_loss =  0.136690363\n",
            "\n",
            "reduce_mean_cls_loss =  0.213650227\n",
            "\n",
            "regression loss =  13.5315533\n",
            "39/41 [===========================>..] - ETA: 1:07 - loss: 9.1363 - custom_mse: 259.4658 - recall: 0.5941\n",
            "reduce_confident_loss =  0.103369728\n",
            "\n",
            "reduce_mean_cls_loss =  0.284731805\n",
            "\n",
            "regression loss =  9.3164854\n",
            "40/41 [============================>.] - ETA: 33s - loss: 9.1505 - custom_mse: 188.4851 - recall: 0.5793 indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.191907316 0.182989627 0.0904974639 ... 0.968666434 0.828355074 0.000184059143]\n",
            " [0.170231938 0.0742145777 0.0476009846 ... 0.977961063 0.939816475 0.00204381347]\n",
            " [0.160188317 0.0448077619 0.044028759 ... 0.990954578 0.961000323 0.000669151545]\n",
            " [0.0359750092 0.0950853229 0.0363273919 ... 0.982959092 0.818678856 0.00441035628]\n",
            " [0.388980329 0.381842166 0.0737454593 ... 0.96659404 0.819439888 0.00321856141]]\n",
            "\n",
            "reduce_confident_loss =  0.159575388\n",
            "\n",
            "reduce_mean_cls_loss =  0.173171431\n",
            "\n",
            "regression loss =  9.13896084\n",
            "41/41 [==============================] - ETA: 0s - loss: 9.1530 - custom_mse: 197.3290 - recall: 0.5651 \n",
            "reduce_confident_loss =  0.288549423\n",
            "\n",
            "reduce_mean_cls_loss =  0.913217425\n",
            "\n",
            "regression loss =  14.4425783\n",
            "\n",
            "reduce_confident_loss =  0.292943209\n",
            "\n",
            "reduce_mean_cls_loss =  0.910912514\n",
            "\n",
            "regression loss =  10.3273287\n",
            "\n",
            "reduce_confident_loss =  0.290916234\n",
            "\n",
            "reduce_mean_cls_loss =  3.79667401\n",
            "\n",
            "regression loss =  18.5194874\n",
            "\n",
            "reduce_confident_loss =  0.380408973\n",
            "\n",
            "reduce_mean_cls_loss =  1.22733712\n",
            "\n",
            "regression loss =  17.6962605\n",
            "\n",
            "reduce_confident_loss =  0.289438665\n",
            "\n",
            "reduce_mean_cls_loss =  3.87053132\n",
            "\n",
            "regression loss =  20.643301\n",
            "\n",
            "reduce_confident_loss =  0.283120185\n",
            "\n",
            "reduce_mean_cls_loss =  1.11665308\n",
            "\n",
            "regression loss =  11.9692173\n",
            "\n",
            "reduce_confident_loss =  0.370309085\n",
            "\n",
            "reduce_mean_cls_loss =  1.17332137\n",
            "\n",
            "regression loss =  12.8978748\n",
            "\n",
            "reduce_confident_loss =  0.271650821\n",
            "\n",
            "reduce_mean_cls_loss =  3.80946398\n",
            "\n",
            "regression loss =  23.0270805\n",
            "\n",
            "reduce_confident_loss =  0.266485274\n",
            "\n",
            "reduce_mean_cls_loss =  3.74396634\n",
            "\n",
            "regression loss =  23.5563774\n",
            "\n",
            "reduce_confident_loss =  0.271782666\n",
            "\n",
            "reduce_mean_cls_loss =  3.85623431\n",
            "\n",
            "regression loss =  17.7934\n",
            "\n",
            "reduce_confident_loss =  0.269180059\n",
            "\n",
            "reduce_mean_cls_loss =  3.84781861\n",
            "\n",
            "regression loss =  22.1629\n",
            "\n",
            "reduce_confident_loss =  0.365573794\n",
            "\n",
            "reduce_mean_cls_loss =  1.18466127\n",
            "\n",
            "regression loss =  19.4863071\n",
            "\n",
            "reduce_confident_loss =  0.335438401\n",
            "\n",
            "reduce_mean_cls_loss =  0.259448856\n",
            "\n",
            "regression loss =  13.5249329\n",
            "\n",
            "reduce_confident_loss =  0.2893866\n",
            "\n",
            "reduce_mean_cls_loss =  3.74236727\n",
            "\n",
            "regression loss =  13.5031033\n",
            "\n",
            "reduce_confident_loss =  0.289024532\n",
            "\n",
            "reduce_mean_cls_loss =  0.0234796237\n",
            "\n",
            "regression loss =  24.4047031\n",
            "41/41 [==============================] - 1634s 40s/step - loss: 9.1530 - custom_mse: 197.3290 - recall: 0.5651 - val_loss: 20.0546 - val_custom_mse: 143.0261 - val_recall: 0.5750\n",
            "Epoch 6/10\n",
            "\n",
            "reduce_confident_loss =  0.179983899\n",
            "\n",
            "reduce_mean_cls_loss =  1.20496261\n",
            "\n",
            "regression loss =  13.6994257\n",
            " 1/41 [..............................] - ETA: 23:40 - loss: 15.0844 - custom_mse: 147.1403 - recall: 0.7439\n",
            "reduce_confident_loss =  0.455071926\n",
            "\n",
            "reduce_mean_cls_loss =  0.132902101\n",
            "\n",
            "regression loss =  8.02049065\n",
            " 2/41 [>.............................] - ETA: 19:55 - loss: 11.8464 - custom_mse: 113.6089 - recall: 0.8720\n",
            "reduce_confident_loss =  0.0734589919\n",
            "\n",
            "reduce_mean_cls_loss =  1.1754564\n",
            "\n",
            "regression loss =  10.1366501\n",
            " 3/41 [=>............................] - ETA: 19:16 - loss: 11.6928 - custom_mse: 221.5708 - recall: 0.6987\n",
            "reduce_confident_loss =  0.360366136\n",
            "\n",
            "reduce_mean_cls_loss =  0.740175068\n",
            "\n",
            "regression loss =  8.56508732\n",
            " 4/41 [=>............................] - ETA: 18:40 - loss: 11.1860 - custom_mse: 242.0857 - recall: 0.7460\n",
            "reduce_confident_loss =  0.494097531\n",
            "\n",
            "reduce_mean_cls_loss =  0.500544369\n",
            "\n",
            "regression loss =  10.5722914\n",
            " 5/41 [==>...........................] - ETA: 18:03 - loss: 11.2622 - custom_mse: 119.3187 - recall: 0.7968\n",
            "reduce_confident_loss =  0.265510291\n",
            "\n",
            "reduce_mean_cls_loss =  0.018447144\n",
            "\n",
            "regression loss =  10.7748013\n",
            " 6/41 [===>..........................] - ETA: 17:43 - loss: 11.2283 - custom_mse: 140.6450 - recall: 0.8307Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.293722779\n",
            "\n",
            "reduce_mean_cls_loss =  0.0025917606\n",
            "\n",
            "regression loss =  10.2296839\n",
            " 7/41 [====>.........................] - ETA: 17:59 - loss: 11.1280 - custom_mse: 248.5138 - recall: 0.8549\n",
            "reduce_confident_loss =  0.138537839\n",
            "\n",
            "reduce_mean_cls_loss =  0.101932637\n",
            "\n",
            "regression loss =  10.323329\n",
            " 8/41 [====>.........................] - ETA: 17:29 - loss: 11.0574 - custom_mse: 276.6265 - recall: 0.7518\n",
            "reduce_confident_loss =  0.907560885\n",
            "\n",
            "reduce_mean_cls_loss =  1.09874165\n",
            "\n",
            "regression loss =  8.97163773\n",
            " 9/41 [=====>........................] - ETA: 17:01 - loss: 11.0486 - custom_mse: 34.1348 - recall: 0.6859 \n",
            "reduce_confident_loss =  0.0446381904\n",
            "\n",
            "reduce_mean_cls_loss =  0.0743318424\n",
            "\n",
            "regression loss =  4.57495737\n",
            "10/41 [======>.......................] - ETA: 16:32 - loss: 10.4131 - custom_mse: 316.9398 - recall: 0.6173\n",
            "reduce_confident_loss =  0.0954915658\n",
            "\n",
            "reduce_mean_cls_loss =  0.0107131256\n",
            "\n",
            "regression loss =  2.71146894\n",
            "11/41 [=======>......................] - ETA: 16:02 - loss: 9.7226 - custom_mse: 293.7880 - recall: 0.5612 \n",
            "reduce_confident_loss =  0.0388179496\n",
            "\n",
            "reduce_mean_cls_loss =  0.0125699937\n",
            "\n",
            "regression loss =  3.37013173\n",
            "12/41 [=======>......................] - ETA: 15:31 - loss: 9.1975 - custom_mse: 240.5343 - recall: 0.5144\n",
            "reduce_confident_loss =  0.0572471954\n",
            "\n",
            "reduce_mean_cls_loss =  0.110624112\n",
            "\n",
            "regression loss =  3.71276498\n",
            "13/41 [========>.....................] - ETA: 15:04 - loss: 8.7886 - custom_mse: 168.2180 - recall: 0.4748\n",
            "reduce_confident_loss =  0.0894150063\n",
            "\n",
            "reduce_mean_cls_loss =  0.279188\n",
            "\n",
            "regression loss =  2.17496061\n",
            "14/41 [=========>....................] - ETA: 14:31 - loss: 8.3425 - custom_mse: 115.1721 - recall: 0.4409\n",
            "reduce_confident_loss =  0.0978941321\n",
            "\n",
            "reduce_mean_cls_loss =  0.248999968\n",
            "\n",
            "regression loss =  8.11983681\n",
            "15/41 [=========>....................] - ETA: 14:06 - loss: 8.3508 - custom_mse: 160.7169 - recall: 0.4115\n",
            "reduce_confident_loss =  0.10914205\n",
            "\n",
            "reduce_mean_cls_loss =  0.316918314\n",
            "\n",
            "regression loss =  10.6632767\n",
            "16/41 [==========>...................] - ETA: 13:36 - loss: 8.5219 - custom_mse: 203.1334 - recall: 0.3890\n",
            "reduce_confident_loss =  0.362597615\n",
            "\n",
            "reduce_mean_cls_loss =  0.391990513\n",
            "\n",
            "regression loss =  11.6640797\n",
            "17/41 [===========>..................] - ETA: 13:42 - loss: 8.7511 - custom_mse: 93.1311 - recall: 0.4116 \n",
            "reduce_confident_loss =  0.0443500541\n",
            "\n",
            "reduce_mean_cls_loss =  0.0806962922\n",
            "\n",
            "regression loss =  16.5502243\n",
            "18/41 [============>.................] - ETA: 13:06 - loss: 9.1914 - custom_mse: 70.1206 - recall: 0.4440\n",
            "reduce_confident_loss =  0.121907182\n",
            "\n",
            "reduce_mean_cls_loss =  0.099960424\n",
            "\n",
            "regression loss =  9.8380661\n",
            "19/41 [============>.................] - ETA: 12:30 - loss: 9.2371 - custom_mse: 26.6057 - recall: 0.4719\n",
            "reduce_confident_loss =  0.264658719\n",
            "\n",
            "reduce_mean_cls_loss =  0.375937939\n",
            "\n",
            "regression loss =  10.8957863\n",
            "20/41 [=============>................] - ETA: 11:54 - loss: 9.3521 - custom_mse: 222.9775 - recall: 0.4895\n",
            "reduce_confident_loss =  0.512415588\n",
            "\n",
            "reduce_mean_cls_loss =  1.88074625\n",
            "\n",
            "regression loss =  8.78795433\n",
            "21/41 [==============>...............] - ETA: 11:19 - loss: 9.4392 - custom_mse: 141.7331 - recall: 0.4891\n",
            "reduce_confident_loss =  0.321656287\n",
            "\n",
            "reduce_mean_cls_loss =  0.470644534\n",
            "\n",
            "regression loss =  7.45369959\n",
            "22/41 [===============>..............] - ETA: 10:43 - loss: 9.3849 - custom_mse: 259.4225 - recall: 0.5114\n",
            "reduce_confident_loss =  0.402868211\n",
            "\n",
            "reduce_mean_cls_loss =  0.295674\n",
            "\n",
            "regression loss =  8.63526535\n",
            "23/41 [===============>..............] - ETA: 10:08 - loss: 9.3827 - custom_mse: 154.6987 - recall: 0.5248\n",
            "reduce_confident_loss =  0.509940743\n",
            "\n",
            "reduce_mean_cls_loss =  0.0261107\n",
            "\n",
            "regression loss =  5.2405324\n",
            "24/41 [================>.............] - ETA: 9:34 - loss: 9.2324 - custom_mse: 134.8409 - recall: 0.5446 \n",
            "reduce_confident_loss =  0.34100157\n",
            "\n",
            "reduce_mean_cls_loss =  0.0581637099\n",
            "\n",
            "regression loss =  5.13145924\n",
            "25/41 [=================>............] - ETA: 9:00 - loss: 9.0844 - custom_mse: 164.4204 - recall: 0.5628\n",
            "reduce_confident_loss =  0.303260058\n",
            "\n",
            "reduce_mean_cls_loss =  0.0384046361\n",
            "\n",
            "regression loss =  7.9794\n",
            "26/41 [==================>...........] - ETA: 8:26 - loss: 9.0550 - custom_mse: 187.6331 - recall: 0.5777\n",
            "reduce_confident_loss =  0.391176939\n",
            "\n",
            "reduce_mean_cls_loss =  0.530523896\n",
            "\n",
            "regression loss =  5.40794945\n",
            "27/41 [==================>...........] - ETA: 7:51 - loss: 8.9541 - custom_mse: 152.1319 - recall: 0.5872\n",
            "reduce_confident_loss =  0.139875397\n",
            "\n",
            "reduce_mean_cls_loss =  0.134934545\n",
            "\n",
            "regression loss =  2.950387\n",
            "28/41 [===================>..........] - ETA: 7:17 - loss: 8.7495 - custom_mse: 123.9913 - recall: 0.6019\n",
            "reduce_confident_loss =  0.0995084867\n",
            "\n",
            "reduce_mean_cls_loss =  0.102579698\n",
            "\n",
            "regression loss =  5.81024551\n",
            "29/41 [====================>.........] - ETA: 6:43 - loss: 8.6551 - custom_mse: 108.4837 - recall: 0.6157\n",
            "reduce_confident_loss =  0.214581043\n",
            "\n",
            "reduce_mean_cls_loss =  0.0945258141\n",
            "\n",
            "regression loss =  5.30732393\n",
            "30/41 [====================>.........] - ETA: 6:09 - loss: 8.5538 - custom_mse: 99.0254 - recall: 0.6285 \n",
            "reduce_confident_loss =  0.174060658\n",
            "\n",
            "reduce_mean_cls_loss =  0.165384084\n",
            "\n",
            "regression loss =  4.65505743\n",
            "31/41 [=====================>........] - ETA: 5:35 - loss: 8.4390 - custom_mse: 79.1984 - recall: 0.6404\n",
            "reduce_confident_loss =  0.383153766\n",
            "\n",
            "reduce_mean_cls_loss =  0.388686717\n",
            "\n",
            "regression loss =  5.6855545\n",
            "32/41 [======================>.......] - ETA: 5:02 - loss: 8.3771 - custom_mse: 80.8542 - recall: 0.6451\n",
            "reduce_confident_loss =  1.34687209\n",
            "\n",
            "reduce_mean_cls_loss =  0.841237545\n",
            "\n",
            "regression loss =  7.70024347\n",
            "33/41 [=======================>......] - ETA: 4:30 - loss: 8.4229 - custom_mse: 73.1554 - recall: 0.6497\n",
            "reduce_confident_loss =  0.167211697\n",
            "\n",
            "reduce_mean_cls_loss =  0.209944114\n",
            "\n",
            "regression loss =  5.41891813\n",
            "34/41 [=======================>......] - ETA: 3:56 - loss: 8.3456 - custom_mse: 80.7961 - recall: 0.6600\n",
            "reduce_confident_loss =  0.308978617\n",
            "\n",
            "reduce_mean_cls_loss =  0.878924787\n",
            "\n",
            "regression loss =  7.04995728\n",
            "35/41 [========================>.....] - ETA: 3:23 - loss: 8.3425 - custom_mse: 145.3731 - recall: 0.6568\n",
            "reduce_confident_loss =  0.340833247\n",
            "\n",
            "reduce_mean_cls_loss =  0.917999685\n",
            "\n",
            "regression loss =  10.7999315\n",
            "36/41 [=========================>....] - ETA: 2:50 - loss: 8.4457 - custom_mse: 119.6373 - recall: 0.6480\n",
            "reduce_confident_loss =  0.529364884\n",
            "\n",
            "reduce_mean_cls_loss =  1.78718019\n",
            "\n",
            "regression loss =  5.87792397\n",
            "37/41 [==========================>...] - ETA: 2:15 - loss: 8.4390 - custom_mse: 54.0557 - recall: 0.6305 \n",
            "reduce_confident_loss =  0.902608573\n",
            "\n",
            "reduce_mean_cls_loss =  0.621815383\n",
            "\n",
            "regression loss =  7.63750315\n",
            "38/41 [==========================>...] - ETA: 1:41 - loss: 8.4580 - custom_mse: 59.5337 - recall: 0.6139\n",
            "reduce_confident_loss =  0.35975185\n",
            "\n",
            "reduce_mean_cls_loss =  0.138829783\n",
            "\n",
            "regression loss =  12.5426359\n",
            "39/41 [===========================>..] - ETA: 1:07 - loss: 8.5755 - custom_mse: 122.5137 - recall: 0.5982\n",
            "reduce_confident_loss =  0.32306847\n",
            "\n",
            "reduce_mean_cls_loss =  0.161225766\n",
            "\n",
            "regression loss =  7.95255756\n",
            "40/41 [============================>.] - ETA: 33s - loss: 8.5720 - custom_mse: 87.5120 - recall: 0.5832  indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.0497975051 0.101377964 0.815765321 ... 0.854176819 0.734948277 0.000537127256]\n",
            " [0.175239623 0.0513633788 0.073167026 ... 0.976352453 0.955480456 2.86079394e-05]\n",
            " [0.215497583 0.0462291539 0.454241365 ... 0.99080646 0.657721162 0.000448614359]\n",
            " [0.583148241 0.140683025 0.34242177 ... 0.976133347 0.993258417 0.000680118799]\n",
            " [0.243019849 0.00975418091 0.792972 ... 0.988727808 0.981415272 0.000421822071]]\n",
            "\n",
            "reduce_confident_loss =  0.421960384\n",
            "\n",
            "reduce_mean_cls_loss =  0.220460847\n",
            "\n",
            "regression loss =  6.47467804\n",
            "41/41 [==============================] - ETA: 0s - loss: 8.5608 - custom_mse: 77.6001 - recall: 0.5690 \n",
            "reduce_confident_loss =  0.424562514\n",
            "\n",
            "reduce_mean_cls_loss =  1.3635298\n",
            "\n",
            "regression loss =  16.0374584\n",
            "\n",
            "reduce_confident_loss =  0.450472087\n",
            "\n",
            "reduce_mean_cls_loss =  1.3787452\n",
            "\n",
            "regression loss =  11.0312366\n",
            "\n",
            "reduce_confident_loss =  0.72302407\n",
            "\n",
            "reduce_mean_cls_loss =  3.11255503\n",
            "\n",
            "regression loss =  21.0171642\n",
            "\n",
            "reduce_confident_loss =  0.748025477\n",
            "\n",
            "reduce_mean_cls_loss =  1.72673202\n",
            "\n",
            "regression loss =  18.6860733\n",
            "\n",
            "reduce_confident_loss =  0.724444807\n",
            "\n",
            "reduce_mean_cls_loss =  3.32744217\n",
            "\n",
            "regression loss =  19.2451916\n",
            "\n",
            "reduce_confident_loss =  0.720675766\n",
            "\n",
            "reduce_mean_cls_loss =  0.942881882\n",
            "\n",
            "regression loss =  10.6165867\n",
            "\n",
            "reduce_confident_loss =  0.74949789\n",
            "\n",
            "reduce_mean_cls_loss =  1.72711861\n",
            "\n",
            "regression loss =  11.5752573\n",
            "\n",
            "reduce_confident_loss =  0.707421899\n",
            "\n",
            "reduce_mean_cls_loss =  3.17077374\n",
            "\n",
            "regression loss =  21.5750446\n",
            "\n",
            "reduce_confident_loss =  0.697450936\n",
            "\n",
            "reduce_mean_cls_loss =  3.18247604\n",
            "\n",
            "regression loss =  25.3900509\n",
            "\n",
            "reduce_confident_loss =  0.702412069\n",
            "\n",
            "reduce_mean_cls_loss =  3.24301481\n",
            "\n",
            "regression loss =  27.1433144\n",
            "\n",
            "reduce_confident_loss =  0.685475767\n",
            "\n",
            "reduce_mean_cls_loss =  3.23440766\n",
            "\n",
            "regression loss =  22.3887\n",
            "\n",
            "reduce_confident_loss =  0.698992431\n",
            "\n",
            "reduce_mean_cls_loss =  1.69427621\n",
            "\n",
            "regression loss =  19.9521\n",
            "\n",
            "reduce_confident_loss =  0.496027\n",
            "\n",
            "reduce_mean_cls_loss =  0.414872915\n",
            "\n",
            "regression loss =  13.8207159\n",
            "\n",
            "reduce_confident_loss =  0.705433071\n",
            "\n",
            "reduce_mean_cls_loss =  3.18087053\n",
            "\n",
            "regression loss =  9.16351795\n",
            "\n",
            "reduce_confident_loss =  0.727927446\n",
            "\n",
            "reduce_mean_cls_loss =  0.04157\n",
            "\n",
            "regression loss =  20.7439957\n",
            "41/41 [==============================] - 1564s 38s/step - loss: 8.5608 - custom_mse: 77.6001 - recall: 0.5690 - val_loss: 20.6584 - val_custom_mse: 45.6573 - val_recall: 0.5750\n",
            "Epoch 7/10\n",
            "\n",
            "reduce_confident_loss =  0.169835523\n",
            "\n",
            "reduce_mean_cls_loss =  1.39384663\n",
            "\n",
            "regression loss =  14.4214354\n",
            " 1/41 [..............................] - ETA: 23:42 - loss: 15.9851 - custom_mse: 139.3189 - recall: 0.7787\n",
            "reduce_confident_loss =  0.187486544\n",
            "\n",
            "reduce_mean_cls_loss =  0.193017378\n",
            "\n",
            "regression loss =  8.7329483\n",
            " 2/41 [>.............................] - ETA: 20:09 - loss: 12.5493 - custom_mse: 89.3543 - recall: 0.8893 \n",
            "reduce_confident_loss =  0.221965685\n",
            "\n",
            "reduce_mean_cls_loss =  0.656291425\n",
            "\n",
            "regression loss =  11.3712616\n",
            " 3/41 [=>............................] - ETA: 19:29 - loss: 12.4494 - custom_mse: 151.1074 - recall: 0.7179\n",
            "reduce_confident_loss =  0.254605621\n",
            "\n",
            "reduce_mean_cls_loss =  0.8978706\n",
            "\n",
            "regression loss =  10.1930656\n",
            " 4/41 [=>............................] - ETA: 19:48 - loss: 12.1734 - custom_mse: 198.9741 - recall: 0.7754\n",
            "reduce_confident_loss =  0.299945742\n",
            "\n",
            "reduce_mean_cls_loss =  0.703213751\n",
            "\n",
            "regression loss =  7.05494452\n",
            " 5/41 [==>...........................] - ETA: 19:14 - loss: 11.3503 - custom_mse: 99.2527 - recall: 0.8203 \n",
            "reduce_confident_loss =  0.448476136\n",
            "\n",
            "reduce_mean_cls_loss =  0.0129499389\n",
            "\n",
            "regression loss =  6.83000374\n",
            " 6/41 [===>..........................] - ETA: 18:37 - loss: 10.6739 - custom_mse: 82.1396 - recall: 0.8503Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.273984581\n",
            "\n",
            "reduce_mean_cls_loss =  0.00177741237\n",
            "\n",
            "regression loss =  11.4661064\n",
            " 7/41 [====>.........................] - ETA: 18:47 - loss: 10.8264 - custom_mse: 182.2755 - recall: 0.8717\n",
            "reduce_confident_loss =  0.0809245929\n",
            "\n",
            "reduce_mean_cls_loss =  0.0637710169\n",
            "\n",
            "regression loss =  14.2586288\n",
            " 8/41 [====>.........................] - ETA: 18:10 - loss: 11.2735 - custom_mse: 283.1008 - recall: 0.7745\n",
            "reduce_confident_loss =  0.69593209\n",
            "\n",
            "reduce_mean_cls_loss =  0.98248893\n",
            "\n",
            "regression loss =  9.75561142\n",
            " 9/41 [=====>........................] - ETA: 17:42 - loss: 11.2914 - custom_mse: 37.6634 - recall: 0.7271 \n",
            "reduce_confident_loss =  0.0548497178\n",
            "\n",
            "reduce_mean_cls_loss =  0.0541684292\n",
            "\n",
            "regression loss =  7.60938549\n",
            "10/41 [======>.......................] - ETA: 17:05 - loss: 10.9341 - custom_mse: 280.0673 - recall: 0.6543\n",
            "reduce_confident_loss =  0.0596378185\n",
            "\n",
            "reduce_mean_cls_loss =  0.020828668\n",
            "\n",
            "regression loss =  5.21815825\n",
            "11/41 [=======>......................] - ETA: 16:33 - loss: 10.4218 - custom_mse: 255.8466 - recall: 0.5949\n",
            "reduce_confident_loss =  0.0581294224\n",
            "\n",
            "reduce_mean_cls_loss =  0.0728320256\n",
            "\n",
            "regression loss =  6.66511965\n",
            "12/41 [=======>......................] - ETA: 16:01 - loss: 10.1196 - custom_mse: 177.6431 - recall: 0.5453\n",
            "reduce_confident_loss =  0.117790423\n",
            "\n",
            "reduce_mean_cls_loss =  0.237192944\n",
            "\n",
            "regression loss =  9.18934536\n",
            "13/41 [========>.....................] - ETA: 15:28 - loss: 10.0754 - custom_mse: 139.6671 - recall: 0.5033\n",
            "reduce_confident_loss =  0.175798416\n",
            "\n",
            "reduce_mean_cls_loss =  0.0962243751\n",
            "\n",
            "regression loss =  8.30899525\n",
            "14/41 [=========>....................] - ETA: 14:55 - loss: 9.9686 - custom_mse: 55.4865 - recall: 0.4674  \n",
            "reduce_confident_loss =  0.130564541\n",
            "\n",
            "reduce_mean_cls_loss =  0.527666152\n",
            "\n",
            "regression loss =  7.48368359\n",
            "15/41 [=========>....................] - ETA: 14:20 - loss: 9.8469 - custom_mse: 128.0276 - recall: 0.4362\n",
            "reduce_confident_loss =  0.163325951\n",
            "\n",
            "reduce_mean_cls_loss =  1.09313\n",
            "\n",
            "regression loss =  10.0145674\n",
            "16/41 [==========>...................] - ETA: 13:47 - loss: 9.9359 - custom_mse: 148.0858 - recall: 0.4200\n",
            "reduce_confident_loss =  0.4374\n",
            "\n",
            "reduce_mean_cls_loss =  0.37012291\n",
            "\n",
            "regression loss =  12.1205406\n",
            "17/41 [===========>..................] - ETA: 13:51 - loss: 10.1119 - custom_mse: 60.1508 - recall: 0.4417\n",
            "reduce_confident_loss =  0.143270016\n",
            "\n",
            "reduce_mean_cls_loss =  0.00390127511\n",
            "\n",
            "regression loss =  14.7917099\n",
            "18/41 [============>.................] - ETA: 13:13 - loss: 10.3800 - custom_mse: 46.9041 - recall: 0.4727\n",
            "reduce_confident_loss =  0.137098923\n",
            "\n",
            "reduce_mean_cls_loss =  0.0548642464\n",
            "\n",
            "regression loss =  9.59704685\n",
            "19/41 [============>.................] - ETA: 12:36 - loss: 10.3489 - custom_mse: 37.4332 - recall: 0.5004\n",
            "reduce_confident_loss =  0.398599088\n",
            "\n",
            "reduce_mean_cls_loss =  0.103562631\n",
            "\n",
            "regression loss =  12.6509256\n",
            "20/41 [=============>................] - ETA: 12:05 - loss: 10.4891 - custom_mse: 155.1099 - recall: 0.5211\n",
            "reduce_confident_loss =  0.417102188\n",
            "\n",
            "reduce_mean_cls_loss =  0.688662827\n",
            "\n",
            "regression loss =  9.33160496\n",
            "21/41 [==============>...............] - ETA: 11:29 - loss: 10.4867 - custom_mse: 148.8453 - recall: 0.5310\n",
            "reduce_confident_loss =  0.159944743\n",
            "\n",
            "reduce_mean_cls_loss =  0.264946133\n",
            "\n",
            "regression loss =  8.76110458\n",
            "22/41 [===============>..............] - ETA: 10:58 - loss: 10.4276 - custom_mse: 267.7067 - recall: 0.5520\n",
            "reduce_confident_loss =  0.112345897\n",
            "\n",
            "reduce_mean_cls_loss =  0.405109733\n",
            "\n",
            "regression loss =  10.1009903\n",
            "23/41 [===============>..............] - ETA: 10:22 - loss: 10.4359 - custom_mse: 160.7595 - recall: 0.5610\n",
            "reduce_confident_loss =  0.539644659\n",
            "\n",
            "reduce_mean_cls_loss =  0.00177977071\n",
            "\n",
            "regression loss =  7.57987\n",
            "24/41 [================>.............] - ETA: 9:46 - loss: 10.3394 - custom_mse: 136.8775 - recall: 0.5793 \n",
            "reduce_confident_loss =  0.391409814\n",
            "\n",
            "reduce_mean_cls_loss =  0.00045691157\n",
            "\n",
            "regression loss =  7.48139811\n",
            "25/41 [=================>............] - ETA: 9:10 - loss: 10.2408 - custom_mse: 202.1840 - recall: 0.5961\n",
            "reduce_confident_loss =  0.225509644\n",
            "\n",
            "reduce_mean_cls_loss =  0.00150915165\n",
            "\n",
            "regression loss =  10.1884613\n",
            "26/41 [==================>...........] - ETA: 8:35 - loss: 10.2475 - custom_mse: 253.1184 - recall: 0.6113\n",
            "reduce_confident_loss =  0.136916563\n",
            "\n",
            "reduce_mean_cls_loss =  0.136023954\n",
            "\n",
            "regression loss =  6.60435867\n",
            "27/41 [==================>...........] - ETA: 7:59 - loss: 10.1227 - custom_mse: 165.4629 - recall: 0.6257\n",
            "reduce_confident_loss =  0.142030478\n",
            "\n",
            "reduce_mean_cls_loss =  0.0113592939\n",
            "\n",
            "regression loss =  3.29884338\n",
            "28/41 [===================>..........] - ETA: 7:24 - loss: 9.8844 - custom_mse: 107.6139 - recall: 0.6391 \n",
            "reduce_confident_loss =  0.143309101\n",
            "\n",
            "reduce_mean_cls_loss =  0.0235758331\n",
            "\n",
            "regression loss =  5.69819832\n",
            "29/41 [====================>.........] - ETA: 6:49 - loss: 9.7458 - custom_mse: 96.4942 - recall: 0.6515 \n",
            "reduce_confident_loss =  0.239419967\n",
            "\n",
            "reduce_mean_cls_loss =  0.111201867\n",
            "\n",
            "regression loss =  3.45586038\n",
            "30/41 [====================>.........] - ETA: 6:15 - loss: 9.5479 - custom_mse: 95.2800 - recall: 0.6631\n",
            "reduce_confident_loss =  0.151541\n",
            "\n",
            "reduce_mean_cls_loss =  0.399605811\n",
            "\n",
            "regression loss =  2.95817804\n",
            "31/41 [=====================>........] - ETA: 5:41 - loss: 9.3531 - custom_mse: 84.8100 - recall: 0.6740\n",
            "reduce_confident_loss =  0.231950626\n",
            "\n",
            "reduce_mean_cls_loss =  0.588308334\n",
            "\n",
            "regression loss =  4.77598143\n",
            "32/41 [======================>.......] - ETA: 5:06 - loss: 9.2357 - custom_mse: 89.3054 - recall: 0.6808\n",
            "reduce_confident_loss =  0.955523789\n",
            "\n",
            "reduce_mean_cls_loss =  0.605121195\n",
            "\n",
            "regression loss =  7.38517237\n",
            "33/41 [=======================>......] - ETA: 4:32 - loss: 9.2269 - custom_mse: 69.2453 - recall: 0.6839\n",
            "reduce_confident_loss =  0.140771389\n",
            "\n",
            "reduce_mean_cls_loss =  0.721100509\n",
            "\n",
            "regression loss =  4.53626204\n",
            "34/41 [=======================>......] - ETA: 3:57 - loss: 9.1143 - custom_mse: 79.0392 - recall: 0.6924\n",
            "reduce_confident_loss =  0.348263025\n",
            "\n",
            "reduce_mean_cls_loss =  0.121948294\n",
            "\n",
            "regression loss =  6.06395102\n",
            "35/41 [========================>.....] - ETA: 3:24 - loss: 9.0406 - custom_mse: 159.3109 - recall: 0.6963\n",
            "reduce_confident_loss =  0.186172009\n",
            "\n",
            "reduce_mean_cls_loss =  0.52493906\n",
            "\n",
            "regression loss =  9.09187126\n",
            "36/41 [=========================>....] - ETA: 2:50 - loss: 9.0617 - custom_mse: 175.1378 - recall: 0.6980\n",
            "reduce_confident_loss =  0.078449145\n",
            "\n",
            "reduce_mean_cls_loss =  0.820676804\n",
            "\n",
            "regression loss =  6.35529137\n",
            "37/41 [==========================>...] - ETA: 2:16 - loss: 9.0129 - custom_mse: 85.2762 - recall: 0.6881 \n",
            "reduce_confident_loss =  0.176337644\n",
            "\n",
            "reduce_mean_cls_loss =  0.373336971\n",
            "\n",
            "regression loss =  9.70034885\n",
            "38/41 [==========================>...] - ETA: 1:42 - loss: 9.0454 - custom_mse: 109.2394 - recall: 0.6700\n",
            "reduce_confident_loss =  0.228249565\n",
            "\n",
            "reduce_mean_cls_loss =  0.254028559\n",
            "\n",
            "regression loss =  10.527873\n",
            "39/41 [===========================>..] - ETA: 1:08 - loss: 9.0958 - custom_mse: 193.1159 - recall: 0.6528\n",
            "reduce_confident_loss =  0.165134117\n",
            "\n",
            "reduce_mean_cls_loss =  0.139238819\n",
            "\n",
            "regression loss =  5.79741859\n",
            "40/41 [============================>.] - ETA: 34s - loss: 9.0210 - custom_mse: 134.0420 - recall: 0.6365 indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 2\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.112862915 0.00151580572 0.341936707 ... 0.999741852 0.984409034 0.000682830811]\n",
            " [0.464578748 0.0562488139 0.145718336 ... 0.983932614 0.875852823 0.0415848494]\n",
            " [0.00837761164 0.0193245411 0.00266376138 ... 0.983980417 0.9681499 0.000121407102]\n",
            " [0.718027592 0.29902631 0.0443768501 ... 0.998508215 0.912380695 0.00218716264]\n",
            " [0.572724938 0.139419287 0.0523664653 ... 0.999488592 0.851735651 2.44133971e-05]]\n",
            "\n",
            "reduce_confident_loss =  0.121675618\n",
            "\n",
            "reduce_mean_cls_loss =  0.437715828\n",
            "\n",
            "regression loss =  4.59081602\n",
            "41/41 [==============================] - ETA: 0s - loss: 8.9910 - custom_mse: 110.6827 - recall: 0.6209 \n",
            "reduce_confident_loss =  0.307055682\n",
            "\n",
            "reduce_mean_cls_loss =  1.39609933\n",
            "\n",
            "regression loss =  13.7944803\n",
            "\n",
            "reduce_confident_loss =  0.302158326\n",
            "\n",
            "reduce_mean_cls_loss =  1.35691071\n",
            "\n",
            "regression loss =  9.81626225\n",
            "\n",
            "reduce_confident_loss =  0.625898898\n",
            "\n",
            "reduce_mean_cls_loss =  2.70986056\n",
            "\n",
            "regression loss =  18.7493248\n",
            "\n",
            "reduce_confident_loss =  0.65294826\n",
            "\n",
            "reduce_mean_cls_loss =  1.78763449\n",
            "\n",
            "regression loss =  15.4332008\n",
            "\n",
            "reduce_confident_loss =  0.64959228\n",
            "\n",
            "reduce_mean_cls_loss =  2.90374207\n",
            "\n",
            "regression loss =  20.6383076\n",
            "\n",
            "reduce_confident_loss =  0.660387337\n",
            "\n",
            "reduce_mean_cls_loss =  0.828678608\n",
            "\n",
            "regression loss =  12.1188536\n",
            "\n",
            "reduce_confident_loss =  0.696861684\n",
            "\n",
            "reduce_mean_cls_loss =  1.79428613\n",
            "\n",
            "regression loss =  13.9975042\n",
            "\n",
            "reduce_confident_loss =  0.664227366\n",
            "\n",
            "reduce_mean_cls_loss =  2.72920871\n",
            "\n",
            "regression loss =  21.1187229\n",
            "\n",
            "reduce_confident_loss =  0.658092856\n",
            "\n",
            "reduce_mean_cls_loss =  2.75863862\n",
            "\n",
            "regression loss =  16.3145218\n",
            "\n",
            "reduce_confident_loss =  0.638061\n",
            "\n",
            "reduce_mean_cls_loss =  2.83974504\n",
            "\n",
            "regression loss =  17.2714272\n",
            "\n",
            "reduce_confident_loss =  0.602814138\n",
            "\n",
            "reduce_mean_cls_loss =  2.77108884\n",
            "\n",
            "regression loss =  17.8711166\n",
            "\n",
            "reduce_confident_loss =  0.599432826\n",
            "\n",
            "reduce_mean_cls_loss =  1.74565208\n",
            "\n",
            "regression loss =  17.9717712\n",
            "\n",
            "reduce_confident_loss =  0.381272316\n",
            "\n",
            "reduce_mean_cls_loss =  0.350638777\n",
            "\n",
            "regression loss =  12.8907537\n",
            "\n",
            "reduce_confident_loss =  0.611913681\n",
            "\n",
            "reduce_mean_cls_loss =  2.74851871\n",
            "\n",
            "regression loss =  10.7868853\n",
            "\n",
            "reduce_confident_loss =  0.630991817\n",
            "\n",
            "reduce_mean_cls_loss =  0.0663120225\n",
            "\n",
            "regression loss =  21.5927505\n",
            "41/41 [==============================] - 1584s 39s/step - loss: 8.9910 - custom_mse: 110.6827 - recall: 0.6209 - val_loss: 18.4584 - val_custom_mse: 95.2628 - val_recall: 0.5750\n",
            "Epoch 8/10\n",
            "\n",
            "reduce_confident_loss =  0.156560466\n",
            "\n",
            "reduce_mean_cls_loss =  1.11904275\n",
            "\n",
            "regression loss =  13.6063566\n",
            " 1/41 [..............................] - ETA: 22:23 - loss: 14.8820 - custom_mse: 155.1891 - recall: 0.4641\n",
            "reduce_confident_loss =  0.163165122\n",
            "\n",
            "reduce_mean_cls_loss =  0.2084914\n",
            "\n",
            "regression loss =  7.91409826\n",
            " 2/41 [>.............................] - ETA: 19:44 - loss: 11.5839 - custom_mse: 81.8705 - recall: 0.7321 \n",
            "reduce_confident_loss =  0.251274586\n",
            "\n",
            "reduce_mean_cls_loss =  0.276803941\n",
            "\n",
            "regression loss =  8.24863815\n",
            " 3/41 [=>............................] - ETA: 19:05 - loss: 10.6481 - custom_mse: 172.2911 - recall: 0.5794\n",
            "reduce_confident_loss =  0.306552947\n",
            "\n",
            "reduce_mean_cls_loss =  0.757322371\n",
            "\n",
            "regression loss =  7.37356949\n",
            " 4/41 [=>............................] - ETA: 18:30 - loss: 10.0955 - custom_mse: 231.3517 - recall: 0.6587\n",
            "reduce_confident_loss =  0.314082295\n",
            "\n",
            "reduce_mean_cls_loss =  0.541819215\n",
            "\n",
            "regression loss =  8.00263691\n",
            " 5/41 [==>...........................] - ETA: 18:21 - loss: 9.8481 - custom_mse: 107.8448 - recall: 0.7270 \n",
            "reduce_confident_loss =  0.517177\n",
            "\n",
            "reduce_mean_cls_loss =  0.0617431477\n",
            "\n",
            "regression loss =  7.27157736\n",
            " 6/41 [===>..........................] - ETA: 18:05 - loss: 9.5152 - custom_mse: 100.5914 - recall: 0.7725Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.309028894\n",
            "\n",
            "reduce_mean_cls_loss =  0.00410268549\n",
            "\n",
            "regression loss =  8.79394722\n",
            " 7/41 [====>.........................] - ETA: 18:27 - loss: 9.4569 - custom_mse: 201.7270 - recall: 0.8050\n",
            "reduce_confident_loss =  0.107568279\n",
            "\n",
            "reduce_mean_cls_loss =  0.0333048515\n",
            "\n",
            "regression loss =  7.63358307\n",
            " 8/41 [====>.........................] - ETA: 17:56 - loss: 9.2466 - custom_mse: 307.5608 - recall: 0.7134\n",
            "reduce_confident_loss =  0.594512343\n",
            "\n",
            "reduce_mean_cls_loss =  1.13246739\n",
            "\n",
            "regression loss =  7.47560263\n",
            " 9/41 [=====>........................] - ETA: 17:24 - loss: 9.2417 - custom_mse: 64.3353 - recall: 0.6548 \n",
            "reduce_confident_loss =  0.18620804\n",
            "\n",
            "reduce_mean_cls_loss =  0.00351662934\n",
            "\n",
            "regression loss =  3.57890511\n",
            "10/41 [======>.......................] - ETA: 16:50 - loss: 8.6944 - custom_mse: 301.1040 - recall: 0.5893\n",
            "reduce_confident_loss =  0.138386175\n",
            "\n",
            "reduce_mean_cls_loss =  0.0057174745\n",
            "\n",
            "regression loss =  3.97748709\n",
            "11/41 [=======>......................] - ETA: 16:18 - loss: 8.2787 - custom_mse: 269.9636 - recall: 0.5357\n",
            "reduce_confident_loss =  0.0602390356\n",
            "\n",
            "reduce_mean_cls_loss =  0.00787605625\n",
            "\n",
            "regression loss =  3.54383373\n",
            "12/41 [=======>......................] - ETA: 16:01 - loss: 7.8898 - custom_mse: 224.4764 - recall: 0.4911\n",
            "reduce_confident_loss =  0.0369939208\n",
            "\n",
            "reduce_mean_cls_loss =  0.026687583\n",
            "\n",
            "regression loss =  1.91577828\n",
            "13/41 [========>.....................] - ETA: 15:28 - loss: 7.4351 - custom_mse: 154.6463 - recall: 0.4533\n",
            "reduce_confident_loss =  0.106939383\n",
            "\n",
            "reduce_mean_cls_loss =  0.144542009\n",
            "\n",
            "regression loss =  3.0467658\n",
            "14/41 [=========>....................] - ETA: 15:06 - loss: 7.1396 - custom_mse: 99.9579 - recall: 0.4209 \n",
            "reduce_confident_loss =  0.0854613408\n",
            "\n",
            "reduce_mean_cls_loss =  0.282405585\n",
            "\n",
            "regression loss =  6.74456787\n",
            "15/41 [=========>....................] - ETA: 14:32 - loss: 7.1378 - custom_mse: 149.8983 - recall: 0.3929\n",
            "reduce_confident_loss =  0.129586145\n",
            "\n",
            "reduce_mean_cls_loss =  0.322315425\n",
            "\n",
            "regression loss =  9.92183\n",
            "16/41 [==========>...................] - ETA: 13:58 - loss: 7.3401 - custom_mse: 196.5967 - recall: 0.3800\n",
            "reduce_confident_loss =  0.352262825\n",
            "\n",
            "reduce_mean_cls_loss =  0.438779205\n",
            "\n",
            "regression loss =  12.0523987\n",
            "17/41 [===========>..................] - ETA: 14:01 - loss: 7.6638 - custom_mse: 82.8029 - recall: 0.4096 \n",
            "reduce_confident_loss =  0.0332832076\n",
            "\n",
            "reduce_mean_cls_loss =  0.0249250736\n",
            "\n",
            "regression loss =  15.3327084\n",
            "18/41 [============>.................] - ETA: 13:23 - loss: 8.0931 - custom_mse: 48.4228 - recall: 0.4422\n",
            "reduce_confident_loss =  0.194158822\n",
            "\n",
            "reduce_mean_cls_loss =  0.0297036264\n",
            "\n",
            "regression loss =  8.77172565\n",
            "19/41 [============>.................] - ETA: 12:45 - loss: 8.1406 - custom_mse: 26.8003 - recall: 0.4716\n",
            "reduce_confident_loss =  0.409099042\n",
            "\n",
            "reduce_mean_cls_loss =  0.119938053\n",
            "\n",
            "regression loss =  11.1147032\n",
            "20/41 [=============>................] - ETA: 12:09 - loss: 8.3157 - custom_mse: 175.9783 - recall: 0.4913\n",
            "reduce_confident_loss =  0.500689209\n",
            "\n",
            "reduce_mean_cls_loss =  0.987056732\n",
            "\n",
            "regression loss =  8.76461411\n",
            "21/41 [==============>...............] - ETA: 11:33 - loss: 8.4080 - custom_mse: 133.1449 - recall: 0.4964\n",
            "reduce_confident_loss =  0.250839889\n",
            "\n",
            "reduce_mean_cls_loss =  0.2519705\n",
            "\n",
            "regression loss =  6.6292634\n",
            "22/41 [===============>..............] - ETA: 10:57 - loss: 8.3500 - custom_mse: 296.8129 - recall: 0.5185\n",
            "reduce_confident_loss =  0.124794364\n",
            "\n",
            "reduce_mean_cls_loss =  0.363706172\n",
            "\n",
            "regression loss =  9.30395412\n",
            "23/41 [===============>..............] - ETA: 10:22 - loss: 8.4127 - custom_mse: 170.3066 - recall: 0.5303\n",
            "reduce_confident_loss =  0.309484899\n",
            "\n",
            "reduce_mean_cls_loss =  0.00194737338\n",
            "\n",
            "regression loss =  4.42623\n",
            "24/41 [================>.............] - ETA: 9:46 - loss: 8.2596 - custom_mse: 201.6475 - recall: 0.5498 \n",
            "reduce_confident_loss =  0.189586192\n",
            "\n",
            "reduce_mean_cls_loss =  0.00199252786\n",
            "\n",
            "regression loss =  3.16984701\n",
            "25/41 [=================>............] - ETA: 9:10 - loss: 8.0636 - custom_mse: 246.6619 - recall: 0.5678\n",
            "reduce_confident_loss =  0.291487902\n",
            "\n",
            "reduce_mean_cls_loss =  0.00371917873\n",
            "\n",
            "regression loss =  7.67614937\n",
            "26/41 [==================>...........] - ETA: 8:35 - loss: 8.0601 - custom_mse: 210.1138 - recall: 0.5831\n",
            "reduce_confident_loss =  0.179368943\n",
            "\n",
            "reduce_mean_cls_loss =  0.0836954266\n",
            "\n",
            "regression loss =  4.61494493\n",
            "27/41 [==================>...........] - ETA: 7:59 - loss: 7.9422 - custom_mse: 189.6160 - recall: 0.5976\n",
            "reduce_confident_loss =  0.0894956961\n",
            "\n",
            "reduce_mean_cls_loss =  0.0929732323\n",
            "\n",
            "regression loss =  2.24537492\n",
            "28/41 [===================>..........] - ETA: 7:25 - loss: 7.7453 - custom_mse: 132.7239 - recall: 0.6102\n",
            "reduce_confident_loss =  0.173866034\n",
            "\n",
            "reduce_mean_cls_loss =  0.0199935529\n",
            "\n",
            "regression loss =  4.23333597\n",
            "29/41 [====================>.........] - ETA: 6:53 - loss: 7.6309 - custom_mse: 103.3688 - recall: 0.6237\n",
            "reduce_confident_loss =  0.354281366\n",
            "\n",
            "reduce_mean_cls_loss =  0.0811038688\n",
            "\n",
            "regression loss =  3.12113357\n",
            "30/41 [====================>.........] - ETA: 6:18 - loss: 7.4951 - custom_mse: 100.7213 - recall: 0.6362\n",
            "reduce_confident_loss =  0.323650509\n",
            "\n",
            "reduce_mean_cls_loss =  0.055399511\n",
            "\n",
            "regression loss =  2.93801141\n",
            "31/41 [=====================>........] - ETA: 5:43 - loss: 7.3603 - custom_mse: 79.7059 - recall: 0.6479 \n",
            "reduce_confident_loss =  0.251194\n",
            "\n",
            "reduce_mean_cls_loss =  0.513807118\n",
            "\n",
            "regression loss =  4.50502586\n",
            "32/41 [======================>.......] - ETA: 5:08 - loss: 7.2950 - custom_mse: 92.1778 - recall: 0.6537\n",
            "reduce_confident_loss =  0.700915694\n",
            "\n",
            "reduce_mean_cls_loss =  0.52705133\n",
            "\n",
            "regression loss =  7.04684305\n",
            "33/41 [=======================>......] - ETA: 4:34 - loss: 7.3247 - custom_mse: 85.9790 - recall: 0.6574\n",
            "reduce_confident_loss =  0.208569795\n",
            "\n",
            "reduce_mean_cls_loss =  0.448562771\n",
            "\n",
            "regression loss =  4.94151545\n",
            "34/41 [=======================>......] - ETA: 3:59 - loss: 7.2739 - custom_mse: 84.9413 - recall: 0.6662\n",
            "reduce_confident_loss =  0.410994858\n",
            "\n",
            "reduce_mean_cls_loss =  0.158491507\n",
            "\n",
            "regression loss =  5.52705479\n",
            "35/41 [========================>.....] - ETA: 3:26 - loss: 7.2403 - custom_mse: 166.6986 - recall: 0.6737\n",
            "reduce_confident_loss =  0.16884616\n",
            "\n",
            "reduce_mean_cls_loss =  0.363723546\n",
            "\n",
            "regression loss =  9.67476654\n",
            "36/41 [=========================>....] - ETA: 2:52 - loss: 7.3227 - custom_mse: 180.3139 - recall: 0.6700\n",
            "reduce_confident_loss =  0.0758171901\n",
            "\n",
            "reduce_mean_cls_loss =  0.354033053\n",
            "\n",
            "regression loss =  7.47217178\n",
            "37/41 [==========================>...] - ETA: 2:17 - loss: 7.3383 - custom_mse: 82.0591 - recall: 0.6712 \n",
            "reduce_confident_loss =  0.158523276\n",
            "\n",
            "reduce_mean_cls_loss =  0.440357834\n",
            "\n",
            "regression loss =  9.04277706\n",
            "38/41 [==========================>...] - ETA: 1:43 - loss: 7.3989 - custom_mse: 137.7552 - recall: 0.6535\n",
            "reduce_confident_loss =  0.166802958\n",
            "\n",
            "reduce_mean_cls_loss =  0.0466076918\n",
            "\n",
            "regression loss =  10.015893\n",
            "39/41 [===========================>..] - ETA: 1:08 - loss: 7.4715 - custom_mse: 173.5079 - recall: 0.6368\n",
            "reduce_confident_loss =  0.185411528\n",
            "\n",
            "reduce_mean_cls_loss =  0.269874543\n",
            "\n",
            "regression loss =  5.31197929\n",
            "40/41 [============================>.] - ETA: 34s - loss: 7.4289 - custom_mse: 126.5457 - recall: 0.6209 indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 2\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 5\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true class = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred class = \n",
            " [[0.278535634 0.0753312707 0.0877720118 ... 0.980417967 0.733453274 0.000789701939]\n",
            " [0.0105821788 0.0255333185 0.00055295229 ... 0.99953258 0.711757958 0.0630943477]\n",
            " [0.0190436244 0.173238128 0.129714757 ... 0.968453884 0.282583714 0.0228818655]\n",
            " [0.16984871 0.0479621291 0.0322330594 ... 0.999885917 0.976797223 0.0337858796]\n",
            " [0.0508724451 0.882956743 0.0234586298 ... 0.997048259 0.853548288 0.00267824531]]\n",
            "\n",
            "reduce_confident_loss =  0.254546046\n",
            "\n",
            "reduce_mean_cls_loss =  0.328623831\n",
            "\n",
            "regression loss =  3.64253163\n",
            "41/41 [==============================] - ETA: 0s - loss: 7.4041 - custom_mse: 97.3768 - recall: 0.6057  \n",
            "reduce_confident_loss =  0.199800968\n",
            "\n",
            "reduce_mean_cls_loss =  1.58185\n",
            "\n",
            "regression loss =  12.9172325\n",
            "\n",
            "reduce_confident_loss =  0.218114406\n",
            "\n",
            "reduce_mean_cls_loss =  1.44136488\n",
            "\n",
            "regression loss =  9.47888851\n",
            "\n",
            "reduce_confident_loss =  0.469351977\n",
            "\n",
            "reduce_mean_cls_loss =  2.97621202\n",
            "\n",
            "regression loss =  18.1979122\n",
            "\n",
            "reduce_confident_loss =  0.537224352\n",
            "\n",
            "reduce_mean_cls_loss =  1.65352798\n",
            "\n",
            "regression loss =  14.9201031\n",
            "\n",
            "reduce_confident_loss =  0.534558415\n",
            "\n",
            "reduce_mean_cls_loss =  2.6734755\n",
            "\n",
            "regression loss =  19.3333378\n",
            "\n",
            "reduce_confident_loss =  0.559632659\n",
            "\n",
            "reduce_mean_cls_loss =  0.890316546\n",
            "\n",
            "regression loss =  11.837512\n",
            "\n",
            "reduce_confident_loss =  0.598135471\n",
            "\n",
            "reduce_mean_cls_loss =  1.68512368\n",
            "\n",
            "regression loss =  14.4390211\n",
            "\n",
            "reduce_confident_loss =  0.560975671\n",
            "\n",
            "reduce_mean_cls_loss =  2.93302488\n",
            "\n",
            "regression loss =  17.7825909\n",
            "\n",
            "reduce_confident_loss =  0.561207294\n",
            "\n",
            "reduce_mean_cls_loss =  2.94015765\n",
            "\n",
            "regression loss =  15.4396734\n",
            "\n",
            "reduce_confident_loss =  0.557155967\n",
            "\n",
            "reduce_mean_cls_loss =  3.05215192\n",
            "\n",
            "regression loss =  16.5097427\n",
            "\n",
            "reduce_confident_loss =  0.424840242\n",
            "\n",
            "reduce_mean_cls_loss =  2.69183731\n",
            "\n",
            "regression loss =  17.0240173\n",
            "\n",
            "reduce_confident_loss =  0.528169751\n",
            "\n",
            "reduce_mean_cls_loss =  1.65056527\n",
            "\n",
            "regression loss =  17.7635\n",
            "\n",
            "reduce_confident_loss =  0.333421\n",
            "\n",
            "reduce_mean_cls_loss =  0.316878259\n",
            "\n",
            "regression loss =  12.7954664\n",
            "\n",
            "reduce_confident_loss =  0.524150312\n",
            "\n",
            "reduce_mean_cls_loss =  2.66392303\n",
            "\n",
            "regression loss =  11.2711401\n",
            "\n",
            "reduce_confident_loss =  0.529833198\n",
            "\n",
            "reduce_mean_cls_loss =  0.0739192069\n",
            "\n",
            "regression loss =  21.4243946\n",
            "41/41 [==============================] - 1587s 39s/step - loss: 7.4041 - custom_mse: 97.3768 - recall: 0.6057 - val_loss: 17.7619 - val_custom_mse: 98.3652 - val_recall: 0.5750\n",
            "Epoch 9/10\n",
            "\n",
            "reduce_confident_loss =  0.147288591\n",
            "\n",
            "reduce_mean_cls_loss =  1.3613714\n",
            "\n",
            "regression loss =  12.8084278\n",
            " 1/41 [..............................] - ETA: 23:38 - loss: 14.3171 - custom_mse: 143.3924 - recall: 0.3317\n",
            "reduce_confident_loss =  0.126186639\n",
            "\n",
            "reduce_mean_cls_loss =  0.251643479\n",
            "\n",
            "regression loss =  8.79251671\n",
            " 2/41 [>.............................] - ETA: 20:18 - loss: 11.7437 - custom_mse: 76.2780 - recall: 0.6364 \n",
            "reduce_confident_loss =  0.116610728\n",
            "\n",
            "reduce_mean_cls_loss =  0.216098651\n",
            "\n",
            "regression loss =  8.69384289\n",
            " 3/41 [=>............................] - ETA: 19:38 - loss: 10.8380 - custom_mse: 202.6590 - recall: 0.5062\n",
            "reduce_confident_loss =  0.278208047\n",
            "\n",
            "reduce_mean_cls_loss =  0.424675316\n",
            "\n",
            "regression loss =  7.86486\n",
            " 4/41 [=>............................] - ETA: 19:02 - loss: 10.2704 - custom_mse: 232.7270 - recall: 0.6022\n",
            "reduce_confident_loss =  0.320710897\n",
            "\n",
            "reduce_mean_cls_loss =  0.568353295\n",
            "\n",
            "regression loss =  8.07375336\n",
            " 5/41 [==>...........................] - ETA: 18:29 - loss: 10.0089 - custom_mse: 121.5098 - recall: 0.6817\n",
            "reduce_confident_loss =  0.428245068\n",
            "\n",
            "reduce_mean_cls_loss =  0.0248891674\n",
            "\n",
            "regression loss =  5.48620176\n",
            " 6/41 [===>..........................] - ETA: 18:17 - loss: 9.3306 - custom_mse: 93.6947 - recall: 0.7348  Can not convert <class 'str'> to int\n",
            "056406 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056406 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056408 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056408 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056410 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056410 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056412 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056412 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056414 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056414 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056416 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056416 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056418 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056418 (1).jpg\n",
            "Can not convert <class 'str'> to int\n",
            "056422 (1)     /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town04/rgb_image/front_image/round4/round4_056422 (1).jpg\n",
            "\n",
            "reduce_confident_loss =  0.213634282\n",
            "\n",
            "reduce_mean_cls_loss =  0.0140062077\n",
            "\n",
            "regression loss =  6.56041574\n",
            " 7/41 [====>.........................] - ETA: 18:39 - loss: 8.9674 - custom_mse: 232.1521 - recall: 0.7727\n",
            "reduce_confident_loss =  0.0421252847\n",
            "\n",
            "reduce_mean_cls_loss =  0.0476430655\n",
            "\n",
            "regression loss =  8.47216606\n",
            " 8/41 [====>.........................] - ETA: 18:11 - loss: 8.9167 - custom_mse: 302.4171 - recall: 0.6825\n",
            "reduce_confident_loss =  0.517739713\n",
            "\n",
            "reduce_mean_cls_loss =  1.45696342\n",
            "\n",
            "regression loss =  7.29843712\n",
            " 9/41 [=====>........................] - ETA: 17:41 - loss: 8.9563 - custom_mse: 73.6900 - recall: 0.6390 \n",
            "reduce_confident_loss =  0.0940532163\n",
            "\n",
            "reduce_mean_cls_loss =  0.00541942148\n",
            "\n",
            "regression loss =  2.59743166\n",
            "10/41 [======>.......................] - ETA: 17:08 - loss: 8.3304 - custom_mse: 305.7885 - recall: 0.5751\n",
            "reduce_confident_loss =  0.0881938264\n",
            "\n",
            "reduce_mean_cls_loss =  0.0490342751\n",
            "\n",
            "regression loss =  3.4736011\n",
            "11/41 [=======>......................] - ETA: 16:35 - loss: 7.9013 - custom_mse: 270.3584 - recall: 0.5228\n",
            "reduce_confident_loss =  0.0929379538\n",
            "\n",
            "reduce_mean_cls_loss =  0.0151776718\n",
            "\n",
            "regression loss =  3.16667843\n",
            "12/41 [=======>......................] - ETA: 16:16 - loss: 7.5158 - custom_mse: 206.2817 - recall: 0.4793\n",
            "reduce_confident_loss =  0.0258049332\n",
            "\n",
            "reduce_mean_cls_loss =  0.0231123883\n",
            "\n",
            "regression loss =  2.31668353\n",
            "13/41 [========>.....................] - ETA: 15:42 - loss: 7.1196 - custom_mse: 167.0790 - recall: 0.4424\n",
            "reduce_confident_loss =  0.0695784912\n",
            "\n",
            "reduce_mean_cls_loss =  0.0591695644\n",
            "\n",
            "regression loss =  2.76257443\n",
            "14/41 [=========>....................] - ETA: 15:07 - loss: 6.8176 - custom_mse: 116.8728 - recall: 0.4108\n",
            "reduce_confident_loss =  0.0970943645\n",
            "\n",
            "reduce_mean_cls_loss =  0.30640915\n",
            "\n",
            "regression loss =  6.40947\n",
            "15/41 [=========>....................] - ETA: 14:35 - loss: 6.8173 - custom_mse: 137.6938 - recall: 0.3834\n",
            "reduce_confident_loss =  0.172407255\n",
            "\n",
            "reduce_mean_cls_loss =  0.528825402\n",
            "\n",
            "regression loss =  9.16239834\n",
            "16/41 [==========>...................] - ETA: 14:03 - loss: 7.0077 - custom_mse: 167.9514 - recall: 0.3741\n",
            "reduce_confident_loss =  0.373781532\n",
            "\n",
            "reduce_mean_cls_loss =  0.216086656\n",
            "\n",
            "regression loss =  10.5879965\n",
            "17/41 [===========>..................] - ETA: 14:08 - loss: 7.2530 - custom_mse: 76.5790 - recall: 0.4109 \n",
            "reduce_confident_loss =  0.0468290076\n",
            "\n",
            "reduce_mean_cls_loss =  0.0115970802\n",
            "\n",
            "regression loss =  16.0328178\n",
            "18/41 [============>.................] - ETA: 13:30 - loss: 7.7440 - custom_mse: 59.3646 - recall: 0.4437\n",
            "reduce_confident_loss =  0.207239628\n",
            "\n",
            "reduce_mean_cls_loss =  0.0694262609\n",
            "\n",
            "regression loss =  8.02165794\n",
            "19/41 [============>.................] - ETA: 12:53 - loss: 7.7732 - custom_mse: 26.0031 - recall: 0.4716\n",
            "reduce_confident_loss =  0.369253784\n",
            "\n",
            "reduce_mean_cls_loss =  0.297363698\n",
            "\n",
            "regression loss =  10.9055462\n",
            "20/41 [=============>................] - ETA: 12:16 - loss: 7.9631 - custom_mse: 188.0469 - recall: 0.4894\n",
            "reduce_confident_loss =  0.491136402\n",
            "\n",
            "reduce_mean_cls_loss =  1.23809087\n",
            "\n",
            "regression loss =  8.38331509\n",
            "21/41 [==============>...............] - ETA: 11:40 - loss: 8.0655 - custom_mse: 180.6674 - recall: 0.4945\n",
            "reduce_confident_loss =  0.247700796\n",
            "\n",
            "reduce_mean_cls_loss =  0.145715907\n",
            "\n",
            "regression loss =  6.11222315\n",
            "22/41 [===============>..............] - ETA: 11:04 - loss: 7.9946 - custom_mse: 275.0432 - recall: 0.5155\n",
            "reduce_confident_loss =  0.211206034\n",
            "\n",
            "reduce_mean_cls_loss =  0.223475084\n",
            "\n",
            "regression loss =  8.51974583\n",
            "23/41 [===============>..............] - ETA: 10:30 - loss: 8.0363 - custom_mse: 168.1083 - recall: 0.5307\n",
            "reduce_confident_loss =  0.296063423\n",
            "\n",
            "reduce_mean_cls_loss =  0.00386322499\n",
            "\n",
            "regression loss =  3.05975199\n",
            "24/41 [================>.............] - ETA: 9:55 - loss: 7.8415 - custom_mse: 186.1622 - recall: 0.5502 \n",
            "reduce_confident_loss =  0.171243101\n",
            "\n",
            "reduce_mean_cls_loss =  0.00188560516\n",
            "\n",
            "regression loss =  2.13357019\n",
            "25/41 [=================>............] - ETA: 9:20 - loss: 7.6201 - custom_mse: 258.4254 - recall: 0.5682\n",
            "reduce_confident_loss =  0.274730474\n",
            "\n",
            "reduce_mean_cls_loss =  0.00592813734\n",
            "\n",
            "regression loss =  6.47669506\n",
            "26/41 [==================>...........] - ETA: 8:45 - loss: 7.5869 - custom_mse: 218.2850 - recall: 0.5842\n",
            "reduce_confident_loss =  0.24722968\n",
            "\n",
            "reduce_mean_cls_loss =  0.284794092\n",
            "\n",
            "regression loss =  4.43271446\n",
            "27/41 [==================>...........] - ETA: 8:11 - loss: 7.4898 - custom_mse: 182.3957 - recall: 0.5899\n",
            "reduce_confident_loss =  0.122663938\n",
            "\n",
            "reduce_mean_cls_loss =  0.0739018917\n",
            "\n",
            "regression loss =  2.27372432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqbC-Z9TUMY8"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYYwj8QycV_Z"
      },
      "source": [
        "## training using recall in callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUvaUNbFeLjT"
      },
      "source": [
        "Reason why: There is a problem with computing precision and recall using keras.metrics and keras.losses API. Remember - that the final value of loss or metric is a mean across every batch - but for precision` and recall - a mean across the batches is not equal to the final metric value. I advise you to use keras.callbacks in order to compute appropriate values.\n",
        "\n",
        "https://stackoverflow.com/questions/48742662/custom-macro-for-recall-in-keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "46YI0GZIdX1C"
      },
      "source": [
        "# Using with the recall metric to avoid error tf.function-decorated function tried to create variables on non-first call.\n",
        "tf.config.run_functions_eagerly(True)\n",
        "history_recall_in_callback = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-l5ZYaSRS1C",
        "outputId": "be7c421f-f1ff-4a2b-b470-2bb4cceadd74"
      },
      "source": [
        "history_1 = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "\n",
            "reduce_confident_loss =  0.803938866\n",
            "\n",
            "reduce_mean_cls_loss =  0.874852777\n",
            "\n",
            "regression loss =  111.121033\n",
            "      1/Unknown - 149s 149s/step - loss: 112.7998 - custom_mse: 2395.1819\n",
            "reduce_confident_loss =  0.829265654\n",
            "\n",
            "reduce_mean_cls_loss =  0.62404573\n",
            "\n",
            "regression loss =  54.5301132\n",
            "      2/Unknown - 205s 57s/step - loss: 84.3916 - custom_mse: 2546.6406  \n",
            "reduce_confident_loss =  0.769748747\n",
            "\n",
            "reduce_mean_cls_loss =  0.699089468\n",
            "\n",
            "regression loss =  47.2647362\n",
            "      3/Unknown - 259s 55s/step - loss: 72.5056 - custom_mse: 2701.7148\n",
            "reduce_confident_loss =  0.775489688\n",
            "\n",
            "reduce_mean_cls_loss =  0.769976497\n",
            "\n",
            "regression loss =  41.7800446\n",
            "      4/Unknown - 312s 55s/step - loss: 65.2106 - custom_mse: 2779.4500\n",
            "reduce_confident_loss =  0.743893087\n",
            "\n",
            "reduce_mean_cls_loss =  0.620705247\n",
            "\n",
            "regression loss =  39.149231\n",
            "      5/Unknown - 366s 54s/step - loss: 60.2712 - custom_mse: 2825.9856\n",
            "reduce_confident_loss =  0.789561629\n",
            "\n",
            "reduce_mean_cls_loss =  0.657691061\n",
            "\n",
            "regression loss =  36.5224609\n",
            "      6/Unknown - 420s 54s/step - loss: 56.5543 - custom_mse: 2863.7764\n",
            "reduce_confident_loss =  0.756919444\n",
            "\n",
            "reduce_mean_cls_loss =  0.660188615\n",
            "\n",
            "regression loss =  48.3564415\n",
            "      7/Unknown - 473s 54s/step - loss: 55.5856 - custom_mse: 2882.9756\n",
            "reduce_confident_loss =  0.875271261\n",
            "\n",
            "reduce_mean_cls_loss =  0.593958557\n",
            "\n",
            "regression loss =  28.46628\n",
            "      8/Unknown - 542s 56s/step - loss: 52.3794 - custom_mse: 3256.3604\n",
            "reduce_confident_loss =  0.983293653\n",
            "\n",
            "reduce_mean_cls_loss =  0.550237536\n",
            "\n",
            "regression loss =  23.1795464\n",
            "      9/Unknown - 593s 55s/step - loss: 49.3053 - custom_mse: 3708.7971\n",
            "reduce_confident_loss =  1.04652476\n",
            "\n",
            "reduce_mean_cls_loss =  0.508198261\n",
            "\n",
            "regression loss =  23.0237865\n",
            "     10/Unknown - 643s 55s/step - loss: 46.8327 - custom_mse: 4100.0400\n",
            "reduce_confident_loss =  0.998942196\n",
            "\n",
            "reduce_mean_cls_loss =  0.471621901\n",
            "\n",
            "regression loss =  24.5156307\n",
            "     11/Unknown - 694s 55s/step - loss: 44.9375 - custom_mse: 4451.8506\n",
            "reduce_confident_loss =  1.03006744\n",
            "\n",
            "reduce_mean_cls_loss =  0.441961288\n",
            "\n",
            "regression loss =  24.0315094\n",
            "     12/Unknown - 745s 54s/step - loss: 43.3180 - custom_mse: 4828.7764\n",
            "reduce_confident_loss =  1.00034893\n",
            "\n",
            "reduce_mean_cls_loss =  0.487470657\n",
            "\n",
            "regression loss =  17.8270073\n",
            "     13/Unknown - 797s 54s/step - loss: 41.4716 - custom_mse: 5214.8774\n",
            "reduce_confident_loss =  1.00165141\n",
            "\n",
            "reduce_mean_cls_loss =  0.427651435\n",
            "\n",
            "regression loss =  18.6697311\n",
            "     14/Unknown - 848s 54s/step - loss: 39.9450 - custom_mse: 5645.8462\n",
            "reduce_confident_loss =  1.02690506\n",
            "\n",
            "reduce_mean_cls_loss =  0.404226303\n",
            "\n",
            "regression loss =  15.8971939\n",
            "     15/Unknown - 900s 54s/step - loss: 38.4372 - custom_mse: 6032.2954\n",
            "reduce_confident_loss =  1.02750266\n",
            "\n",
            "reduce_mean_cls_loss =  0.426943451\n",
            "\n",
            "regression loss =  14.256609\n",
            "     16/Unknown - 952s 54s/step - loss: 37.0168 - custom_mse: 6418.9307\n",
            "reduce_confident_loss =  1.06699038\n",
            "\n",
            "reduce_mean_cls_loss =  0.481234848\n",
            "\n",
            "regression loss =  39.7593422\n",
            "     17/Unknown - 1004s 53s/step - loss: 37.2692 - custom_mse: 6779.9341\n",
            "reduce_confident_loss =  0.792576671\n",
            "\n",
            "reduce_mean_cls_loss =  0.679874837\n",
            "\n",
            "regression loss =  62.9575272\n",
            "     18/Unknown - 1055s 53s/step - loss: 38.7782 - custom_mse: 6923.8940\n",
            "reduce_confident_loss =  0.799659908\n",
            "\n",
            "reduce_mean_cls_loss =  0.738155901\n",
            "\n",
            "regression loss =  67.6987228\n",
            "     19/Unknown - 1146s 55s/step - loss: 40.3812 - custom_mse: 6975.2427\n",
            "reduce_confident_loss =  0.777786255\n",
            "\n",
            "reduce_mean_cls_loss =  0.645353436\n",
            "\n",
            "regression loss =  74.0907898\n",
            "     20/Unknown - 1199s 55s/step - loss: 42.1379 - custom_mse: 7196.1265\n",
            "reduce_confident_loss =  0.765221775\n",
            "\n",
            "reduce_mean_cls_loss =  0.64242965\n",
            "\n",
            "regression loss =  55.8800087\n",
            "     21/Unknown - 1251s 55s/step - loss: 42.8593 - custom_mse: 7393.2622\n",
            "reduce_confident_loss =  0.811901927\n",
            "\n",
            "reduce_mean_cls_loss =  0.620795786\n",
            "\n",
            "regression loss =  69.7269058\n",
            "     22/Unknown - 1303s 55s/step - loss: 44.1457 - custom_mse: 7402.9512\n",
            "reduce_confident_loss =  0.887404859\n",
            "\n",
            "reduce_mean_cls_loss =  0.588072717\n",
            "\n",
            "regression loss =  30.0984936\n",
            "     23/Unknown - 1377s 56s/step - loss: 43.5991 - custom_mse: 7678.2334\n",
            "reduce_confident_loss =  0.952687562\n",
            "\n",
            "reduce_mean_cls_loss =  0.538274586\n",
            "\n",
            "regression loss =  15.5775824\n",
            "     24/Unknown - 1431s 56s/step - loss: 42.4936 - custom_mse: 7897.6387\n",
            "reduce_confident_loss =  0.941658\n",
            "\n",
            "reduce_mean_cls_loss =  0.551239908\n",
            "\n",
            "regression loss =  12.4546757\n",
            "     25/Unknown - 1491s 56s/step - loss: 41.3518 - custom_mse: 8120.3320\n",
            "reduce_confident_loss =  1.05675304\n",
            "\n",
            "reduce_mean_cls_loss =  0.399676114\n",
            "\n",
            "regression loss =  27.764616\n",
            "     26/Unknown - 1542s 56s/step - loss: 40.8852 - custom_mse: 8535.2842\n",
            "reduce_confident_loss =  0.776097953\n",
            "\n",
            "reduce_mean_cls_loss =  0.557153881\n",
            "\n",
            "regression loss =  15.3947744\n",
            "     27/Unknown - 1593s 56s/step - loss: 39.9905 - custom_mse: 8794.8232\n",
            "reduce_confident_loss =  0.775941432\n",
            "\n",
            "reduce_mean_cls_loss =  0.540933788\n",
            "\n",
            "regression loss =  9.57405472\n",
            "     28/Unknown - 1644s 55s/step - loss: 38.9513 - custom_mse: 8852.3496\n",
            "reduce_confident_loss =  0.790006042\n",
            "\n",
            "reduce_mean_cls_loss =  0.560797751\n",
            "\n",
            "regression loss =  8.86601353\n",
            "     29/Unknown - 1695s 55s/step - loss: 37.9604 - custom_mse: 8889.3516\n",
            "reduce_confident_loss =  0.765200734\n",
            "\n",
            "reduce_mean_cls_loss =  0.50962323\n",
            "\n",
            "regression loss =  10.2397251\n",
            "     30/Unknown - 1746s 55s/step - loss: 37.0789 - custom_mse: 8957.1689\n",
            "reduce_confident_loss =  0.756825328\n",
            "\n",
            "reduce_mean_cls_loss =  0.579282761\n",
            "\n",
            "regression loss =  11.5710173\n",
            "     31/Unknown - 1796s 55s/step - loss: 36.2991 - custom_mse: 9066.5225\n",
            "reduce_confident_loss =  0.770497501\n",
            "\n",
            "reduce_mean_cls_loss =  0.580566585\n",
            "\n",
            "regression loss =  12.5980759\n",
            "     32/Unknown - 1848s 55s/step - loss: 35.6007 - custom_mse: 9297.6777\n",
            "reduce_confident_loss =  0.887491345\n",
            "\n",
            "reduce_mean_cls_loss =  0.479226798\n",
            "\n",
            "regression loss =  6.71500397\n",
            "     33/Unknown - 1900s 55s/step - loss: 34.7668 - custom_mse: 9603.4297\n",
            "reduce_confident_loss =  1.02934527\n",
            "\n",
            "reduce_mean_cls_loss =  0.332781255\n",
            "\n",
            "regression loss =  7.326478\n",
            "     34/Unknown - 1950s 55s/step - loss: 33.9998 - custom_mse: 9730.9141\n",
            "reduce_confident_loss =  0.99961412\n",
            "\n",
            "reduce_mean_cls_loss =  0.36345306\n",
            "\n",
            "regression loss =  6.62387753\n",
            "     35/Unknown - 2001s 54s/step - loss: 33.2566 - custom_mse: 9763.2822\n",
            "reduce_confident_loss =  0.974879086\n",
            "\n",
            "reduce_mean_cls_loss =  0.331653744\n",
            "\n",
            "regression loss =  83.3633194\n",
            "     36/Unknown - 2067s 55s/step - loss: 34.6847 - custom_mse: 18162.4727\n",
            "reduce_confident_loss =  0.977084577\n",
            "\n",
            "reduce_mean_cls_loss =  1.12015247\n",
            "\n",
            "regression loss =  137.253494\n",
            "     37/Unknown - 2117s 55s/step - loss: 37.5135 - custom_mse: 26682.1230\n",
            "reduce_confident_loss =  0.873090744\n",
            "\n",
            "reduce_mean_cls_loss =  0.539783657\n",
            "\n",
            "regression loss =  106.684761\n",
            "     38/Unknown - 2171s 55s/step - loss: 39.3710 - custom_mse: 30752.9219\n",
            "reduce_confident_loss =  0.965759\n",
            "\n",
            "reduce_mean_cls_loss =  1.27322853\n",
            "\n",
            "regression loss =  138.360962\n",
            "     39/Unknown - 2221s 55s/step - loss: 41.9666 - custom_mse: 40097.9336\n",
            "reduce_confident_loss =  0.703599513\n",
            "\n",
            "reduce_mean_cls_loss =  0.937586129\n",
            "\n",
            "regression loss =  132.217911\n",
            "     40/Unknown - 2272s 54s/step - loss: 44.2639 - custom_mse: 48535.8125\n",
            "reduce_confident_loss =  0.963364661\n",
            "\n",
            "reduce_mean_cls_loss =  1.2405355\n",
            "\n",
            "regression loss =  153.08873\n",
            "     41/Unknown - 2337s 55s/step - loss: 46.9719 - custom_mse: 62457.9375\n",
            "reduce_confident_loss =  0.794252694\n",
            "\n",
            "reduce_mean_cls_loss =  1.04830992\n",
            "\n",
            "regression loss =  168.136673\n",
            "     42/Unknown - 2387s 55s/step - loss: 49.9007 - custom_mse: 78955.8906\n",
            "reduce_confident_loss =  0.915359497\n",
            "\n",
            "reduce_mean_cls_loss =  1.20306861\n",
            "\n",
            "regression loss =  140.4664\n",
            "     43/Unknown - 2466s 55s/step - loss: 52.0561 - custom_mse: 91803.3984\n",
            "reduce_confident_loss =  0.943990409\n",
            "\n",
            "reduce_mean_cls_loss =  1.29626048\n",
            "\n",
            "regression loss =  19.751812\n",
            "     44/Unknown - 2516s 55s/step - loss: 51.3729 - custom_mse: 92346.7266\n",
            "reduce_confident_loss =  0.848481596\n",
            "\n",
            "reduce_mean_cls_loss =  1.13185918\n",
            "\n",
            "regression loss =  26.6942635\n",
            "     45/Unknown - 2569s 55s/step - loss: 50.8684 - custom_mse: 93178.4453\n",
            "reduce_confident_loss =  0.926542699\n",
            "\n",
            "reduce_mean_cls_loss =  1.24157143\n",
            "\n",
            "regression loss =  46.8029976\n",
            "     46/Unknown - 2624s 55s/step - loss: 50.8272 - custom_mse: 94886.3516\n",
            "reduce_confident_loss =  0.935631096\n",
            "\n",
            "reduce_mean_cls_loss =  1.29403937\n",
            "\n",
            "regression loss =  72.3774\n",
            "     47/Unknown - 2674s 55s/step - loss: 51.3332 - custom_mse: 98096.6016\n",
            "reduce_confident_loss =  0.945914865\n",
            "\n",
            "reduce_mean_cls_loss =  0.700168312\n",
            "\n",
            "regression loss =  18.7460022\n",
            "     48/Unknown - 2725s 55s/step - loss: 50.6886 - custom_mse: 99089.3594\n",
            "reduce_confident_loss =  0.646744907\n",
            "\n",
            "reduce_mean_cls_loss =  0.551184356\n",
            "\n",
            "regression loss =  11.3984241\n",
            "     49/Unknown - 2776s 55s/step - loss: 49.9112 - custom_mse: 99375.1953\n",
            "reduce_confident_loss =  0.649957359\n",
            "\n",
            "reduce_mean_cls_loss =  0.573191345\n",
            "\n",
            "regression loss =  9.38474751\n",
            "     50/Unknown - 2835s 55s/step - loss: 49.1251 - custom_mse: 99543.2656\n",
            "reduce_confident_loss =  0.948601604\n",
            "\n",
            "reduce_mean_cls_loss =  0.324730814\n",
            "\n",
            "regression loss =  8.75238514\n",
            "     51/Unknown - 2884s 55s/step - loss: 48.3584 - custom_mse: 99582.0938\n",
            "reduce_confident_loss =  0.943585038\n",
            "\n",
            "reduce_mean_cls_loss =  0.348778158\n",
            "\n",
            "regression loss =  9.17112923\n",
            "     52/Unknown - 2933s 55s/step - loss: 47.6297 - custom_mse: 99614.3359\n",
            "reduce_confident_loss =  0.925215065\n",
            "\n",
            "reduce_mean_cls_loss =  0.347412437\n",
            "\n",
            "regression loss =  62.474514\n",
            "     53/Unknown - 2994s 55s/step - loss: 47.9338 - custom_mse: 101510.7500\n",
            "reduce_confident_loss =  0.927601695\n",
            "\n",
            "reduce_mean_cls_loss =  1.19532311\n",
            "\n",
            "regression loss =  233.756241\n",
            "     54/Unknown - 3044s 55s/step - loss: 51.4143 - custom_mse: 130462.4688\n",
            "reduce_confident_loss =  0.955861747\n",
            "\n",
            "reduce_mean_cls_loss =  1.24682808\n",
            "\n",
            "regression loss =  223.835571\n",
            "     55/Unknown - 3094s 55s/step - loss: 54.5892 - custom_mse: 158332.8438\n",
            "reduce_confident_loss =  0.932733417\n",
            "\n",
            "reduce_mean_cls_loss =  1.2125808\n",
            "\n",
            "regression loss =  218.703049\n",
            "     56/Unknown - 3142s 54s/step - loss: 57.5582 - custom_mse: 184879.2031\n",
            "reduce_confident_loss =  0.95280081\n",
            "\n",
            "reduce_mean_cls_loss =  0.989642203\n",
            "\n",
            "regression loss =  71.8139267\n",
            "     57/Unknown - 3206s 55s/step - loss: 57.8423 - custom_mse: 193035.7812\n",
            "reduce_confident_loss =  0.857827723\n",
            "\n",
            "reduce_mean_cls_loss =  0.456618637\n",
            "\n",
            "regression loss =  7.91417408\n",
            "     58/Unknown - 3257s 55s/step - loss: 57.0042 - custom_mse: 193180.1406\n",
            "reduce_confident_loss =  0.740368187\n",
            "\n",
            "reduce_mean_cls_loss =  0.509487033\n",
            "\n",
            "regression loss =  33.5373802\n",
            "     59/Unknown - 3309s 54s/step - loss: 56.6276 - custom_mse: 193565.7969\n",
            "reduce_confident_loss =  0.751800716\n",
            "\n",
            "reduce_mean_cls_loss =  0.565666735\n",
            "\n",
            "regression loss =  32.4209595\n",
            "     60/Unknown - 3360s 54s/step - loss: 56.2461 - custom_mse: 193754.9688\n",
            "reduce_confident_loss =  0.975480139\n",
            "\n",
            "reduce_mean_cls_loss =  0.391850144\n",
            "\n",
            "regression loss =  8.63107204\n",
            "     61/Unknown - 3410s 54s/step - loss: 55.4880 - custom_mse: 193953.9844\n",
            "reduce_confident_loss =  0.972806811\n",
            "\n",
            "reduce_mean_cls_loss =  0.365170151\n",
            "\n",
            "regression loss =  65.0709686\n",
            "     62/Unknown - 3461s 54s/step - loss: 55.6641 - custom_mse: 194116.5000\n",
            "reduce_confident_loss =  0.705275357\n",
            "\n",
            "reduce_mean_cls_loss =  0.696633339\n",
            "\n",
            "regression loss =  88.6108\n",
            "     63/Unknown - 3512s 54s/step - loss: 56.2093 - custom_mse: 194462.7031\n",
            "reduce_confident_loss =  0.929783702\n",
            "\n",
            "reduce_mean_cls_loss =  0.440286487\n",
            "\n",
            "regression loss =  31.6813431\n",
            "     64/Unknown - 3563s 54s/step - loss: 55.8475 - custom_mse: 194861.1875\n",
            "reduce_confident_loss =  0.978640437\n",
            "\n",
            "reduce_mean_cls_loss =  0.375112087\n",
            "\n",
            "regression loss =  12.4588518\n",
            "     65/Unknown - 3614s 54s/step - loss: 55.2008 - custom_mse: 195324.7344\n",
            "reduce_confident_loss =  0.997604489\n",
            "\n",
            "reduce_mean_cls_loss =  0.396030575\n",
            "\n",
            "regression loss =  17.0248394\n",
            "     66/Unknown - 3665s 54s/step - loss: 54.6435 - custom_mse: 195736.0000\n",
            "reduce_confident_loss =  0.993533552\n",
            "\n",
            "reduce_mean_cls_loss =  0.394956917\n",
            "\n",
            "regression loss =  18.1175117\n",
            "     67/Unknown - 3716s 54s/step - loss: 54.1190 - custom_mse: 196162.6250\n",
            "reduce_confident_loss =  0.994665802\n",
            "\n",
            "reduce_mean_cls_loss =  0.35051766\n",
            "\n",
            "regression loss =  18.1801777\n",
            "     68/Unknown - 3766s 54s/step - loss: 53.6103 - custom_mse: 196588.0312\n",
            "reduce_confident_loss =  0.983734548\n",
            "\n",
            "reduce_mean_cls_loss =  0.463069648\n",
            "\n",
            "regression loss =  10.4976282\n",
            "     69/Unknown - 3827s 54s/step - loss: 53.0065 - custom_mse: 197023.3125\n",
            "reduce_confident_loss =  0.979388356\n",
            "\n",
            "reduce_mean_cls_loss =  0.36061123\n",
            "\n",
            "regression loss =  8.3722887\n",
            "     70/Unknown - 3878s 54s/step - loss: 52.3880 - custom_mse: 197154.9219\n",
            "reduce_confident_loss =  0.99656111\n",
            "\n",
            "reduce_mean_cls_loss =  0.3315382\n",
            "\n",
            "regression loss =  6.48214293\n",
            "     71/Unknown - 3929s 54s/step - loss: 51.7601 - custom_mse: 197184.1562\n",
            "reduce_confident_loss =  0.86992687\n",
            "\n",
            "reduce_mean_cls_loss =  0.479873836\n",
            "\n",
            "regression loss =  11.6087713\n",
            "     72/Unknown - 3980s 54s/step - loss: 51.2212 - custom_mse: 197310.1094\n",
            "reduce_confident_loss =  0.962868273\n",
            "\n",
            "reduce_mean_cls_loss =  0.436033249\n",
            "\n",
            "regression loss =  17.1281815\n",
            "     73/Unknown - 4006s 54s/step - loss: 50.9957 - custom_mse: 197468.2344\n",
            "reduce_confident_loss =  1.00455689\n",
            "\n",
            "reduce_mean_cls_loss =  0.500079751\n",
            "\n",
            "regression loss =  0.94701916\n",
            "\n",
            "reduce_confident_loss =  0.942792296\n",
            "\n",
            "reduce_mean_cls_loss =  0.536283\n",
            "\n",
            "regression loss =  7.62335253\n",
            "\n",
            "reduce_confident_loss =  0.96344322\n",
            "\n",
            "reduce_mean_cls_loss =  0.52240628\n",
            "\n",
            "regression loss =  3.87490702\n",
            "\n",
            "reduce_confident_loss =  0.901696265\n",
            "\n",
            "reduce_mean_cls_loss =  0.559489846\n",
            "\n",
            "regression loss =  11.3866796\n",
            "\n",
            "reduce_confident_loss =  0.718599856\n",
            "\n",
            "reduce_mean_cls_loss =  0.677563608\n",
            "\n",
            "regression loss =  51.8790932\n",
            "\n",
            "reduce_confident_loss =  0.637245715\n",
            "\n",
            "reduce_mean_cls_loss =  0.692045212\n",
            "\n",
            "regression loss =  29.4034252\n",
            "\n",
            "reduce_confident_loss =  1.00401402\n",
            "\n",
            "reduce_mean_cls_loss =  0.497546494\n",
            "\n",
            "regression loss =  1.84598577\n",
            "\n",
            "reduce_confident_loss =  1.00377023\n",
            "\n",
            "reduce_mean_cls_loss =  0.496180862\n",
            "\n",
            "regression loss =  1.76366544\n",
            "\n",
            "reduce_confident_loss =  1.00388885\n",
            "\n",
            "reduce_mean_cls_loss =  0.500152111\n",
            "\n",
            "regression loss =  1.68931878\n",
            "\n",
            "reduce_confident_loss =  1.00400186\n",
            "\n",
            "reduce_mean_cls_loss =  0.501384437\n",
            "\n",
            "regression loss =  1.60478508\n",
            "\n",
            "reduce_confident_loss =  1.00453675\n",
            "\n",
            "reduce_mean_cls_loss =  0.499879301\n",
            "\n",
            "regression loss =  6.25437\n",
            "\n",
            "reduce_confident_loss =  0.942735553\n",
            "\n",
            "reduce_mean_cls_loss =  0.5361\n",
            "\n",
            "regression loss =  6.17685747\n",
            "\n",
            "reduce_confident_loss =  1.00482023\n",
            "\n",
            "reduce_mean_cls_loss =  0.497015208\n",
            "\n",
            "regression loss =  12.0515575\n",
            "\n",
            "reduce_confident_loss =  1.00458133\n",
            "\n",
            "reduce_mean_cls_loss =  0.686343551\n",
            "\n",
            "regression loss =  61.6223793\n",
            "73/73 [==============================] - 4577s 62s/step - loss: 50.9957 - custom_mse: 197468.2344 - val_loss: 14.1026 - val_custom_mse: 6258.1685\n",
            "Epoch 2/4\n",
            "\n",
            "reduce_confident_loss =  0.799350739\n",
            "\n",
            "reduce_mean_cls_loss =  0.851572156\n",
            "\n",
            "regression loss =  104.563644\n",
            " 1/73 [..............................] - ETA: 49:56 - loss: 106.2146 - custom_mse: 2492.7480\n",
            "reduce_confident_loss =  0.891029477\n",
            "\n",
            "reduce_mean_cls_loss =  0.494117409\n",
            "\n",
            "regression loss =  45.1033936\n",
            " 2/73 [..............................] - ETA: 32:53 - loss: 76.3516 - custom_mse: 2641.6326 \n",
            "reduce_confident_loss =  0.719659\n",
            "\n",
            "reduce_mean_cls_loss =  0.517954051\n",
            "\n",
            "regression loss =  33.473835\n",
            " 3/73 [>.............................] - ETA: 31:31 - loss: 62.4715 - custom_mse: 2754.7869\n",
            "reduce_confident_loss =  0.721557081\n",
            "\n",
            "reduce_mean_cls_loss =  0.535443246\n",
            "\n",
            "regression loss =  31.012989\n",
            " 4/73 [>.............................] - ETA: 30:46 - loss: 54.9211 - custom_mse: 2814.7449\n",
            "reduce_confident_loss =  0.722565234\n",
            "\n",
            "reduce_mean_cls_loss =  0.547989786\n",
            "\n",
            "regression loss =  29.2856159\n",
            " 5/73 [=>............................] - ETA: 30:09 - loss: 50.0481 - custom_mse: 2862.2085\n",
            "reduce_confident_loss =  0.745155215\n",
            "\n",
            "reduce_mean_cls_loss =  0.534221\n",
            "\n",
            "regression loss =  28.555336\n",
            " 6/73 [=>............................] - ETA: 29:37 - loss: 46.6792 - custom_mse: 2894.1631\n",
            "reduce_confident_loss =  0.740397751\n",
            "\n",
            "reduce_mean_cls_loss =  0.54853195\n",
            "\n",
            "regression loss =  42.562027\n",
            " 7/73 [=>............................] - ETA: 29:07 - loss: 46.2752 - custom_mse: 2913.2468\n",
            "reduce_confident_loss =  0.835904121\n",
            "\n",
            "reduce_mean_cls_loss =  0.536051333\n",
            "\n",
            "regression loss =  20.4640789\n",
            " 8/73 [==>...........................] - ETA: 28:51 - loss: 43.2203 - custom_mse: 3272.8621\n",
            "reduce_confident_loss =  0.994570851\n",
            "\n",
            "reduce_mean_cls_loss =  0.329485685\n",
            "\n",
            "regression loss =  10.4638729\n",
            " 9/73 [==>...........................] - ETA: 28:22 - loss: 39.7278 - custom_mse: 3672.2136\n",
            "reduce_confident_loss =  1.03734088\n",
            "\n",
            "reduce_mean_cls_loss =  0.338325173\n",
            "\n",
            "regression loss =  9.39691639\n",
            "10/73 [===>..........................] - ETA: 27:50 - loss: 36.8323 - custom_mse: 4065.9199\n",
            "reduce_confident_loss =  1.02699101\n",
            "\n",
            "reduce_mean_cls_loss =  0.331592292\n",
            "\n",
            "regression loss =  22.6121941\n",
            "11/73 [===>..........................] - ETA: 27:19 - loss: 35.6631 - custom_mse: 4416.1226\n",
            "reduce_confident_loss =  1.03106105\n",
            "\n",
            "reduce_mean_cls_loss =  0.329002708\n",
            "\n",
            "regression loss =  28.922102\n",
            "12/73 [===>..........................] - ETA: 26:48 - loss: 35.2147 - custom_mse: 4840.8848\n",
            "reduce_confident_loss =  0.98786515\n",
            "\n",
            "reduce_mean_cls_loss =  0.35753113\n",
            "\n",
            "regression loss =  8.59385777\n",
            "13/73 [====>.........................] - ETA: 26:18 - loss: 33.2704 - custom_mse: 5300.3057\n",
            "reduce_confident_loss =  1.01176131\n",
            "\n",
            "reduce_mean_cls_loss =  0.344941854\n",
            "\n",
            "regression loss =  4.73712778\n",
            "14/73 [====>.........................] - ETA: 25:50 - loss: 31.3292 - custom_mse: 5731.3945\n",
            "reduce_confident_loss =  1.0338043\n",
            "\n",
            "reduce_mean_cls_loss =  0.35854134\n",
            "\n",
            "regression loss =  4.92688084\n",
            "15/73 [=====>........................] - ETA: 25:21 - loss: 29.6619 - custom_mse: 6175.6709\n",
            "reduce_confident_loss =  1.00114965\n",
            "\n",
            "reduce_mean_cls_loss =  0.323600054\n",
            "\n",
            "regression loss =  3.00988483\n",
            "16/73 [=====>........................] - ETA: 24:53 - loss: 28.0789 - custom_mse: 6639.4277\n",
            "reduce_confident_loss =  1.00057614\n",
            "\n",
            "reduce_mean_cls_loss =  0.320659637\n",
            "\n",
            "regression loss =  32.2653427\n",
            "17/73 [=====>........................] - ETA: 24:25 - loss: 28.4029 - custom_mse: 7075.4541\n",
            "reduce_confident_loss =  0.732897\n",
            "\n",
            "reduce_mean_cls_loss =  0.503655672\n",
            "\n",
            "regression loss =  59.8488121\n",
            "18/73 [======>.......................] - ETA: 23:58 - loss: 30.2186 - custom_mse: 7268.3076\n",
            "reduce_confident_loss =  0.712738574\n",
            "\n",
            "reduce_mean_cls_loss =  0.587007523\n",
            "\n",
            "regression loss =  63.7786255\n",
            "19/73 [======>.......................] - ETA: 25:10 - loss: 32.0533 - custom_mse: 7344.3945\n",
            "reduce_confident_loss =  0.729931176\n",
            "\n",
            "reduce_mean_cls_loss =  0.472294539\n",
            "\n",
            "regression loss =  64.1794586\n",
            "20/73 [=======>......................] - ETA: 24:38 - loss: 33.7197 - custom_mse: 7493.1099\n",
            "reduce_confident_loss =  0.722440302\n",
            "\n",
            "reduce_mean_cls_loss =  0.457466632\n",
            "\n",
            "regression loss =  48.1307869\n",
            "21/73 [=======>......................] - ETA: 24:05 - loss: 34.4622 - custom_mse: 7620.1372\n",
            "reduce_confident_loss =  0.723190844\n",
            "\n",
            "reduce_mean_cls_loss =  0.470184326\n",
            "\n",
            "regression loss =  60.677536\n",
            "22/73 [========>.....................] - ETA: 23:33 - loss: 35.7080 - custom_mse: 7629.6240\n",
            "reduce_confident_loss =  0.795498729\n",
            "\n",
            "reduce_mean_cls_loss =  0.378966063\n",
            "\n",
            "regression loss =  23.5415955\n",
            "23/73 [========>.....................] - ETA: 23:04 - loss: 35.2301 - custom_mse: 7800.7759\n",
            "reduce_confident_loss =  0.849545777\n",
            "\n",
            "reduce_mean_cls_loss =  0.353551894\n",
            "\n",
            "regression loss =  15.5246038\n",
            "24/73 [========>.....................] - ETA: 22:36 - loss: 34.4592 - custom_mse: 7915.6357\n",
            "reduce_confident_loss =  0.842452586\n",
            "\n",
            "reduce_mean_cls_loss =  0.33122614\n",
            "\n",
            "regression loss =  17.1248302\n",
            "25/73 [=========>....................] - ETA: 22:18 - loss: 33.8127 - custom_mse: 8079.2031\n",
            "reduce_confident_loss =  0.992850602\n",
            "\n",
            "reduce_mean_cls_loss =  0.337966889\n",
            "\n",
            "regression loss =  27.6908779\n",
            "26/73 [=========>....................] - ETA: 21:45 - loss: 33.6285 - custom_mse: 8478.3018\n",
            "reduce_confident_loss =  0.71928823\n",
            "\n",
            "reduce_mean_cls_loss =  0.444664925\n",
            "\n",
            "regression loss =  15.2703686\n",
            "27/73 [==========>...................] - ETA: 21:14 - loss: 32.9917 - custom_mse: 8828.3223\n",
            "reduce_confident_loss =  0.713427961\n",
            "\n",
            "reduce_mean_cls_loss =  0.445557117\n",
            "\n",
            "regression loss =  7.52825403\n",
            "28/73 [==========>...................] - ETA: 20:43 - loss: 32.1236 - custom_mse: 8927.4463\n",
            "reduce_confident_loss =  0.714227259\n",
            "\n",
            "reduce_mean_cls_loss =  0.435863107\n",
            "\n",
            "regression loss =  6.28184\n",
            "29/73 [==========>...................] - ETA: 20:13 - loss: 31.2722 - custom_mse: 8999.7939\n",
            "reduce_confident_loss =  0.715303\n",
            "\n",
            "reduce_mean_cls_loss =  0.4401187\n",
            "\n",
            "regression loss =  8.25171471\n",
            "30/73 [===========>..................] - ETA: 19:43 - loss: 30.5434 - custom_mse: 9108.4541\n",
            "reduce_confident_loss =  0.711320758\n",
            "\n",
            "reduce_mean_cls_loss =  0.474728644\n",
            "\n",
            "regression loss =  11.1630135\n",
            "31/73 [===========>..................] - ETA: 19:13 - loss: 29.9565 - custom_mse: 9304.7578\n",
            "reduce_confident_loss =  0.70402652\n",
            "\n",
            "reduce_mean_cls_loss =  0.470371813\n",
            "\n",
            "regression loss =  12.6246738\n",
            "32/73 [============>.................] - ETA: 18:43 - loss: 29.4515 - custom_mse: 9626.4043\n",
            "reduce_confident_loss =  0.863850653\n",
            "\n",
            "reduce_mean_cls_loss =  0.322206765\n",
            "\n",
            "regression loss =  5.15330076\n",
            "33/73 [============>.................] - ETA: 18:16 - loss: 28.7512 - custom_mse: 9814.0098\n",
            "reduce_confident_loss =  1.00242102\n",
            "\n",
            "reduce_mean_cls_loss =  0.319139153\n",
            "\n",
            "regression loss =  9.15827274\n",
            "34/73 [============>.................] - ETA: 17:47 - loss: 28.2138 - custom_mse: 9875.7842\n",
            "reduce_confident_loss =  0.982707143\n",
            "\n",
            "reduce_mean_cls_loss =  0.317751914\n",
            "\n",
            "regression loss =  3.93976665\n",
            "35/73 [=============>................] - ETA: 17:18 - loss: 27.5574 - custom_mse: 9884.7900\n",
            "reduce_confident_loss =  0.976893485\n",
            "\n",
            "reduce_mean_cls_loss =  0.315084875\n",
            "\n",
            "regression loss =  76.2841797\n",
            "36/73 [=============>................] - ETA: 16:53 - loss: 28.9468 - custom_mse: 18256.2070\n",
            "reduce_confident_loss =  0.950979114\n",
            "\n",
            "reduce_mean_cls_loss =  1.21724236\n",
            "\n",
            "regression loss =  123.049797\n",
            "37/73 [==============>...............] - ETA: 16:24 - loss: 31.5487 - custom_mse: 26806.4004\n",
            "reduce_confident_loss =  0.844195485\n",
            "\n",
            "reduce_mean_cls_loss =  0.712538421\n",
            "\n",
            "regression loss =  91.7894287\n",
            "38/73 [==============>...............] - ETA: 15:59 - loss: 33.1750 - custom_mse: 30782.4688\n",
            "reduce_confident_loss =  0.981481433\n",
            "\n",
            "reduce_mean_cls_loss =  1.29052365\n",
            "\n",
            "regression loss =  123.300758\n",
            "39/73 [===============>..............] - ETA: 15:30 - loss: 35.5441 - custom_mse: 40086.2109\n",
            "reduce_confident_loss =  0.687147677\n",
            "\n",
            "reduce_mean_cls_loss =  1.09937918\n",
            "\n",
            "regression loss =  123.230927\n",
            "40/73 [===============>..............] - ETA: 15:01 - loss: 37.7810 - custom_mse: 48477.5469\n",
            "reduce_confident_loss =  0.97646457\n",
            "\n",
            "reduce_mean_cls_loss =  1.30045342\n",
            "\n",
            "regression loss =  158.348389\n",
            "41/73 [===============>..............] - ETA: 14:36 - loss: 40.7772 - custom_mse: 62861.7266\n",
            "reduce_confident_loss =  0.756885529\n",
            "\n",
            "reduce_mean_cls_loss =  1.21978211\n",
            "\n",
            "regression loss =  169.585907\n",
            "42/73 [================>.............] - ETA: 14:07 - loss: 43.8911 - custom_mse: 79800.9219\n",
            "reduce_confident_loss =  0.904662251\n",
            "\n",
            "reduce_mean_cls_loss =  1.3037349\n",
            "\n",
            "regression loss =  143.508682\n",
            "43/73 [================>.............] - ETA: 13:51 - loss: 46.2592 - custom_mse: 93097.8906\n",
            "reduce_confident_loss =  0.959848583\n",
            "\n",
            "reduce_mean_cls_loss =  1.29156911\n",
            "\n",
            "regression loss =  22.8671932\n",
            "44/73 [=================>............] - ETA: 13:22 - loss: 45.7787 - custom_mse: 93750.7031\n",
            "reduce_confident_loss =  0.861999393\n",
            "\n",
            "reduce_mean_cls_loss =  1.30007088\n",
            "\n",
            "regression loss =  26.4471588\n",
            "45/73 [=================>............] - ETA: 12:53 - loss: 45.3972 - custom_mse: 94656.1641\n",
            "reduce_confident_loss =  0.962972522\n",
            "\n",
            "reduce_mean_cls_loss =  1.30593991\n",
            "\n",
            "regression loss =  44.8312111\n",
            "46/73 [=================>............] - ETA: 12:26 - loss: 45.4342 - custom_mse: 96436.1172\n",
            "reduce_confident_loss =  0.960924745\n",
            "\n",
            "reduce_mean_cls_loss =  1.29945099\n",
            "\n",
            "regression loss =  71.0732803\n",
            "47/73 [==================>...........] - ETA: 11:57 - loss: 46.0278 - custom_mse: 99623.3516\n",
            "reduce_confident_loss =  0.959392369\n",
            "\n",
            "reduce_mean_cls_loss =  0.738747418\n",
            "\n",
            "regression loss =  22.381731\n",
            "48/73 [==================>...........] - ETA: 11:30 - loss: 45.5705 - custom_mse: 100794.0312\n",
            "reduce_confident_loss =  0.672270119\n",
            "\n",
            "reduce_mean_cls_loss =  0.453420162\n",
            "\n",
            "regression loss =  12.2699184\n",
            "49/73 [===================>..........] - ETA: 11:01 - loss: 44.9139 - custom_mse: 101251.8359\n",
            "reduce_confident_loss =  0.670720398\n",
            "\n",
            "reduce_mean_cls_loss =  0.470848411\n",
            "\n",
            "regression loss =  11.2709475\n",
            "50/73 [===================>..........] - ETA: 10:33 - loss: 44.2639 - custom_mse: 101526.6875\n",
            "reduce_confident_loss =  0.958109558\n",
            "\n",
            "reduce_mean_cls_loss =  0.314845145\n",
            "\n",
            "regression loss =  11.1973028\n",
            "51/73 [===================>..........] - ETA: 10:04 - loss: 43.6405 - custom_mse: 101612.8984\n",
            "reduce_confident_loss =  0.96383673\n",
            "\n",
            "reduce_mean_cls_loss =  0.316764623\n",
            "\n",
            "regression loss =  9.84438\n",
            "52/73 [====================>.........] - ETA: 9:36 - loss: 43.0152 - custom_mse: 101689.9375 \n",
            "reduce_confident_loss =  0.94984597\n",
            "\n",
            "reduce_mean_cls_loss =  0.32340768\n",
            "\n",
            "regression loss =  57.2941093\n",
            "53/73 [====================>.........] - ETA: 9:08 - loss: 43.3086 - custom_mse: 103580.8906\n",
            "reduce_confident_loss =  0.960852504\n",
            "\n",
            "reduce_mean_cls_loss =  1.306669\n",
            "\n",
            "regression loss =  197.244965\n",
            "54/73 [=====================>........] - ETA: 8:40 - loss: 46.2013 - custom_mse: 131965.0625\n",
            "reduce_confident_loss =  0.951048315\n",
            "\n",
            "reduce_mean_cls_loss =  1.30718291\n",
            "\n",
            "regression loss =  191.483871\n",
            "55/73 [=====================>........] - ETA: 8:11 - loss: 48.8838 - custom_mse: 159221.7188\n",
            "reduce_confident_loss =  0.957562149\n",
            "\n",
            "reduce_mean_cls_loss =  1.30734324\n",
            "\n",
            "regression loss =  183.98494\n",
            "56/73 [======================>.......] - ETA: 7:43 - loss: 51.3368 - custom_mse: 185485.2188\n",
            "reduce_confident_loss =  0.961323619\n",
            "\n",
            "reduce_mean_cls_loss =  1.02628624\n",
            "\n",
            "regression loss =  67.6256332\n",
            "57/73 [======================>.......] - ETA: 7:15 - loss: 51.6574 - custom_mse: 193542.4219\n",
            "reduce_confident_loss =  0.858480275\n",
            "\n",
            "reduce_mean_cls_loss =  0.314975202\n",
            "\n",
            "regression loss =  13.8014545\n",
            "58/73 [======================>.......] - ETA: 6:48 - loss: 51.0250 - custom_mse: 193746.0000\n",
            "reduce_confident_loss =  0.766883731\n",
            "\n",
            "reduce_mean_cls_loss =  0.379267484\n",
            "\n",
            "regression loss =  34.8221588\n",
            "59/73 [=======================>......] - ETA: 6:20 - loss: 50.7698 - custom_mse: 194213.4062\n",
            "reduce_confident_loss =  0.749788284\n",
            "\n",
            "reduce_mean_cls_loss =  0.614267647\n",
            "\n",
            "regression loss =  34.280735\n",
            "60/73 [=======================>......] - ETA: 5:52 - loss: 50.5177 - custom_mse: 194407.0781\n",
            "reduce_confident_loss =  0.971436799\n",
            "\n",
            "reduce_mean_cls_loss =  0.31781128\n",
            "\n",
            "regression loss =  5.09420395\n",
            "61/73 [========================>.....] - ETA: 5:25 - loss: 49.7942 - custom_mse: 194479.4531\n",
            "reduce_confident_loss =  0.966613173\n",
            "\n",
            "reduce_mean_cls_loss =  0.318147868\n",
            "\n",
            "regression loss =  64.9253159\n",
            "62/73 [========================>.....] - ETA: 4:57 - loss: 50.0590 - custom_mse: 194532.7500\n",
            "reduce_confident_loss =  0.681734622\n",
            "\n",
            "reduce_mean_cls_loss =  0.667466044\n",
            "\n",
            "regression loss =  90.1829758\n",
            "63/73 [========================>.....] - ETA: 4:30 - loss: 50.7173 - custom_mse: 194875.4531\n",
            "reduce_confident_loss =  0.916160226\n",
            "\n",
            "reduce_mean_cls_loss =  0.528806627\n",
            "\n",
            "regression loss =  32.9788\n",
            "64/73 [=========================>....] - ETA: 4:02 - loss: 50.4627 - custom_mse: 195512.5781\n",
            "reduce_confident_loss =  0.965841889\n",
            "\n",
            "reduce_mean_cls_loss =  0.314337224\n",
            "\n",
            "regression loss =  12.6654301\n",
            "65/73 [=========================>....] - ETA: 3:35 - loss: 49.9009 - custom_mse: 196192.5000\n",
            "reduce_confident_loss =  0.97072804\n",
            "\n",
            "reduce_mean_cls_loss =  0.314001828\n",
            "\n",
            "regression loss =  17.0252972\n",
            "66/73 [==========================>...] - ETA: 3:08 - loss: 49.4222 - custom_mse: 196933.7812\n",
            "reduce_confident_loss =  0.963906169\n",
            "\n",
            "reduce_mean_cls_loss =  0.314458907\n",
            "\n",
            "regression loss =  17.7999744\n",
            "67/73 [==========================>...] - ETA: 2:41 - loss: 48.9693 - custom_mse: 197750.5312\n",
            "reduce_confident_loss =  0.962619305\n",
            "\n",
            "reduce_mean_cls_loss =  0.314018786\n",
            "\n",
            "regression loss =  15.691824\n",
            "68/73 [==========================>...] - ETA: 2:14 - loss: 48.4987 - custom_mse: 198553.3125\n",
            "reduce_confident_loss =  0.956348717\n",
            "\n",
            "reduce_mean_cls_loss =  0.321339399\n",
            "\n",
            "regression loss =  10.813364\n",
            "69/73 [===========================>..] - ETA: 1:47 - loss: 47.9711 - custom_mse: 199326.0156\n",
            "reduce_confident_loss =  0.956369519\n",
            "\n",
            "reduce_mean_cls_loss =  0.313728511\n",
            "\n",
            "regression loss =  9.71516705\n",
            "70/73 [===========================>..] - ETA: 1:20 - loss: 47.4427 - custom_mse: 199548.8125\n",
            "reduce_confident_loss =  0.961015642\n",
            "\n",
            "reduce_mean_cls_loss =  0.314870209\n",
            "\n",
            "regression loss =  8.12050819\n",
            "71/73 [============================>.] - ETA: 53s - loss: 46.9069 - custom_mse: 199656.7656 \n",
            "reduce_confident_loss =  0.825933039\n",
            "\n",
            "reduce_mean_cls_loss =  0.316129655\n",
            "\n",
            "regression loss =  12.0481653\n",
            "72/73 [============================>.] - ETA: 26s - loss: 46.4386 - custom_mse: 200009.7031\n",
            "reduce_confident_loss =  0.926948547\n",
            "\n",
            "reduce_mean_cls_loss =  0.313994735\n",
            "\n",
            "regression loss =  16.5022621\n",
            "73/73 [==============================] - ETA: 0s - loss: 46.2407 - custom_mse: 200482.5156 \n",
            "reduce_confident_loss =  0.95979017\n",
            "\n",
            "reduce_mean_cls_loss =  0.336888641\n",
            "\n",
            "regression loss =  4.55815697\n",
            "\n",
            "reduce_confident_loss =  0.898557544\n",
            "\n",
            "reduce_mean_cls_loss =  0.331578225\n",
            "\n",
            "regression loss =  6.77382612\n",
            "\n",
            "reduce_confident_loss =  0.918991506\n",
            "\n",
            "reduce_mean_cls_loss =  0.332952529\n",
            "\n",
            "regression loss =  2.28348327\n",
            "\n",
            "reduce_confident_loss =  0.857719302\n",
            "\n",
            "reduce_mean_cls_loss =  0.328232497\n",
            "\n",
            "regression loss =  13.774744\n",
            "\n",
            "reduce_confident_loss =  0.674068928\n",
            "\n",
            "reduce_mean_cls_loss =  0.514100075\n",
            "\n",
            "regression loss =  49.8145523\n",
            "\n",
            "reduce_confident_loss =  0.592459142\n",
            "\n",
            "reduce_mean_cls_loss =  0.591281056\n",
            "\n",
            "regression loss =  27.1927719\n",
            "\n",
            "reduce_confident_loss =  0.959797204\n",
            "\n",
            "reduce_mean_cls_loss =  0.335982472\n",
            "\n",
            "regression loss =  2.31281424\n",
            "\n",
            "reduce_confident_loss =  0.959814847\n",
            "\n",
            "reduce_mean_cls_loss =  0.33508873\n",
            "\n",
            "regression loss =  2.40443778\n",
            "\n",
            "reduce_confident_loss =  0.959793031\n",
            "\n",
            "reduce_mean_cls_loss =  0.336411953\n",
            "\n",
            "regression loss =  2.48275423\n",
            "\n",
            "reduce_confident_loss =  0.959827244\n",
            "\n",
            "reduce_mean_cls_loss =  0.336656719\n",
            "\n",
            "regression loss =  2.58933902\n",
            "\n",
            "reduce_confident_loss =  0.959813416\n",
            "\n",
            "reduce_mean_cls_loss =  0.337228686\n",
            "\n",
            "regression loss =  5.34755611\n",
            "\n",
            "reduce_confident_loss =  0.89855206\n",
            "\n",
            "reduce_mean_cls_loss =  0.331959873\n",
            "\n",
            "regression loss =  4.19713163\n",
            "\n",
            "reduce_confident_loss =  0.959871411\n",
            "\n",
            "reduce_mean_cls_loss =  0.335692823\n",
            "\n",
            "regression loss =  1.10713434\n",
            "\n",
            "reduce_confident_loss =  0.959828198\n",
            "\n",
            "reduce_mean_cls_loss =  0.728840947\n",
            "\n",
            "regression loss =  64.7282486\n",
            "73/73 [==============================] - 2106s 29s/step - loss: 46.2407 - custom_mse: 200482.5156 - val_loss: 13.1643 - val_custom_mse: 8474.0352\n",
            "Epoch 3/4\n",
            "\n",
            "reduce_confident_loss =  0.75503695\n",
            "\n",
            "reduce_mean_cls_loss =  1.00267708\n",
            "\n",
            "regression loss =  104.097717\n",
            " 1/73 [..............................] - ETA: 47:58 - loss: 105.8554 - custom_mse: 2935.1631\n",
            "reduce_confident_loss =  0.83683461\n",
            "\n",
            "reduce_mean_cls_loss =  0.59257412\n",
            "\n",
            "regression loss =  45.7719955\n",
            " 2/73 [..............................] - ETA: 32:15 - loss: 76.5284 - custom_mse: 3024.6313 \n",
            "reduce_confident_loss =  0.6776281\n",
            "\n",
            "reduce_mean_cls_loss =  0.442140907\n",
            "\n",
            "regression loss =  32.4084854\n",
            " 3/73 [>.............................] - ETA: 31:21 - loss: 62.1950 - custom_mse: 3032.9421\n",
            "reduce_confident_loss =  0.678070962\n",
            "\n",
            "reduce_mean_cls_loss =  0.441174299\n",
            "\n",
            "regression loss =  31.5422611\n",
            " 4/73 [>.............................] - ETA: 30:46 - loss: 54.8117 - custom_mse: 3044.8772\n",
            "reduce_confident_loss =  0.683375835\n",
            "\n",
            "reduce_mean_cls_loss =  0.444182783\n",
            "\n",
            "regression loss =  29.2888451\n",
            " 5/73 [=>............................] - ETA: 30:12 - loss: 49.9326 - custom_mse: 3065.0798\n",
            "reduce_confident_loss =  0.678623736\n",
            "\n",
            "reduce_mean_cls_loss =  0.442517072\n",
            "\n",
            "regression loss =  27.6783829\n",
            " 6/73 [=>............................] - ETA: 29:43 - loss: 46.4104 - custom_mse: 3102.7725\n",
            "reduce_confident_loss =  0.675628\n",
            "\n",
            "reduce_mean_cls_loss =  0.459580749\n",
            "\n",
            "regression loss =  41.9556084\n",
            " 7/73 [=>............................] - ETA: 29:16 - loss: 45.9362 - custom_mse: 3148.9111\n",
            "reduce_confident_loss =  0.794765472\n",
            "\n",
            "reduce_mean_cls_loss =  0.37023443\n",
            "\n",
            "regression loss =  20.5360394\n",
            " 8/73 [==>...........................] - ETA: 29:00 - loss: 42.9068 - custom_mse: 3260.0935\n",
            "reduce_confident_loss =  0.947923541\n",
            "\n",
            "reduce_mean_cls_loss =  0.325545698\n",
            "\n",
            "regression loss =  8.97569752\n",
            " 9/73 [==>...........................] - ETA: 28:29 - loss: 39.2782 - custom_mse: 3384.8789\n",
            "reduce_confident_loss =  0.964567125\n",
            "\n",
            "reduce_mean_cls_loss =  0.316635698\n",
            "\n",
            "regression loss =  9.18348598\n",
            "10/73 [===>..........................] - ETA: 27:58 - loss: 36.3968 - custom_mse: 3508.3628\n",
            "reduce_confident_loss =  0.964127958\n",
            "\n",
            "reduce_mean_cls_loss =  0.332337618\n",
            "\n",
            "regression loss =  21.5862808\n",
            "11/73 [===>..........................] - ETA: 27:31 - loss: 35.1683 - custom_mse: 3703.3440\n",
            "reduce_confident_loss =  0.973967135\n",
            "\n",
            "reduce_mean_cls_loss =  0.332373887\n",
            "\n",
            "regression loss =  25.6555729\n",
            "12/73 [===>..........................] - ETA: 27:02 - loss: 34.4844 - custom_mse: 4288.7852\n",
            "reduce_confident_loss =  0.98630625\n",
            "\n",
            "reduce_mean_cls_loss =  0.325194299\n",
            "\n",
            "regression loss =  10.6162014\n",
            "13/73 [====>.........................] - ETA: 26:33 - loss: 32.7493 - custom_mse: 5140.8887\n",
            "reduce_confident_loss =  0.966757715\n",
            "\n",
            "reduce_mean_cls_loss =  0.318440855\n",
            "\n",
            "regression loss =  7.56604433\n",
            "14/73 [====>.........................] - ETA: 26:03 - loss: 31.0423 - custom_mse: 6006.1875\n",
            "reduce_confident_loss =  0.979039609\n",
            "\n",
            "reduce_mean_cls_loss =  0.317834407\n",
            "\n",
            "regression loss =  7.31183243\n",
            "15/73 [=====>........................] - ETA: 25:35 - loss: 29.5467 - custom_mse: 6885.3633\n",
            "reduce_confident_loss =  0.969475091\n",
            "\n",
            "reduce_mean_cls_loss =  0.33665514\n",
            "\n",
            "regression loss =  5.88599586\n",
            "16/73 [=====>........................] - ETA: 25:06 - loss: 28.1495 - custom_mse: 7791.2124\n",
            "reduce_confident_loss =  0.978387892\n",
            "\n",
            "reduce_mean_cls_loss =  0.318543434\n",
            "\n",
            "regression loss =  26.7882614\n",
            "17/73 [=====>........................] - ETA: 24:38 - loss: 28.1458 - custom_mse: 8687.5576\n",
            "reduce_confident_loss =  0.682693481\n",
            "\n",
            "reduce_mean_cls_loss =  0.473082572\n",
            "\n",
            "regression loss =  53.1644554\n",
            "18/73 [======>.......................] - ETA: 24:11 - loss: 29.5999 - custom_mse: 9197.2871\n",
            "reduce_confident_loss =  0.676697433\n",
            "\n",
            "reduce_mean_cls_loss =  0.518902659\n",
            "\n",
            "regression loss =  57.4635658\n",
            "19/73 [======>.......................] - ETA: 25:25 - loss: 31.1293 - custom_mse: 9425.3877\n",
            "reduce_confident_loss =  0.699297547\n",
            "\n",
            "reduce_mean_cls_loss =  0.44618696\n",
            "\n",
            "regression loss =  55.595314\n",
            "20/73 [=======>......................] - ETA: 24:51 - loss: 32.4099 - custom_mse: 9443.2227\n",
            "reduce_confident_loss =  0.684125781\n",
            "\n",
            "reduce_mean_cls_loss =  0.444734246\n",
            "\n",
            "regression loss =  41.910759\n",
            "21/73 [=======>......................] - ETA: 24:20 - loss: 32.9161 - custom_mse: 9457.8672\n",
            "reduce_confident_loss =  0.679661214\n",
            "\n",
            "reduce_mean_cls_loss =  0.443927139\n",
            "\n",
            "regression loss =  51.9041328\n",
            "22/73 [========>.....................] - ETA: 23:47 - loss: 33.8302 - custom_mse: 9583.5186\n",
            "reduce_confident_loss =  0.758543313\n",
            "\n",
            "reduce_mean_cls_loss =  0.36831513\n",
            "\n",
            "regression loss =  22.9628754\n",
            "23/73 [========>.....................] - ETA: 23:17 - loss: 33.4067 - custom_mse: 9642.1250\n",
            "reduce_confident_loss =  0.820400774\n",
            "\n",
            "reduce_mean_cls_loss =  0.31414932\n",
            "\n",
            "regression loss =  16.6624851\n",
            "24/73 [========>.....................] - ETA: 22:49 - loss: 32.7563 - custom_mse: 9740.5225\n",
            "reduce_confident_loss =  0.818674922\n",
            "\n",
            "reduce_mean_cls_loss =  0.314024955\n",
            "\n",
            "regression loss =  16.2331161\n",
            "25/73 [=========>....................] - ETA: 22:30 - loss: 32.1407 - custom_mse: 9846.6787\n",
            "reduce_confident_loss =  0.961070478\n",
            "\n",
            "reduce_mean_cls_loss =  0.315524817\n",
            "\n",
            "regression loss =  29.9549427\n",
            "26/73 [=========>....................] - ETA: 21:57 - loss: 32.1058 - custom_mse: 10154.9346\n",
            "reduce_confident_loss =  0.675104439\n",
            "\n",
            "reduce_mean_cls_loss =  0.442829579\n",
            "\n",
            "regression loss =  17.6926556\n",
            "27/73 [==========>...................] - ETA: 21:25 - loss: 31.6133 - custom_mse: 10874.2598\n",
            "reduce_confident_loss =  0.675602615\n",
            "\n",
            "reduce_mean_cls_loss =  0.441906869\n",
            "\n",
            "regression loss =  10.0870266\n",
            "28/73 [==========>...................] - ETA: 20:53 - loss: 30.8845 - custom_mse: 11270.5361\n",
            "reduce_confident_loss =  0.67391479\n",
            "\n",
            "reduce_mean_cls_loss =  0.440465868\n",
            "\n",
            "regression loss =  9.01795387\n",
            "29/73 [==========>...................] - ETA: 20:21 - loss: 30.1689 - custom_mse: 11580.1943\n",
            "reduce_confident_loss =  0.676211238\n",
            "\n",
            "reduce_mean_cls_loss =  0.435456336\n",
            "\n",
            "regression loss =  12.7092\n",
            "30/73 [===========>..................] - ETA: 19:51 - loss: 29.6239 - custom_mse: 12016.4668\n",
            "reduce_confident_loss =  0.675374031\n",
            "\n",
            "reduce_mean_cls_loss =  0.45882383\n",
            "\n",
            "regression loss =  16.4565887\n",
            "31/73 [===========>..................] - ETA: 19:20 - loss: 29.2358 - custom_mse: 12604.7490\n",
            "reduce_confident_loss =  0.674520433\n",
            "\n",
            "reduce_mean_cls_loss =  0.481515676\n",
            "\n",
            "regression loss =  14.9328165\n",
            "32/73 [============>.................] - ETA: 18:51 - loss: 28.8249 - custom_mse: 13439.3164\n",
            "reduce_confident_loss =  0.816457927\n",
            "\n",
            "reduce_mean_cls_loss =  0.314600974\n",
            "\n",
            "regression loss =  8.77711201\n",
            "33/73 [============>.................] - ETA: 18:21 - loss: 28.2517 - custom_mse: 13447.0947\n",
            "reduce_confident_loss =  0.959808707\n",
            "\n",
            "reduce_mean_cls_loss =  0.314173788\n",
            "\n",
            "regression loss =  8.82805157\n",
            "34/73 [============>.................] - ETA: 17:51 - loss: 27.7179 - custom_mse: 13487.2324\n",
            "reduce_confident_loss =  0.959095061\n",
            "\n",
            "reduce_mean_cls_loss =  0.316706508\n",
            "\n",
            "regression loss =  6.7662487\n",
            "35/73 [=============>................] - ETA: 17:21 - loss: 27.1557 - custom_mse: 13580.7334\n",
            "reduce_confident_loss =  0.959362864\n",
            "\n",
            "reduce_mean_cls_loss =  0.316061825\n",
            "\n",
            "regression loss =  63.2518768\n",
            "36/73 [=============>................] - ETA: 16:56 - loss: 28.1938 - custom_mse: 22143.6641\n",
            "reduce_confident_loss =  0.940670431\n",
            "\n",
            "reduce_mean_cls_loss =  1.23611152\n",
            "\n",
            "regression loss =  104.219856\n",
            "37/73 [==============>...............] - ETA: 16:27 - loss: 30.3074 - custom_mse: 30631.2461\n",
            "reduce_confident_loss =  0.8381868\n",
            "\n",
            "reduce_mean_cls_loss =  0.744434655\n",
            "\n",
            "regression loss =  75.6388702\n",
            "38/73 [==============>...............] - ETA: 16:01 - loss: 31.5420 - custom_mse: 34663.3945\n",
            "reduce_confident_loss =  0.959376454\n",
            "\n",
            "reduce_mean_cls_loss =  1.26401019\n",
            "\n",
            "regression loss =  105.77314\n",
            "39/73 [===============>..............] - ETA: 15:32 - loss: 33.5024 - custom_mse: 44036.8984\n",
            "reduce_confident_loss =  0.67802459\n",
            "\n",
            "reduce_mean_cls_loss =  1.09450269\n",
            "\n",
            "regression loss =  111.811447\n",
            "40/73 [===============>..............] - ETA: 15:03 - loss: 35.5044 - custom_mse: 52798.7383\n",
            "reduce_confident_loss =  0.959902108\n",
            "\n",
            "reduce_mean_cls_loss =  1.29938149\n",
            "\n",
            "regression loss =  155.020798\n",
            "41/73 [===============>..............] - ETA: 14:37 - loss: 38.4745 - custom_mse: 68783.7891\n",
            "reduce_confident_loss =  0.758546174\n",
            "\n",
            "reduce_mean_cls_loss =  1.22195423\n",
            "\n",
            "regression loss =  171.361191\n",
            "42/73 [================>.............] - ETA: 14:08 - loss: 41.6857 - custom_mse: 87364.9375\n",
            "reduce_confident_loss =  0.898094356\n",
            "\n",
            "reduce_mean_cls_loss =  1.30407417\n",
            "\n",
            "regression loss =  139.213867\n",
            "43/73 [================>.............] - ETA: 13:52 - loss: 44.0050 - custom_mse: 102215.3047\n",
            "reduce_confident_loss =  0.960428715\n",
            "\n",
            "reduce_mean_cls_loss =  1.29089344\n",
            "\n",
            "regression loss =  17.7412567\n",
            "44/73 [=================>............] - ETA: 13:23 - loss: 43.4592 - custom_mse: 103416.0703\n",
            "reduce_confident_loss =  0.858489335\n",
            "\n",
            "reduce_mean_cls_loss =  1.31031334\n",
            "\n",
            "regression loss =  23.043644\n",
            "45/73 [=================>............] - ETA: 12:54 - loss: 43.0537 - custom_mse: 104794.9062\n",
            "reduce_confident_loss =  0.962553442\n",
            "\n",
            "reduce_mean_cls_loss =  1.28162396\n",
            "\n",
            "regression loss =  36.6023865\n",
            "46/73 [=================>............] - ETA: 12:27 - loss: 42.9623 - custom_mse: 107161.0234\n",
            "reduce_confident_loss =  0.960955739\n",
            "\n",
            "reduce_mean_cls_loss =  1.30044639\n",
            "\n",
            "regression loss =  60.9680595\n",
            "47/73 [==================>...........] - ETA: 11:58 - loss: 43.3935 - custom_mse: 110959.1719\n",
            "reduce_confident_loss =  0.961326122\n",
            "\n",
            "reduce_mean_cls_loss =  0.73371166\n",
            "\n",
            "regression loss =  27.2761879\n",
            "48/73 [==================>...........] - ETA: 11:30 - loss: 43.0930 - custom_mse: 112756.6328\n",
            "reduce_confident_loss =  0.674044728\n",
            "\n",
            "reduce_mean_cls_loss =  0.440380245\n",
            "\n",
            "regression loss =  18.3600712\n",
            "\n",
            "reduce_confident_loss =  0.679735541\n",
            "\n",
            "reduce_mean_cls_loss =  0.446689337\n",
            "\n",
            "regression loss =  14.1612825\n",
            "50/73 [===================>..........] - ETA: 10:33 - loss: 42.0646 - custom_mse: 114200.7891\n",
            "reduce_confident_loss =  0.959602952\n",
            "\n",
            "reduce_mean_cls_loss =  0.322667181\n",
            "\n",
            "regression loss =  5.46976614\n",
            "51/73 [===================>..........] - ETA: 10:04 - loss: 41.3722 - custom_mse: 114522.8828\n",
            "reduce_confident_loss =  0.974620283\n",
            "\n",
            "reduce_mean_cls_loss =  0.317123592\n",
            "\n",
            "regression loss =  4.19388437\n",
            "52/73 [====================>.........] - ETA: 9:36 - loss: 40.6820 - custom_mse: 114826.6406 \n",
            "reduce_confident_loss =  0.962527871\n",
            "\n",
            "reduce_mean_cls_loss =  0.319457263\n",
            "\n",
            "regression loss =  47.2994232\n",
            "53/73 [====================>.........] - ETA: 9:08 - loss: 40.8311 - custom_mse: 116632.3203\n",
            "reduce_confident_loss =  0.961818695\n",
            "\n",
            "reduce_mean_cls_loss =  1.29585624\n",
            "\n",
            "regression loss =  154.176\n",
            "54/73 [=====================>........] - ETA: 8:40 - loss: 42.9719 - custom_mse: 142554.2031\n",
            "reduce_confident_loss =  0.964346588\n",
            "\n",
            "reduce_mean_cls_loss =  1.30851686\n",
            "\n",
            "regression loss =  145.20668\n",
            "55/73 [=====================>........] - ETA: 8:11 - loss: 44.8720 - custom_mse: 167408.5156\n",
            "reduce_confident_loss =  0.960507095\n",
            "\n",
            "reduce_mean_cls_loss =  1.27675223\n",
            "\n",
            "regression loss =  137.877502\n",
            "56/73 [======================>.......] - ETA: 7:43 - loss: 46.5728 - custom_mse: 191023.6562\n",
            "reduce_confident_loss =  0.963739216\n",
            "\n",
            "reduce_mean_cls_loss =  1.02377236\n",
            "\n",
            "regression loss =  49.1775703\n",
            "57/73 [======================>.......] - ETA: 7:16 - loss: 46.6533 - custom_mse: 198413.0781\n",
            "reduce_confident_loss =  0.85827446\n",
            "\n",
            "reduce_mean_cls_loss =  0.31390354\n",
            "\n",
            "regression loss =  8.40396\n",
            "58/73 [======================>.......] - ETA: 6:48 - loss: 46.0141 - custom_mse: 198947.0000\n",
            "reduce_confident_loss =  0.765107691\n",
            "\n",
            "reduce_mean_cls_loss =  0.368840039\n",
            "\n",
            "regression loss =  31.7122822\n",
            "59/73 [=======================>......] - ETA: 6:20 - loss: 45.7909 - custom_mse: 199913.9844\n",
            "reduce_confident_loss =  0.7364766\n",
            "\n",
            "reduce_mean_cls_loss =  0.618856966\n",
            "\n",
            "regression loss =  30.1806297\n",
            "60/73 [=======================>......] - ETA: 5:52 - loss: 45.5533 - custom_mse: 200372.9844\n",
            "reduce_confident_loss =  0.963389695\n",
            "\n",
            "reduce_mean_cls_loss =  0.314885646\n",
            "\n",
            "regression loss =  7.53228855\n",
            "61/73 [========================>.....] - ETA: 5:24 - loss: 44.9510 - custom_mse: 200390.5625\n",
            "reduce_confident_loss =  0.95955658\n",
            "\n",
            "reduce_mean_cls_loss =  0.317585796\n",
            "\n",
            "regression loss =  62.5833397\n",
            "62/73 [========================>.....] - ETA: 4:57 - loss: 45.2560 - custom_mse: 200489.7969\n",
            "reduce_confident_loss =  0.675463378\n",
            "\n",
            "reduce_mean_cls_loss =  0.694869339\n",
            "\n",
            "regression loss =  86.179863\n",
            "63/73 [========================>.....] - ETA: 4:30 - loss: 45.9273 - custom_mse: 201186.8594\n",
            "reduce_confident_loss =  0.898340344\n",
            "\n",
            "reduce_mean_cls_loss =  0.527957618\n",
            "\n",
            "regression loss =  36.0771408\n",
            "64/73 [=========================>....] - ETA: 4:02 - loss: 45.7957 - custom_mse: 202426.4531\n",
            "reduce_confident_loss =  0.959721088\n",
            "\n",
            "reduce_mean_cls_loss =  0.314770162\n",
            "\n",
            "regression loss =  20.3730946\n",
            "65/73 [=========================>....] - ETA: 3:35 - loss: 45.4242 - custom_mse: 203742.8594\n",
            "reduce_confident_loss =  0.959963381\n",
            "\n",
            "reduce_mean_cls_loss =  0.313799798\n",
            "\n",
            "regression loss =  21.8829098\n",
            "66/73 [==========================>...] - ETA: 3:08 - loss: 45.0868 - custom_mse: 205036.0000\n",
            "reduce_confident_loss =  0.959233761\n",
            "\n",
            "reduce_mean_cls_loss =  0.314517915\n",
            "\n",
            "regression loss =  20.4306545\n",
            "67/73 [==========================>...] - ETA: 2:41 - loss: 44.7378 - custom_mse: 206469.2500\n",
            "reduce_confident_loss =  0.963709\n",
            "\n",
            "reduce_mean_cls_loss =  0.31769684\n",
            "\n",
            "regression loss =  19.048914\n",
            "68/73 [==========================>...] - ETA: 2:14 - loss: 44.3789 - custom_mse: 207882.7344\n",
            "reduce_confident_loss =  0.96039331\n",
            "\n",
            "reduce_mean_cls_loss =  0.317718089\n",
            "\n",
            "regression loss =  17.0259686\n",
            "69/73 [===========================>..] - ETA: 1:47 - loss: 44.0010 - custom_mse: 209195.5938\n",
            "reduce_confident_loss =  0.95926559\n",
            "\n",
            "reduce_mean_cls_loss =  0.314383239\n",
            "\n",
            "regression loss =  10.8159389\n",
            "70/73 [===========================>..] - ETA: 1:20 - loss: 43.5451 - custom_mse: 209743.7188\n",
            "reduce_confident_loss =  0.961818039\n",
            "\n",
            "reduce_mean_cls_loss =  0.313441753\n",
            "\n",
            "regression loss =  12.1393967\n",
            "71/73 [============================>.] - ETA: 53s - loss: 43.1207 - custom_mse: 210109.1719 \n",
            "reduce_confident_loss =  0.817829\n",
            "\n",
            "reduce_mean_cls_loss =  0.313401163\n",
            "\n",
            "regression loss =  15.0182467\n",
            "72/73 [============================>.] - ETA: 26s - loss: 42.7461 - custom_mse: 210796.5781\n",
            "reduce_confident_loss =  0.922781944\n",
            "\n",
            "reduce_mean_cls_loss =  0.31369552\n",
            "\n",
            "regression loss =  10.6806059\n",
            "73/73 [==============================] - ETA: 0s - loss: 42.5335 - custom_mse: 211671.0781 \n",
            "reduce_confident_loss =  0.959142327\n",
            "\n",
            "reduce_mean_cls_loss =  0.317215443\n",
            "\n",
            "regression loss =  14.4831095\n",
            "\n",
            "reduce_confident_loss =  0.897914\n",
            "\n",
            "reduce_mean_cls_loss =  0.316219956\n",
            "\n",
            "regression loss =  11.7162075\n",
            "\n",
            "reduce_confident_loss =  0.918335736\n",
            "\n",
            "reduce_mean_cls_loss =  0.316524178\n",
            "\n",
            "regression loss =  11.8986626\n",
            "\n",
            "reduce_confident_loss =  0.857098877\n",
            "\n",
            "reduce_mean_cls_loss =  0.315644234\n",
            "\n",
            "regression loss =  14.9330816\n",
            "\n",
            "reduce_confident_loss =  0.673436821\n",
            "\n",
            "reduce_mean_cls_loss =  0.510037482\n",
            "\n",
            "regression loss =  45.514679\n",
            "\n",
            "reduce_confident_loss =  0.59180212\n",
            "\n",
            "reduce_mean_cls_loss =  0.586697161\n",
            "\n",
            "regression loss =  29.0170765\n",
            "\n",
            "reduce_confident_loss =  0.959148288\n",
            "\n",
            "reduce_mean_cls_loss =  0.316983759\n",
            "\n",
            "regression loss =  12.356576\n",
            "\n",
            "reduce_confident_loss =  0.959152758\n",
            "\n",
            "reduce_mean_cls_loss =  0.316864759\n",
            "\n",
            "regression loss =  12.3769007\n",
            "\n",
            "reduce_confident_loss =  0.95914185\n",
            "\n",
            "reduce_mean_cls_loss =  0.316936225\n",
            "\n",
            "regression loss =  12.469347\n",
            "\n",
            "reduce_confident_loss =  0.959148824\n",
            "\n",
            "reduce_mean_cls_loss =  0.317003578\n",
            "\n",
            "regression loss =  12.5424471\n",
            "\n",
            "reduce_confident_loss =  0.959156573\n",
            "\n",
            "reduce_mean_cls_loss =  0.31721884\n",
            "\n",
            "regression loss =  12.354867\n",
            "\n",
            "reduce_confident_loss =  0.897921085\n",
            "\n",
            "reduce_mean_cls_loss =  0.316432267\n",
            "\n",
            "regression loss =  12.0143\n",
            "\n",
            "reduce_confident_loss =  0.959152043\n",
            "\n",
            "reduce_mean_cls_loss =  0.316989422\n",
            "\n",
            "regression loss =  9.89403248\n",
            "\n",
            "reduce_confident_loss =  0.959143162\n",
            "\n",
            "reduce_mean_cls_loss =  0.739465475\n",
            "\n",
            "regression loss =  73.1086578\n",
            "73/73 [==============================] - 2101s 29s/step - loss: 42.5335 - custom_mse: 211671.0781 - val_loss: 19.8917 - val_custom_mse: 12801.6982\n",
            "Epoch 4/4\n",
            "\n",
            "reduce_confident_loss =  0.755689502\n",
            "\n",
            "reduce_mean_cls_loss =  1.00379443\n",
            "\n",
            "regression loss =  102.62513\n",
            " 1/73 [..............................] - ETA: 48:09 - loss: 104.3846 - custom_mse: 3492.7695\n",
            "reduce_confident_loss =  0.836618721\n",
            "\n",
            "reduce_mean_cls_loss =  0.588701367\n",
            "\n",
            "regression loss =  48.281498\n",
            " 2/73 [..............................] - ETA: 32:42 - loss: 77.0457 - custom_mse: 3697.2908 \n",
            "reduce_confident_loss =  0.677003\n",
            "\n",
            "reduce_mean_cls_loss =  0.440565526\n",
            "\n",
            "regression loss =  34.8296509\n",
            " 3/73 [>.............................] - ETA: 31:21 - loss: 63.3462 - custom_mse: 3765.5151\n",
            "reduce_confident_loss =  0.673969686\n",
            "\n",
            "reduce_mean_cls_loss =  0.441730708\n",
            "\n",
            "regression loss =  33.6530228\n",
            " 4/73 [>.............................] - ETA: 30:37 - loss: 56.2018 - custom_mse: 3900.3284\n",
            "reduce_confident_loss =  0.674274802\n",
            "\n",
            "reduce_mean_cls_loss =  0.442081183\n",
            "\n",
            "regression loss =  30.88694\n",
            " 5/73 [=>............................] - ETA: 30:05 - loss: 51.3621 - custom_mse: 4045.4075\n",
            "reduce_confident_loss =  0.678224266\n",
            "\n",
            "reduce_mean_cls_loss =  0.440018415\n",
            "\n",
            "regression loss =  29.8442612\n",
            " 6/73 [=>............................] - ETA: 29:36 - loss: 47.9622 - custom_mse: 4202.6128\n",
            "reduce_confident_loss =  0.675025105\n",
            "\n",
            "reduce_mean_cls_loss =  0.461071223\n",
            "\n",
            "regression loss =  41.1625099\n",
            " 7/73 [=>............................] - ETA: 29:06 - loss: 47.1531 - custom_mse: 4394.8223\n",
            "reduce_confident_loss =  0.77982533\n",
            "\n",
            "reduce_mean_cls_loss =  0.368042886\n",
            "\n",
            "regression loss =  26.9527225\n",
            " 8/73 [==>...........................] - ETA: 28:52 - loss: 44.7715 - custom_mse: 4430.6943\n",
            "reduce_confident_loss =  0.943587601\n",
            "\n",
            "reduce_mean_cls_loss =  0.314754337\n",
            "\n",
            "regression loss =  8.84322548\n",
            " 9/73 [==>...........................] - ETA: 28:19 - loss: 40.9193 - custom_mse: 4459.2754\n",
            "reduce_confident_loss =  0.961240888\n",
            "\n",
            "reduce_mean_cls_loss =  0.314959526\n",
            "\n",
            "regression loss =  7.66831064\n",
            "10/73 [===>..........................] - ETA: 27:50 - loss: 37.7218 - custom_mse: 4489.6089\n",
            "reduce_confident_loss =  0.959801257\n",
            "\n",
            "reduce_mean_cls_loss =  0.319808722\n",
            "\n",
            "regression loss =  28.1495972\n",
            "11/73 [===>..........................] - ETA: 27:21 - loss: 36.9680 - custom_mse: 4671.4517\n",
            "reduce_confident_loss =  0.962415159\n",
            "\n",
            "reduce_mean_cls_loss =  0.317493737\n",
            "\n",
            "regression loss =  35.2513199\n",
            "12/73 [===>..........................] - ETA: 26:51 - loss: 36.9316 - custom_mse: 5440.9175\n",
            "reduce_confident_loss =  0.961500049\n",
            "\n",
            "reduce_mean_cls_loss =  0.320649296\n",
            "\n",
            "regression loss =  12.779707\n",
            "13/73 [====>.........................] - ETA: 26:26 - loss: 35.1724 - custom_mse: 6623.9268\n",
            "reduce_confident_loss =  0.960101247\n",
            "\n",
            "reduce_mean_cls_loss =  0.314525336\n",
            "\n",
            "regression loss =  3.6187892\n",
            "14/73 [====>.........................] - ETA: 25:58 - loss: 33.0096 - custom_mse: 7977.8223\n",
            "reduce_confident_loss =  0.960779488\n",
            "\n",
            "reduce_mean_cls_loss =  0.314251155\n",
            "\n",
            "regression loss =  2.48189235\n",
            "15/73 [=====>........................] - ETA: 25:29 - loss: 31.0594 - custom_mse: 9256.3145\n",
            "reduce_confident_loss =  0.959833801\n",
            "\n",
            "reduce_mean_cls_loss =  0.315170437\n",
            "\n",
            "regression loss =  3.79665542\n",
            "16/73 [=====>........................] - ETA: 25:01 - loss: 29.4352 - custom_mse: 10581.7852\n",
            "reduce_confident_loss =  0.962378919\n",
            "\n",
            "reduce_mean_cls_loss =  0.314184397\n",
            "\n",
            "regression loss =  23.0564766\n",
            "17/73 [=====>........................] - ETA: 24:34 - loss: 29.1350 - custom_mse: 11887.8398\n",
            "reduce_confident_loss =  0.674464226\n",
            "\n",
            "reduce_mean_cls_loss =  0.468205869\n",
            "\n",
            "regression loss =  51.1699\n",
            "18/73 [======>.......................] - ETA: 24:06 - loss: 30.4227 - custom_mse: 12713.2725\n",
            "reduce_confident_loss =  0.678803265\n",
            "\n",
            "reduce_mean_cls_loss =  0.505531967\n",
            "\n",
            "regression loss =  55.9445648\n",
            "19/73 [======>.......................] - ETA: 25:18 - loss: 31.8283 - custom_mse: 13179.5908\n",
            "reduce_confident_loss =  0.677959859\n",
            "\n",
            "reduce_mean_cls_loss =  0.440553039\n",
            "\n",
            "regression loss =  43.8408585\n",
            "20/73 [=======>......................] - ETA: 24:44 - loss: 32.4848 - custom_mse: 13202.0146\n",
            "reduce_confident_loss =  0.674660861\n",
            "\n",
            "reduce_mean_cls_loss =  0.441006333\n",
            "\n",
            "regression loss =  36.6645584\n",
            "21/73 [=======>......................] - ETA: 24:11 - loss: 32.7370 - custom_mse: 13263.5488\n",
            "reduce_confident_loss =  0.674581707\n",
            "\n",
            "reduce_mean_cls_loss =  0.440215141\n",
            "\n",
            "regression loss =  38.4538765\n",
            "22/73 [========>.....................] - ETA: 23:38 - loss: 33.0475 - custom_mse: 13603.1348\n",
            "reduce_confident_loss =  0.755830646\n",
            "\n",
            "reduce_mean_cls_loss =  0.367968023\n",
            "\n",
            "regression loss =  24.1997128\n",
            "23/73 [========>.....................] - ETA: 23:10 - loss: 32.7117 - custom_mse: 13721.7490\n",
            "reduce_confident_loss =  0.817634106\n",
            "\n",
            "reduce_mean_cls_loss =  0.316286564\n",
            "\n",
            "regression loss =  22.0894928\n",
            "24/73 [========>.....................] - ETA: 22:42 - loss: 32.3163 - custom_mse: 13918.1914\n",
            "reduce_confident_loss =  0.817403734\n",
            "\n",
            "reduce_mean_cls_loss =  0.313504398\n",
            "\n",
            "regression loss =  20.1446667\n",
            "25/73 [=========>....................] - ETA: 22:24 - loss: 31.8747 - custom_mse: 14144.2910\n",
            "reduce_confident_loss =  0.959965706\n",
            "\n",
            "reduce_mean_cls_loss =  0.326841086\n",
            "\n",
            "regression loss =  29.9606724\n",
            "26/73 [=========>....................] - ETA: 21:51 - loss: 31.8506 - custom_mse: 14464.4580\n",
            "reduce_confident_loss =  0.673882604\n",
            "\n",
            "reduce_mean_cls_loss =  0.445581973\n",
            "\n",
            "regression loss =  19.8420124\n",
            "27/73 [==========>...................] - ETA: 21:19 - loss: 31.4473 - custom_mse: 15442.3184\n",
            "reduce_confident_loss =  0.683649182\n",
            "\n",
            "reduce_mean_cls_loss =  0.440823823\n",
            "\n",
            "regression loss =  18.8787346\n",
            "28/73 [==========>...................] - ETA: 20:48 - loss: 31.0386 - custom_mse: 16015.9570\n",
            "reduce_confident_loss =  0.675377965\n",
            "\n",
            "reduce_mean_cls_loss =  0.440138519\n",
            "\n",
            "regression loss =  17.403\n",
            "29/73 [==========>...................] - ETA: 20:17 - loss: 30.6068 - custom_mse: 16467.2500\n",
            "reduce_confident_loss =  0.681911469\n",
            "\n",
            "reduce_mean_cls_loss =  0.440004855\n",
            "\n",
            "regression loss =  20.2511425\n",
            "30/73 [===========>..................] - ETA: 19:46 - loss: 30.2991 - custom_mse: 17089.1953\n",
            "reduce_confident_loss =  0.673570931\n",
            "\n",
            "reduce_mean_cls_loss =  0.490279853\n",
            "\n",
            "regression loss =  23.1291447\n",
            "31/73 [===========>..................] - ETA: 19:16 - loss: 30.1053 - custom_mse: 17899.9531\n",
            "reduce_confident_loss =  0.673860431\n",
            "\n",
            "reduce_mean_cls_loss =  0.501142442\n",
            "\n",
            "regression loss =  20.366312\n",
            "32/73 [============>.................] - ETA: 18:46 - loss: 29.8377 - custom_mse: 18962.8789\n",
            "reduce_confident_loss =  0.816975057\n",
            "\n",
            "reduce_mean_cls_loss =  0.31363067\n",
            "\n",
            "regression loss =  12.8535395\n",
            "33/73 [============>.................] - ETA: 18:16 - loss: 29.3573 - custom_mse: 18979.8672\n",
            "reduce_confident_loss =  0.959805071\n",
            "\n",
            "reduce_mean_cls_loss =  0.31439808\n",
            "\n",
            "regression loss =  7.45995855\n",
            "34/73 [============>.................] - ETA: 17:47 - loss: 28.7507 - custom_mse: 19062.1172\n",
            "reduce_confident_loss =  0.960543811\n",
            "\n",
            "reduce_mean_cls_loss =  0.31532684\n",
            "\n",
            "regression loss =  7.57772732\n",
            "35/73 [=============>................] - ETA: 17:18 - loss: 28.1822 - custom_mse: 19238.8047\n",
            "reduce_confident_loss =  0.960186303\n",
            "\n",
            "reduce_mean_cls_loss =  0.314937532\n",
            "\n",
            "regression loss =  50.5743065\n",
            "36/73 [=============>................] - ETA: 16:54 - loss: 28.8396 - custom_mse: 28053.1445\n",
            "reduce_confident_loss =  0.941238\n",
            "\n",
            "reduce_mean_cls_loss =  1.23195434\n",
            "\n",
            "regression loss =  85.7242737\n",
            "37/73 [==============>...............] - ETA: 16:25 - loss: 30.4358 - custom_mse: 36805.5078\n",
            "reduce_confident_loss =  0.836968541\n",
            "\n",
            "reduce_mean_cls_loss =  0.746061802\n",
            "\n",
            "regression loss =  59.3074188\n",
            "38/73 [==============>...............] - ETA: 16:00 - loss: 31.2372 - custom_mse: 41029.5234\n",
            "reduce_confident_loss =  0.964128196\n",
            "\n",
            "reduce_mean_cls_loss =  1.3081702\n",
            "\n",
            "regression loss =  91.1567688\n",
            "39/73 [===============>..............] - ETA: 15:31 - loss: 32.8319 - custom_mse: 50794.4141\n",
            "reduce_confident_loss =  0.680026412\n",
            "\n",
            "reduce_mean_cls_loss =  1.10525155\n",
            "\n",
            "regression loss =  104.220901\n",
            "40/73 [===============>..............] - ETA: 15:02 - loss: 34.6613 - custom_mse: 59868.0820\n",
            "reduce_confident_loss =  0.961895823\n",
            "\n",
            "reduce_mean_cls_loss =  1.27687728\n",
            "\n",
            "regression loss =  136.625153\n",
            "41/73 [===============>..............] - ETA: 14:37 - loss: 37.2028 - custom_mse: 75773.8281\n",
            "reduce_confident_loss =  0.756823838\n",
            "\n",
            "reduce_mean_cls_loss =  1.22455657\n",
            "\n",
            "regression loss =  160.010559\n",
            "42/73 [================>.............] - ETA: 14:08 - loss: 40.1740 - custom_mse: 94084.5703\n",
            "reduce_confident_loss =  0.898995757\n",
            "\n",
            "reduce_mean_cls_loss =  1.28701007\n",
            "\n",
            "regression loss =  124.060364\n",
            "43/73 [================>.............] - ETA: 13:52 - loss: 42.1756 - custom_mse: 108596.6250\n",
            "reduce_confident_loss =  0.969257236\n",
            "\n",
            "reduce_mean_cls_loss =  1.30976975\n",
            "\n",
            "regression loss =  7.01911497\n",
            "44/73 [=================>............] - ETA: 13:23 - loss: 41.4284 - custom_mse: 109795.1953\n",
            "reduce_confident_loss =  0.858841896\n",
            "\n",
            "reduce_mean_cls_loss =  1.31089902\n",
            "\n",
            "regression loss =  13.9393244\n",
            "45/73 [=================>............] - ETA: 12:54 - loss: 40.8658 - custom_mse: 111173.2188\n",
            "reduce_confident_loss =  0.960474789\n",
            "\n",
            "reduce_mean_cls_loss =  1.29401648\n",
            "\n",
            "regression loss =  22.1451855\n",
            "46/73 [=================>............] - ETA: 12:27 - loss: 40.5078 - custom_mse: 113463.7734\n",
            "reduce_confident_loss =  0.964658618\n",
            "\n",
            "reduce_mean_cls_loss =  1.30724549\n",
            "\n",
            "regression loss =  46.3438072\n",
            "47/73 [==================>...........] - ETA: 11:58 - loss: 40.6803 - custom_mse: 117122.0000\n",
            "reduce_confident_loss =  0.960281193\n",
            "\n",
            "reduce_mean_cls_loss =  0.745164096\n",
            "\n",
            "regression loss =  27.3160362\n",
            "48/73 [==================>...........] - ETA: 11:30 - loss: 40.4374 - custom_mse: 118915.4609\n",
            "reduce_confident_loss =  0.67849344\n",
            "\n",
            "reduce_mean_cls_loss =  0.440287381\n",
            "\n",
            "regression loss =  21.0780602\n",
            "49/73 [===================>..........] - ETA: 11:02 - loss: 40.0652 - custom_mse: 119774.0234\n",
            "reduce_confident_loss =  0.6819188\n",
            "\n",
            "reduce_mean_cls_loss =  0.442506641\n",
            "\n",
            "regression loss =  15.798749\n",
            "50/73 [===================>..........] - ETA: 10:33 - loss: 39.6023 - custom_mse: 120329.0156\n",
            "reduce_confident_loss =  0.960406959\n",
            "\n",
            "reduce_mean_cls_loss =  0.314925581\n",
            "\n",
            "regression loss =  4.88694572\n",
            "51/73 [===================>..........] - ETA: 10:04 - loss: 38.9466 - custom_mse: 120650.9688\n",
            "reduce_confident_loss =  0.960561216\n",
            "\n",
            "reduce_mean_cls_loss =  0.316455692\n",
            "\n",
            "regression loss =  4.37472868\n",
            "52/73 [====================>.........] - ETA: 9:36 - loss: 38.3064 - custom_mse: 120904.9922 \n",
            "reduce_confident_loss =  0.961394966\n",
            "\n",
            "reduce_mean_cls_loss =  0.325627625\n",
            "\n",
            "regression loss =  38.1526451\n",
            "53/73 [====================>.........] - ETA: 9:08 - loss: 38.3277 - custom_mse: 122659.3438\n",
            "reduce_confident_loss =  0.961819053\n",
            "\n",
            "reduce_mean_cls_loss =  1.26162028\n",
            "\n",
            "regression loss =  112.963112\n",
            "54/73 [=====================>........] - ETA: 8:40 - loss: 39.7511 - custom_mse: 149124.6250\n",
            "reduce_confident_loss =  0.971772611\n",
            "\n",
            "reduce_mean_cls_loss =  1.30135834\n",
            "\n",
            "regression loss =  102.937141\n",
            "55/73 [=====================>........] - ETA: 8:12 - loss: 40.9412 - custom_mse: 174435.4375\n",
            "reduce_confident_loss =  0.960107446\n",
            "\n",
            "reduce_mean_cls_loss =  1.30201972\n",
            "\n",
            "regression loss =  95.2612457\n",
            "56/73 [======================>.......] - ETA: 7:44 - loss: 41.9516 - custom_mse: 198642.0156\n",
            "reduce_confident_loss =  0.962404668\n",
            "\n",
            "reduce_mean_cls_loss =  1.02328587\n",
            "\n",
            "regression loss =  35.3833542\n",
            "57/73 [======================>.......] - ETA: 7:16 - loss: 41.8712 - custom_mse: 206111.5625\n",
            "reduce_confident_loss =  0.859431803\n",
            "\n",
            "reduce_mean_cls_loss =  0.331367433\n",
            "\n",
            "regression loss =  10.2022438\n",
            "58/73 [======================>.......] - ETA: 6:48 - loss: 41.3457 - custom_mse: 206608.2969\n",
            "reduce_confident_loss =  0.764475644\n",
            "\n",
            "reduce_mean_cls_loss =  0.368943185\n",
            "\n",
            "regression loss =  30.7701\n",
            "59/73 [=======================>......] - ETA: 6:20 - loss: 41.1857 - custom_mse: 207459.1562\n",
            "reduce_confident_loss =  0.74185437\n",
            "\n",
            "reduce_mean_cls_loss =  0.611447692\n",
            "\n",
            "regression loss =  29.6798115\n",
            "60/73 [=======================>......] - ETA: 5:53 - loss: 41.0165 - custom_mse: 207869.9219\n",
            "reduce_confident_loss =  0.960431397\n",
            "\n",
            "reduce_mean_cls_loss =  0.321075857\n",
            "\n",
            "regression loss =  6.87559795\n",
            "61/73 [========================>.....] - ETA: 5:25 - loss: 40.4778 - custom_mse: 207880.0312\n",
            "reduce_confident_loss =  0.970781446\n",
            "\n",
            "reduce_mean_cls_loss =  0.32839632\n",
            "\n",
            "regression loss =  58.8957939\n",
            "62/73 [========================>.....] - ETA: 4:57 - loss: 40.7958 - custom_mse: 207980.5000\n",
            "reduce_confident_loss =  0.673631191\n",
            "\n",
            "reduce_mean_cls_loss =  0.670719504\n",
            "\n",
            "regression loss =  82.4647293\n",
            "63/73 [========================>.....] - ETA: 4:30 - loss: 41.4786 - custom_mse: 208629.0156\n",
            "reduce_confident_loss =  0.901544154\n",
            "\n",
            "reduce_mean_cls_loss =  0.533371508\n",
            "\n",
            "regression loss =  30.1952782\n",
            "64/73 [=========================>....] - ETA: 4:02 - loss: 41.3247 - custom_mse: 209814.9375\n",
            "reduce_confident_loss =  0.963965416\n",
            "\n",
            "reduce_mean_cls_loss =  0.313758373\n",
            "\n",
            "regression loss =  16.9086399\n",
            "65/73 [=========================>....] - ETA: 3:35 - loss: 40.9687 - custom_mse: 211135.1875\n",
            "reduce_confident_loss =  0.959912598\n",
            "\n",
            "reduce_mean_cls_loss =  0.313579947\n",
            "\n",
            "regression loss =  16.9210873\n",
            "66/73 [==========================>...] - ETA: 3:08 - loss: 40.6237 - custom_mse: 212426.3281\n",
            "reduce_confident_loss =  0.960134506\n",
            "\n",
            "reduce_mean_cls_loss =  0.315288156\n",
            "\n",
            "regression loss =  15.5292873\n",
            "67/73 [==========================>...] - ETA: 2:41 - loss: 40.2682 - custom_mse: 213815.0000\n",
            "reduce_confident_loss =  0.959404171\n",
            "\n",
            "reduce_mean_cls_loss =  0.313383639\n",
            "\n",
            "regression loss =  14.7202187\n",
            "68/73 [==========================>...] - ETA: 2:14 - loss: 39.9112 - custom_mse: 215276.3594\n",
            "reduce_confident_loss =  0.959385037\n",
            "\n",
            "reduce_mean_cls_loss =  0.319691509\n",
            "\n",
            "regression loss =  14.492589\n",
            "69/73 [===========================>..] - ETA: 1:47 - loss: 39.5613 - custom_mse: 216662.0469\n",
            "reduce_confident_loss =  0.960009396\n",
            "\n",
            "reduce_mean_cls_loss =  0.320374817\n",
            "\n",
            "regression loss =  12.4537954\n",
            "70/73 [===========================>..] - ETA: 1:20 - loss: 39.1924 - custom_mse: 217180.8125\n",
            "reduce_confident_loss =  0.963433444\n",
            "\n",
            "reduce_mean_cls_loss =  0.31384632\n",
            "\n",
            "regression loss =  12.547245\n",
            "71/73 [============================>.] - ETA: 53s - loss: 38.8351 - custom_mse: 217476.9375 \n",
            "reduce_confident_loss =  0.819264531\n",
            "\n",
            "reduce_mean_cls_loss =  0.313658148\n",
            "\n",
            "regression loss =  17.6199799\n",
            "72/73 [============================>.] - ETA: 26s - loss: 38.5562 - custom_mse: 218134.7344\n",
            "reduce_confident_loss =  0.919834137\n",
            "\n",
            "reduce_mean_cls_loss =  0.313975\n",
            "\n",
            "regression loss =  12.5273046\n",
            "73/73 [==============================] - ETA: 0s - loss: 38.3852 - custom_mse: 218943.7344 \n",
            "reduce_confident_loss =  0.9591676\n",
            "\n",
            "reduce_mean_cls_loss =  0.317971528\n",
            "\n",
            "regression loss =  16.0713329\n",
            "\n",
            "reduce_confident_loss =  0.897931397\n",
            "\n",
            "reduce_mean_cls_loss =  0.316612482\n",
            "\n",
            "regression loss =  13.0338678\n",
            "\n",
            "reduce_confident_loss =  0.918384552\n",
            "\n",
            "reduce_mean_cls_loss =  0.316777557\n",
            "\n",
            "regression loss =  13.7997532\n",
            "\n",
            "reduce_confident_loss =  0.857147574\n",
            "\n",
            "reduce_mean_cls_loss =  0.31591323\n",
            "\n",
            "regression loss =  16.6929913\n",
            "\n",
            "reduce_confident_loss =  0.673449039\n",
            "\n",
            "reduce_mean_cls_loss =  0.504920185\n",
            "\n",
            "regression loss =  46.1448975\n",
            "\n",
            "reduce_confident_loss =  0.591860235\n",
            "\n",
            "reduce_mean_cls_loss =  0.587603509\n",
            "\n",
            "regression loss =  29.66152\n",
            "\n",
            "reduce_confident_loss =  0.959181786\n",
            "\n",
            "reduce_mean_cls_loss =  0.3174164\n",
            "\n",
            "regression loss =  14.1360025\n",
            "\n",
            "reduce_confident_loss =  0.959186256\n",
            "\n",
            "reduce_mean_cls_loss =  0.317337215\n",
            "\n",
            "regression loss =  14.0651493\n",
            "\n",
            "reduce_confident_loss =  0.9591766\n",
            "\n",
            "reduce_mean_cls_loss =  0.317359835\n",
            "\n",
            "regression loss =  14.1813745\n",
            "\n",
            "reduce_confident_loss =  0.959186912\n",
            "\n",
            "reduce_mean_cls_loss =  0.317486703\n",
            "\n",
            "regression loss =  14.3202333\n",
            "\n",
            "reduce_confident_loss =  0.959198654\n",
            "\n",
            "reduce_mean_cls_loss =  0.317468643\n",
            "\n",
            "regression loss =  13.7622547\n",
            "\n",
            "reduce_confident_loss =  0.897980273\n",
            "\n",
            "reduce_mean_cls_loss =  0.316795975\n",
            "\n",
            "regression loss =  13.531496\n",
            "\n",
            "reduce_confident_loss =  0.959197104\n",
            "\n",
            "reduce_mean_cls_loss =  0.317429334\n",
            "\n",
            "regression loss =  11.61619\n",
            "\n",
            "reduce_confident_loss =  0.959206\n",
            "\n",
            "reduce_mean_cls_loss =  0.739425242\n",
            "\n",
            "regression loss =  72.8040848\n",
            "73/73 [==============================] - 2104s 29s/step - loss: 38.3852 - custom_mse: 218943.7344 - val_loss: 21.3130 - val_custom_mse: 12116.9023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JV90q2zSlzQ",
        "outputId": "c8cc9f45-cf66-48c8-b127-8ef88b5d6ed8"
      },
      "source": [
        "vit_resnet_backbone_model.save(\"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_92_layer_call_and_return_conditional_losses, dense_92_layer_call_fn, embedding_4_layer_call_and_return_conditional_losses, embedding_4_layer_call_fn, dense_113_layer_call_and_return_conditional_losses while saving (showing 5 of 1000). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_1/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_1/assets\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPJITpa_6qFZ",
        "outputId": "054d9579-b42f-44b2-8706-22b5cd7f47c0"
      },
      "source": [
        "history_2 = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mKết quả truyền trực tuyến bị cắt bớt đến 5000 dòng cuối.\u001b[0m\n",
            "\n",
            "reduce_mean_cls_loss =  0.315029293\n",
            "\n",
            "regression loss =  6.49456644\n",
            "\n",
            "reduce_confident_loss =  0.785332859\n",
            "\n",
            "reduce_mean_cls_loss =  0.314976901\n",
            "\n",
            "regression loss =  5.18259907\n",
            "\n",
            "reduce_confident_loss =  0.785292506\n",
            "\n",
            "reduce_mean_cls_loss =  0.314655155\n",
            "\n",
            "regression loss =  7.91458941\n",
            "\n",
            "reduce_confident_loss =  0.785356402\n",
            "\n",
            "reduce_mean_cls_loss =  0.315101892\n",
            "\n",
            "regression loss =  13.0387039\n",
            "\n",
            "reduce_confident_loss =  0.785395324\n",
            "\n",
            "reduce_mean_cls_loss =  0.740825236\n",
            "\n",
            "regression loss =  68.3882294\n",
            "73/73 [==============================] - 865s 12s/step - loss: 48.6335 - custom_mse: 218912.6250 - val_loss: 18.8952 - val_custom_mse: 22479.0469\n",
            "Epoch 3/15\n",
            "\n",
            "reduce_confident_loss =  0.776610553\n",
            "\n",
            "reduce_mean_cls_loss =  1.11907709\n",
            "\n",
            "regression loss =  101.015701\n",
            " 1/73 [..............................] - ETA: 32:36 - loss: 102.9114 - custom_mse: 3806.4944\n",
            "reduce_confident_loss =  0.78212589\n",
            "\n",
            "reduce_mean_cls_loss =  0.656089246\n",
            "\n",
            "regression loss =  44.7755928\n",
            " 2/73 [..............................] - ETA: 12:07 - loss: 74.5626 - custom_mse: 4438.9360 \n",
            "reduce_confident_loss =  0.77286768\n",
            "\n",
            "reduce_mean_cls_loss =  0.646435261\n",
            "\n",
            "regression loss =  37.1190453\n",
            " 3/73 [>.............................] - ETA: 11:44 - loss: 62.5545 - custom_mse: 4968.2671\n",
            "reduce_confident_loss =  0.76376754\n",
            "\n",
            "reduce_mean_cls_loss =  0.646918178\n",
            "\n",
            "regression loss =  35.0137825\n",
            " 4/73 [>.............................] - ETA: 11:15 - loss: 56.0220 - custom_mse: 5556.4810\n",
            "reduce_confident_loss =  0.772427738\n",
            "\n",
            "reduce_mean_cls_loss =  0.646416783\n",
            "\n",
            "regression loss =  31.2157936\n",
            " 5/73 [=>............................] - ETA: 10:55 - loss: 51.3445 - custom_mse: 6144.1729\n",
            "reduce_confident_loss =  0.774374664\n",
            "\n",
            "reduce_mean_cls_loss =  0.646185\n",
            "\n",
            "regression loss =  28.9494152\n",
            " 6/73 [=>............................] - ETA: 10:41 - loss: 47.8488 - custom_mse: 6745.7476\n",
            "reduce_confident_loss =  0.779674113\n",
            "\n",
            "reduce_mean_cls_loss =  0.674752414\n",
            "\n",
            "regression loss =  39.9564476\n",
            " 7/73 [=>............................] - ETA: 10:28 - loss: 46.9291 - custom_mse: 7370.9912\n",
            "reduce_confident_loss =  0.779203415\n",
            "\n",
            "reduce_mean_cls_loss =  0.436696142\n",
            "\n",
            "regression loss =  22.2661915\n",
            " 8/73 [==>...........................] - ETA: 10:36 - loss: 43.9982 - custom_mse: 7813.2080\n",
            "reduce_confident_loss =  0.781005263\n",
            "\n",
            "reduce_mean_cls_loss =  0.314254075\n",
            "\n",
            "regression loss =  19.1072235\n",
            " 9/73 [==>...........................] - ETA: 10:20 - loss: 41.3542 - custom_mse: 8233.4170\n",
            "reduce_confident_loss =  0.781581461\n",
            "\n",
            "reduce_mean_cls_loss =  0.318510264\n",
            "\n",
            "regression loss =  9.02254581\n",
            "10/73 [===>..........................] - ETA: 10:05 - loss: 38.2311 - custom_mse: 8779.1455\n",
            "reduce_confident_loss =  0.781103194\n",
            "\n",
            "reduce_mean_cls_loss =  0.323157161\n",
            "\n",
            "regression loss =  21.0007668\n",
            "11/73 [===>..........................] - ETA: 9:50 - loss: 36.7651 - custom_mse: 9326.6328 \n",
            "reduce_confident_loss =  0.779946744\n",
            "\n",
            "reduce_mean_cls_loss =  0.316841602\n",
            "\n",
            "regression loss =  23.1779728\n",
            "12/73 [===>..........................] - ETA: 9:37 - loss: 35.7242 - custom_mse: 11265.4268\n",
            "reduce_confident_loss =  0.780395091\n",
            "\n",
            "reduce_mean_cls_loss =  0.314626545\n",
            "\n",
            "regression loss =  11.827939\n",
            "13/73 [====>.........................] - ETA: 9:27 - loss: 33.9703 - custom_mse: 13158.5322\n",
            "reduce_confident_loss =  0.78194344\n",
            "\n",
            "reduce_mean_cls_loss =  0.314428627\n",
            "\n",
            "regression loss =  3.82800961\n",
            "14/73 [====>.........................] - ETA: 9:15 - loss: 31.8956 - custom_mse: 14742.9092\n",
            "reduce_confident_loss =  0.773031652\n",
            "\n",
            "reduce_mean_cls_loss =  0.317528248\n",
            "\n",
            "regression loss =  2.1275878\n",
            "15/73 [=====>........................] - ETA: 9:03 - loss: 29.9837 - custom_mse: 16365.5889\n",
            "reduce_confident_loss =  0.78068924\n",
            "\n",
            "reduce_mean_cls_loss =  0.315418094\n",
            "\n",
            "regression loss =  2.44211817\n",
            "16/73 [=====>........................] - ETA: 8:51 - loss: 28.3309 - custom_mse: 18014.8633\n",
            "reduce_confident_loss =  0.781607628\n",
            "\n",
            "reduce_mean_cls_loss =  0.313966215\n",
            "\n",
            "regression loss =  28.0262032\n",
            "17/73 [=====>........................] - ETA: 8:41 - loss: 28.3774 - custom_mse: 19706.4277\n",
            "reduce_confident_loss =  0.779100955\n",
            "\n",
            "reduce_mean_cls_loss =  0.703045\n",
            "\n",
            "regression loss =  57.095314\n",
            "18/73 [======>.......................] - ETA: 8:31 - loss: 30.0552 - custom_mse: 20932.2246\n",
            "reduce_confident_loss =  0.77845186\n",
            "\n",
            "reduce_mean_cls_loss =  0.724301875\n",
            "\n",
            "regression loss =  65.631691\n",
            "19/73 [======>.......................] - ETA: 10:30 - loss: 32.0067 - custom_mse: 21701.7305\n",
            "reduce_confident_loss =  0.779050171\n",
            "\n",
            "reduce_mean_cls_loss =  0.635543942\n",
            "\n",
            "regression loss =  67.5757294\n",
            "20/73 [=======>......................] - ETA: 10:12 - loss: 33.8559 - custom_mse: 22115.7188\n",
            "reduce_confident_loss =  0.780696213\n",
            "\n",
            "reduce_mean_cls_loss =  0.647136033\n",
            "\n",
            "regression loss =  53.3103943\n",
            "21/73 [=======>......................] - ETA: 9:53 - loss: 34.8503 - custom_mse: 22530.1719 \n",
            "reduce_confident_loss =  0.779887259\n",
            "\n",
            "reduce_mean_cls_loss =  0.642614365\n",
            "\n",
            "regression loss =  66.3569565\n",
            "22/73 [========>.....................] - ETA: 9:36 - loss: 36.3471 - custom_mse: 23218.9961\n",
            "reduce_confident_loss =  0.778721213\n",
            "\n",
            "reduce_mean_cls_loss =  0.460125983\n",
            "\n",
            "regression loss =  27.133606\n",
            "23/73 [========>.....................] - ETA: 9:23 - loss: 36.0004 - custom_mse: 23504.6035\n",
            "reduce_confident_loss =  0.77951616\n",
            "\n",
            "reduce_mean_cls_loss =  0.314221412\n",
            "\n",
            "regression loss =  22.1836185\n",
            "24/73 [========>.....................] - ETA: 9:12 - loss: 35.4702 - custom_mse: 23812.9902\n",
            "reduce_confident_loss =  0.78004837\n",
            "\n",
            "reduce_mean_cls_loss =  0.315843761\n",
            "\n",
            "regression loss =  19.8363724\n",
            "25/73 [=========>....................] - ETA: 9:14 - loss: 34.8887 - custom_mse: 24233.8594\n",
            "reduce_confident_loss =  0.786819339\n",
            "\n",
            "reduce_mean_cls_loss =  0.338848412\n",
            "\n",
            "regression loss =  30.7985764\n",
            "26/73 [=========>....................] - ETA: 8:56 - loss: 34.7747 - custom_mse: 25701.6875\n",
            "reduce_confident_loss =  0.780058324\n",
            "\n",
            "reduce_mean_cls_loss =  0.314705402\n",
            "\n",
            "regression loss =  18.295557\n",
            "27/73 [==========>...................] - ETA: 8:39 - loss: 34.2049 - custom_mse: 27439.1328\n",
            "reduce_confident_loss =  0.778803\n",
            "\n",
            "reduce_mean_cls_loss =  0.314744413\n",
            "\n",
            "regression loss =  16.1288414\n",
            "28/73 [==========>...................] - ETA: 8:23 - loss: 33.5984 - custom_mse: 28401.0781\n",
            "reduce_confident_loss =  0.772648156\n",
            "\n",
            "reduce_mean_cls_loss =  0.319865316\n",
            "\n",
            "regression loss =  14.6980991\n",
            "29/73 [==========>...................] - ETA: 8:08 - loss: 32.9843 - custom_mse: 29170.8477\n",
            "reduce_confident_loss =  0.777243733\n",
            "\n",
            "reduce_mean_cls_loss =  0.327008\n",
            "\n",
            "regression loss =  16.7598667\n",
            "30/73 [===========>..................] - ETA: 7:53 - loss: 32.4803 - custom_mse: 30061.8691\n",
            "reduce_confident_loss =  0.763110816\n",
            "\n",
            "reduce_mean_cls_loss =  0.327028811\n",
            "\n",
            "regression loss =  19.3559074\n",
            "31/73 [===========>..................] - ETA: 7:38 - loss: 32.0921 - custom_mse: 31033.3828\n",
            "reduce_confident_loss =  0.77563107\n",
            "\n",
            "reduce_mean_cls_loss =  0.330504984\n",
            "\n",
            "regression loss =  15.5984478\n",
            "32/73 [============>.................] - ETA: 7:24 - loss: 31.6113 - custom_mse: 32053.3613\n",
            "reduce_confident_loss =  0.772290528\n",
            "\n",
            "reduce_mean_cls_loss =  0.320485592\n",
            "\n",
            "regression loss =  16.99827\n",
            "33/73 [============>.................] - ETA: 7:10 - loss: 31.2016 - custom_mse: 32404.7305\n",
            "reduce_confident_loss =  0.78140074\n",
            "\n",
            "reduce_mean_cls_loss =  0.314351618\n",
            "\n",
            "regression loss =  8.62197685\n",
            "34/73 [============>.................] - ETA: 6:57 - loss: 30.5697 - custom_mse: 32801.6641\n",
            "reduce_confident_loss =  0.780855536\n",
            "\n",
            "reduce_mean_cls_loss =  0.345350504\n",
            "\n",
            "regression loss =  19.0650711\n",
            "35/73 [=============>................] - ETA: 6:43 - loss: 30.2732 - custom_mse: 33251.3789\n",
            "reduce_confident_loss =  0.777971327\n",
            "\n",
            "reduce_mean_cls_loss =  0.331868917\n",
            "\n",
            "regression loss =  76.0339432\n",
            "36/73 [=============>................] - ETA: 6:37 - loss: 31.5751 - custom_mse: 39951.6523\n",
            "reduce_confident_loss =  0.775469244\n",
            "\n",
            "reduce_mean_cls_loss =  1.2266748\n",
            "\n",
            "regression loss =  121.237976\n",
            "37/73 [==============>...............] - ETA: 6:24 - loss: 34.0525 - custom_mse: 46721.1562\n",
            "reduce_confident_loss =  0.762396932\n",
            "\n",
            "reduce_mean_cls_loss =  0.749536216\n",
            "\n",
            "regression loss =  92.5433273\n",
            "38/73 [==============>...............] - ETA: 6:16 - loss: 35.6316 - custom_mse: 49630.7305\n",
            "reduce_confident_loss =  0.774246216\n",
            "\n",
            "reduce_mean_cls_loss =  1.30635798\n",
            "\n",
            "regression loss =  123.367775\n",
            "39/73 [===============>..............] - ETA: 6:03 - loss: 37.9346 - custom_mse: 56057.2852\n",
            "reduce_confident_loss =  0.763384342\n",
            "\n",
            "reduce_mean_cls_loss =  1.30740929\n",
            "\n",
            "regression loss =  124.906601\n",
            "40/73 [===============>..............] - ETA: 5:50 - loss: 40.1606 - custom_mse: 61693.3906\n",
            "reduce_confident_loss =  0.770336211\n",
            "\n",
            "reduce_mean_cls_loss =  1.27920532\n",
            "\n",
            "regression loss =  151.23288\n",
            "41/73 [===============>..............] - ETA: 5:42 - loss: 42.9197 - custom_mse: 80169.2578\n",
            "reduce_confident_loss =  0.770099163\n",
            "\n",
            "reduce_mean_cls_loss =  1.30478168\n",
            "\n",
            "regression loss =  166.468765\n",
            "42/73 [================>.............] - ETA: 5:29 - loss: 45.9107 - custom_mse: 101485.4062\n",
            "reduce_confident_loss =  0.77150631\n",
            "\n",
            "reduce_mean_cls_loss =  1.28959179\n",
            "\n",
            "regression loss =  138.347382\n",
            "43/73 [================>.............] - ETA: 5:34 - loss: 48.1084 - custom_mse: 118460.8594\n",
            "reduce_confident_loss =  0.781122506\n",
            "\n",
            "reduce_mean_cls_loss =  1.30845356\n",
            "\n",
            "regression loss =  14.8376732\n",
            "44/73 [=================>............] - ETA: 5:21 - loss: 47.3997 - custom_mse: 120268.2188\n",
            "reduce_confident_loss =  0.775324285\n",
            "\n",
            "reduce_mean_cls_loss =  1.30731368\n",
            "\n",
            "regression loss =  21.2055111\n",
            "45/73 [=================>............] - ETA: 5:08 - loss: 46.8639 - custom_mse: 122537.5625\n",
            "reduce_confident_loss =  0.761807561\n",
            "\n",
            "reduce_mean_cls_loss =  1.28557205\n",
            "\n",
            "regression loss =  37.0168495\n",
            "46/73 [=================>............] - ETA: 4:58 - loss: 46.6943 - custom_mse: 126415.1797\n",
            "reduce_confident_loss =  0.759148657\n",
            "\n",
            "reduce_mean_cls_loss =  1.30338645\n",
            "\n",
            "regression loss =  62.4119301\n",
            "47/73 [==================>...........] - ETA: 4:46 - loss: 47.0726 - custom_mse: 132652.2969\n",
            "reduce_confident_loss =  0.762696564\n",
            "\n",
            "reduce_mean_cls_loss =  0.741312921\n",
            "\n",
            "regression loss =  25.0812473\n",
            "48/73 [==================>...........] - ETA: 4:34 - loss: 46.6458 - custom_mse: 134974.8750\n",
            "reduce_confident_loss =  0.765751064\n",
            "\n",
            "reduce_mean_cls_loss =  0.314607561\n",
            "\n",
            "regression loss =  17.3423786\n",
            "49/73 [===================>..........] - ETA: 4:22 - loss: 46.0698 - custom_mse: 135992.7031\n",
            "reduce_confident_loss =  0.759128213\n",
            "\n",
            "reduce_mean_cls_loss =  0.316616058\n",
            "\n",
            "regression loss =  14.9776592\n",
            "50/73 [===================>..........] - ETA: 4:10 - loss: 45.4695 - custom_mse: 136742.2969\n",
            "reduce_confident_loss =  0.764428139\n",
            "\n",
            "reduce_mean_cls_loss =  0.338179886\n",
            "\n",
            "regression loss =  11.3804379\n",
            "51/73 [===================>..........] - ETA: 3:57 - loss: 44.8227 - custom_mse: 137439.8594\n",
            "reduce_confident_loss =  0.7738415\n",
            "\n",
            "reduce_mean_cls_loss =  0.323133439\n",
            "\n",
            "regression loss =  10.5761414\n",
            "52/73 [====================>.........] - ETA: 3:45 - loss: 44.1852 - custom_mse: 138087.8750\n",
            "reduce_confident_loss =  0.772612631\n",
            "\n",
            "reduce_mean_cls_loss =  0.33160156\n",
            "\n",
            "regression loss =  65.529953\n",
            "53/73 [====================>.........] - ETA: 3:35 - loss: 44.6088 - custom_mse: 139956.0625\n",
            "reduce_confident_loss =  0.777219951\n",
            "\n",
            "reduce_mean_cls_loss =  1.3091619\n",
            "\n",
            "regression loss =  206.106323\n",
            "54/73 [=====================>........] - ETA: 3:22 - loss: 47.6381 - custom_mse: 161203.9062\n",
            "reduce_confident_loss =  0.764565051\n",
            "\n",
            "reduce_mean_cls_loss =  1.28911746\n",
            "\n",
            "regression loss =  199.639206\n",
            "55/73 [=====================>........] - ETA: 3:11 - loss: 50.4391 - custom_mse: 181134.3750\n",
            "reduce_confident_loss =  0.778836608\n",
            "\n",
            "reduce_mean_cls_loss =  1.2879467\n",
            "\n",
            "regression loss =  192.537048\n",
            "56/73 [======================>.......] - ETA: 2:59 - loss: 53.0135 - custom_mse: 199671.9844\n",
            "reduce_confident_loss =  0.767634\n",
            "\n",
            "reduce_mean_cls_loss =  1.02416337\n",
            "\n",
            "regression loss =  68.3603058\n",
            "57/73 [======================>.......] - ETA: 2:48 - loss: 53.3142 - custom_mse: 205741.9844\n",
            "reduce_confident_loss =  0.75636065\n",
            "\n",
            "reduce_mean_cls_loss =  0.321197838\n",
            "\n",
            "regression loss =  9.9081955\n",
            "58/73 [======================>.......] - ETA: 2:36 - loss: 52.5844 - custom_mse: 206753.9062\n",
            "reduce_confident_loss =  0.728752136\n",
            "\n",
            "reduce_mean_cls_loss =  0.458357394\n",
            "\n",
            "regression loss =  32.0080376\n",
            "59/73 [=======================>......] - ETA: 2:25 - loss: 52.2557 - custom_mse: 208584.3125\n",
            "reduce_confident_loss =  0.730569422\n",
            "\n",
            "reduce_mean_cls_loss =  0.679795206\n",
            "\n",
            "regression loss =  34.4230614\n",
            "60/73 [=======================>......] - ETA: 2:14 - loss: 51.9820 - custom_mse: 209648.0469\n",
            "reduce_confident_loss =  0.731277168\n",
            "\n",
            "reduce_mean_cls_loss =  0.330993176\n",
            "\n",
            "regression loss =  19.5232506\n",
            "61/73 [========================>.....] - ETA: 2:03 - loss: 51.4673 - custom_mse: 210263.8750\n",
            "reduce_confident_loss =  0.756178796\n",
            "\n",
            "reduce_mean_cls_loss =  0.329267025\n",
            "\n",
            "regression loss =  65.2845764\n",
            "62/73 [========================>.....] - ETA: 1:52 - loss: 51.7077 - custom_mse: 211438.9062\n",
            "reduce_confident_loss =  0.716545701\n",
            "\n",
            "reduce_mean_cls_loss =  0.841547191\n",
            "\n",
            "regression loss =  86.5837326\n",
            "63/73 [========================>.....] - ETA: 1:41 - loss: 52.2860 - custom_mse: 213177.3906\n",
            "reduce_confident_loss =  0.749590576\n",
            "\n",
            "reduce_mean_cls_loss =  0.542630136\n",
            "\n",
            "regression loss =  31.5527706\n",
            "64/73 [=========================>....] - ETA: 1:31 - loss: 51.9823 - custom_mse: 214653.9375\n",
            "reduce_confident_loss =  0.771071434\n",
            "\n",
            "reduce_mean_cls_loss =  0.318879366\n",
            "\n",
            "regression loss =  13.3347578\n",
            "65/73 [=========================>....] - ETA: 1:20 - loss: 51.4044 - custom_mse: 216242.6094\n",
            "reduce_confident_loss =  0.763510406\n",
            "\n",
            "reduce_mean_cls_loss =  0.320125252\n",
            "\n",
            "regression loss =  15.8877077\n",
            "66/73 [==========================>...] - ETA: 1:10 - loss: 50.8827 - custom_mse: 217850.3906\n",
            "reduce_confident_loss =  0.768200755\n",
            "\n",
            "reduce_mean_cls_loss =  0.350134075\n",
            "\n",
            "regression loss =  14.9491606\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 50.3631 - custom_mse: 219642.6719 \n",
            "reduce_confident_loss =  0.754587829\n",
            "\n",
            "reduce_mean_cls_loss =  0.326052219\n",
            "\n",
            "regression loss =  14.7681398\n",
            "68/73 [==========================>...] - ETA: 49s - loss: 49.8555 - custom_mse: 221385.5781\n",
            "reduce_confident_loss =  0.777770698\n",
            "\n",
            "reduce_mean_cls_loss =  0.319152087\n",
            "\n",
            "regression loss =  12.4239588\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 49.3289 - custom_mse: 223287.0469\n",
            "reduce_confident_loss =  0.787776\n",
            "\n",
            "reduce_mean_cls_loss =  0.316997498\n",
            "\n",
            "regression loss =  10.9741907\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 48.7968 - custom_mse: 225060.6250\n",
            "reduce_confident_loss =  0.782886922\n",
            "\n",
            "reduce_mean_cls_loss =  0.318881422\n",
            "\n",
            "regression loss =  12.5540819\n",
            "71/73 [============================>.] - ETA: 19s - loss: 48.3019 - custom_mse: 226358.5938\n",
            "reduce_confident_loss =  0.770913422\n",
            "\n",
            "reduce_mean_cls_loss =  0.315965176\n",
            "\n",
            "regression loss =  11.0461521\n",
            "72/73 [============================>.] - ETA: 9s - loss: 47.7995 - custom_mse: 227578.4062 \n",
            "reduce_confident_loss =  0.758831859\n",
            "\n",
            "reduce_mean_cls_loss =  0.316290766\n",
            "\n",
            "regression loss =  7.69026947\n",
            "73/73 [==============================] - ETA: 0s - loss: 47.5303 - custom_mse: 229086.7188\n",
            "reduce_confident_loss =  0.762858391\n",
            "\n",
            "reduce_mean_cls_loss =  0.313793331\n",
            "\n",
            "regression loss =  15.1193209\n",
            "\n",
            "reduce_confident_loss =  0.762402415\n",
            "\n",
            "reduce_mean_cls_loss =  0.313694149\n",
            "\n",
            "regression loss =  11.6709204\n",
            "\n",
            "reduce_confident_loss =  0.762121618\n",
            "\n",
            "reduce_mean_cls_loss =  0.313744396\n",
            "\n",
            "regression loss =  12.7274332\n",
            "\n",
            "reduce_confident_loss =  0.762605667\n",
            "\n",
            "reduce_mean_cls_loss =  0.313640594\n",
            "\n",
            "regression loss =  13.8210278\n",
            "\n",
            "reduce_confident_loss =  0.751181602\n",
            "\n",
            "reduce_mean_cls_loss =  0.622811\n",
            "\n",
            "regression loss =  49.8313\n",
            "\n",
            "reduce_confident_loss =  0.750604\n",
            "\n",
            "reduce_mean_cls_loss =  0.474948436\n",
            "\n",
            "regression loss =  31.2267036\n",
            "\n",
            "reduce_confident_loss =  0.761065364\n",
            "\n",
            "reduce_mean_cls_loss =  0.313862145\n",
            "\n",
            "regression loss =  12.8905668\n",
            "\n",
            "reduce_confident_loss =  0.761151314\n",
            "\n",
            "reduce_mean_cls_loss =  0.313849509\n",
            "\n",
            "regression loss =  12.9985924\n",
            "\n",
            "reduce_confident_loss =  0.761294305\n",
            "\n",
            "reduce_mean_cls_loss =  0.313873559\n",
            "\n",
            "regression loss =  13.0964832\n",
            "\n",
            "reduce_confident_loss =  0.761719704\n",
            "\n",
            "reduce_mean_cls_loss =  0.313818544\n",
            "\n",
            "regression loss =  13.1859589\n",
            "\n",
            "reduce_confident_loss =  0.761427522\n",
            "\n",
            "reduce_mean_cls_loss =  0.313839853\n",
            "\n",
            "regression loss =  10.8143291\n",
            "\n",
            "reduce_confident_loss =  0.760633171\n",
            "\n",
            "reduce_mean_cls_loss =  0.313718975\n",
            "\n",
            "regression loss =  12.1371899\n",
            "\n",
            "reduce_confident_loss =  0.762476087\n",
            "\n",
            "reduce_mean_cls_loss =  0.313920647\n",
            "\n",
            "regression loss =  10.6010084\n",
            "\n",
            "reduce_confident_loss =  0.762844503\n",
            "\n",
            "reduce_mean_cls_loss =  0.741457283\n",
            "\n",
            "regression loss =  73.8156509\n",
            "73/73 [==============================] - 846s 11s/step - loss: 47.5303 - custom_mse: 229086.7188 - val_loss: 20.4180 - val_custom_mse: 26711.8594\n",
            "Epoch 4/15\n",
            "\n",
            "reduce_confident_loss =  0.768073618\n",
            "\n",
            "reduce_mean_cls_loss =  1.12093484\n",
            "\n",
            "regression loss =  100.520874\n",
            " 1/73 [..............................] - ETA: 31:59 - loss: 102.4099 - custom_mse: 4351.4717\n",
            "reduce_confident_loss =  0.756433547\n",
            "\n",
            "reduce_mean_cls_loss =  0.64955008\n",
            "\n",
            "regression loss =  46.3394547\n",
            " 2/73 [..............................] - ETA: 11:56 - loss: 75.0777 - custom_mse: 5345.2427 \n",
            "reduce_confident_loss =  0.742573857\n",
            "\n",
            "reduce_mean_cls_loss =  0.644085\n",
            "\n",
            "regression loss =  36.8741608\n",
            " 3/73 [>.............................] - ETA: 11:13 - loss: 62.8054 - custom_mse: 6424.8965\n",
            "reduce_confident_loss =  0.738869369\n",
            "\n",
            "reduce_mean_cls_loss =  0.640482128\n",
            "\n",
            "regression loss =  31.9688892\n",
            " 4/73 [>.............................] - ETA: 10:56 - loss: 55.4411 - custom_mse: 7514.8809\n",
            "reduce_confident_loss =  0.762303293\n",
            "\n",
            "reduce_mean_cls_loss =  0.641898215\n",
            "\n",
            "regression loss =  29.268877\n",
            " 5/73 [=>............................] - ETA: 10:47 - loss: 50.4875 - custom_mse: 8609.0322\n",
            "reduce_confident_loss =  0.752903819\n",
            "\n",
            "reduce_mean_cls_loss =  0.644853711\n",
            "\n",
            "regression loss =  27.3030682\n",
            " 6/73 [=>............................] - ETA: 10:32 - loss: 46.8564 - custom_mse: 9688.1758\n",
            "reduce_confident_loss =  0.764950097\n",
            "\n",
            "reduce_mean_cls_loss =  0.647197723\n",
            "\n",
            "regression loss =  39.0283661\n",
            " 7/73 [=>............................] - ETA: 10:20 - loss: 45.9398 - custom_mse: 10821.6758\n",
            "reduce_confident_loss =  0.769236445\n",
            "\n",
            "reduce_mean_cls_loss =  0.414764404\n",
            "\n",
            "regression loss =  23.4341812\n",
            " 8/73 [==>...........................] - ETA: 10:26 - loss: 43.2746 - custom_mse: 11647.1172\n",
            "reduce_confident_loss =  0.742923617\n",
            "\n",
            "reduce_mean_cls_loss =  0.313806564\n",
            "\n",
            "regression loss =  9.64389324\n",
            " 9/73 [==>...........................] - ETA: 10:11 - loss: 39.6553 - custom_mse: 12381.1465\n",
            "reduce_confident_loss =  0.765818179\n",
            "\n",
            "reduce_mean_cls_loss =  0.319100291\n",
            "\n",
            "regression loss =  7.68668509\n",
            "10/73 [===>..........................] - ETA: 9:56 - loss: 36.5669 - custom_mse: 13337.1738 \n",
            "reduce_confident_loss =  0.767775834\n",
            "\n",
            "reduce_mean_cls_loss =  0.318898261\n",
            "\n",
            "regression loss =  23.7548122\n",
            "11/73 [===>..........................] - ETA: 9:43 - loss: 35.5010 - custom_mse: 14250.2852\n",
            "reduce_confident_loss =  0.759175599\n",
            "\n",
            "reduce_mean_cls_loss =  0.318342894\n",
            "\n",
            "regression loss =  24.0231838\n",
            "12/73 [===>..........................] - ETA: 9:30 - loss: 34.6343 - custom_mse: 16957.0586\n",
            "reduce_confident_loss =  0.769182265\n",
            "\n",
            "reduce_mean_cls_loss =  0.315416753\n",
            "\n",
            "regression loss =  14.8795805\n",
            "13/73 [====>.........................] - ETA: 9:17 - loss: 33.1981 - custom_mse: 19346.9160\n",
            "reduce_confident_loss =  0.75407654\n",
            "\n",
            "reduce_mean_cls_loss =  0.319946021\n",
            "\n",
            "regression loss =  6.94292879\n",
            "14/73 [====>.........................] - ETA: 9:06 - loss: 31.3995 - custom_mse: 21165.7051\n",
            "reduce_confident_loss =  0.762691796\n",
            "\n",
            "reduce_mean_cls_loss =  0.323848933\n",
            "\n",
            "regression loss =  4.36364937\n",
            "15/73 [=====>........................] - ETA: 8:57 - loss: 29.6695 - custom_mse: 23173.1074\n",
            "reduce_confident_loss =  0.768594801\n",
            "\n",
            "reduce_mean_cls_loss =  0.318651259\n",
            "\n",
            "regression loss =  3.80553246\n",
            "16/73 [=====>........................] - ETA: 8:46 - loss: 28.1210 - custom_mse: 25214.1113\n",
            "reduce_confident_loss =  0.76369065\n",
            "\n",
            "reduce_mean_cls_loss =  0.313857883\n",
            "\n",
            "regression loss =  25.4060535\n",
            "17/73 [=====>........................] - ETA: 8:35 - loss: 28.0247 - custom_mse: 27409.4824\n",
            "reduce_confident_loss =  0.756069303\n",
            "\n",
            "reduce_mean_cls_loss =  0.657551169\n",
            "\n",
            "regression loss =  54.9428444\n",
            "18/73 [======>.......................] - ETA: 8:25 - loss: 29.5986 - custom_mse: 29108.3750\n",
            "reduce_confident_loss =  0.773631215\n",
            "\n",
            "reduce_mean_cls_loss =  0.669938743\n",
            "\n",
            "regression loss =  62.2764473\n",
            "19/73 [======>.......................] - ETA: 10:25 - loss: 31.3945 - custom_mse: 30354.8438\n",
            "reduce_confident_loss =  0.763358\n",
            "\n",
            "reduce_mean_cls_loss =  0.644582629\n",
            "\n",
            "regression loss =  62.541832\n",
            "20/73 [=======>......................] - ETA: 10:07 - loss: 33.0223 - custom_mse: 31196.4434\n",
            "reduce_confident_loss =  0.775397897\n",
            "\n",
            "reduce_mean_cls_loss =  0.646098793\n",
            "\n",
            "regression loss =  48.7622\n",
            "21/73 [=======>......................] - ETA: 9:48 - loss: 33.8395 - custom_mse: 31985.8398 \n",
            "reduce_confident_loss =  0.774208248\n",
            "\n",
            "reduce_mean_cls_loss =  0.643363\n",
            "\n",
            "regression loss =  61.3072\n",
            "22/73 [========>.....................] - ETA: 9:32 - loss: 35.1525 - custom_mse: 33140.4570\n",
            "reduce_confident_loss =  0.772620797\n",
            "\n",
            "reduce_mean_cls_loss =  0.455321252\n",
            "\n",
            "regression loss =  30.6419563\n",
            "23/73 [========>.....................] - ETA: 9:19 - loss: 35.0097 - custom_mse: 33720.8047\n",
            "reduce_confident_loss =  0.769674\n",
            "\n",
            "reduce_mean_cls_loss =  0.314575285\n",
            "\n",
            "regression loss =  22.9447498\n",
            "24/73 [========>.....................] - ETA: 9:07 - loss: 34.5522 - custom_mse: 34352.9375\n",
            "reduce_confident_loss =  0.772235513\n",
            "\n",
            "reduce_mean_cls_loss =  0.313487381\n",
            "\n",
            "regression loss =  22.355423\n",
            "25/73 [=========>....................] - ETA: 9:09 - loss: 34.1078 - custom_mse: 35166.2930\n",
            "reduce_confident_loss =  0.775895298\n",
            "\n",
            "reduce_mean_cls_loss =  0.314942062\n",
            "\n",
            "regression loss =  27.2583809\n",
            "26/73 [=========>....................] - ETA: 8:52 - loss: 33.8863 - custom_mse: 37474.4453\n",
            "reduce_confident_loss =  0.772681713\n",
            "\n",
            "reduce_mean_cls_loss =  0.314617068\n",
            "\n",
            "regression loss =  17.25527\n",
            "27/73 [==========>...................] - ETA: 8:36 - loss: 33.3106 - custom_mse: 39893.6211\n",
            "reduce_confident_loss =  0.749873102\n",
            "\n",
            "reduce_mean_cls_loss =  0.314230949\n",
            "\n",
            "regression loss =  15.885601\n",
            "28/73 [==========>...................] - ETA: 8:20 - loss: 32.7263 - custom_mse: 41253.0586\n",
            "reduce_confident_loss =  0.740261436\n",
            "\n",
            "reduce_mean_cls_loss =  0.321549714\n",
            "\n",
            "regression loss =  13.784976\n",
            "29/73 [==========>...................] - ETA: 8:05 - loss: 32.1097 - custom_mse: 42357.1680\n",
            "reduce_confident_loss =  0.752272487\n",
            "\n",
            "reduce_mean_cls_loss =  0.335137933\n",
            "\n",
            "regression loss =  15.9245396\n",
            "30/73 [===========>..................] - ETA: 7:50 - loss: 31.6065 - custom_mse: 43551.3477\n",
            "reduce_confident_loss =  0.755753\n",
            "\n",
            "reduce_mean_cls_loss =  0.405797929\n",
            "\n",
            "regression loss =  20.8438129\n",
            "31/73 [===========>..................] - ETA: 7:35 - loss: 31.2968 - custom_mse: 44892.2852\n",
            "reduce_confident_loss =  0.75487417\n",
            "\n",
            "reduce_mean_cls_loss =  0.407232553\n",
            "\n",
            "regression loss =  20.7437363\n",
            "32/73 [============>.................] - ETA: 7:21 - loss: 31.0033 - custom_mse: 46283.3867\n",
            "reduce_confident_loss =  0.75611943\n",
            "\n",
            "reduce_mean_cls_loss =  0.314005226\n",
            "\n",
            "regression loss =  12.7124186\n",
            "33/73 [============>.................] - ETA: 7:07 - loss: 30.4815 - custom_mse: 46927.3516\n",
            "reduce_confident_loss =  0.771018207\n",
            "\n",
            "reduce_mean_cls_loss =  0.314472944\n",
            "\n",
            "regression loss =  6.71530151\n",
            "34/73 [============>.................] - ETA: 6:54 - loss: 29.8144 - custom_mse: 47808.7734\n",
            "reduce_confident_loss =  0.775249302\n",
            "\n",
            "reduce_mean_cls_loss =  0.316611111\n",
            "\n",
            "regression loss =  2.69400907\n",
            "35/73 [=============>................] - ETA: 6:41 - loss: 29.0707 - custom_mse: 48744.2578\n",
            "reduce_confident_loss =  0.76298058\n",
            "\n",
            "reduce_mean_cls_loss =  0.314346045\n",
            "\n",
            "regression loss =  69.7689438\n",
            "36/73 [=============>................] - ETA: 6:35 - loss: 30.2311 - custom_mse: 54875.7266\n",
            "reduce_confident_loss =  0.763176501\n",
            "\n",
            "reduce_mean_cls_loss =  1.23144042\n",
            "\n",
            "regression loss =  112.687538\n",
            "37/73 [==============>...............] - ETA: 6:22 - loss: 32.5136 - custom_mse: 60794.9688\n",
            "reduce_confident_loss =  0.749530494\n",
            "\n",
            "reduce_mean_cls_loss =  0.743194461\n",
            "\n",
            "regression loss =  85.3375854\n",
            "38/73 [==============>...............] - ETA: 6:14 - loss: 33.9430 - custom_mse: 63377.2188\n",
            "reduce_confident_loss =  0.771062315\n",
            "\n",
            "reduce_mean_cls_loss =  1.2880646\n",
            "\n",
            "regression loss =  116.179283\n",
            "39/73 [===============>..............] - ETA: 6:01 - loss: 36.1044 - custom_mse: 68338.8359\n",
            "reduce_confident_loss =  0.756211281\n",
            "\n",
            "reduce_mean_cls_loss =  1.31110549\n",
            "\n",
            "regression loss =  118.891556\n",
            "40/73 [===============>..............] - ETA: 5:49 - loss: 38.2258 - custom_mse: 72680.9766\n",
            "reduce_confident_loss =  0.763355792\n",
            "\n",
            "reduce_mean_cls_loss =  1.30445802\n",
            "\n",
            "regression loss =  143.778397\n",
            "41/73 [===============>..............] - ETA: 5:40 - loss: 40.8507 - custom_mse: 93586.8906\n",
            "reduce_confident_loss =  0.74448359\n",
            "\n",
            "reduce_mean_cls_loss =  1.29700768\n",
            "\n",
            "regression loss =  157.348526\n",
            "42/73 [================>.............] - ETA: 5:28 - loss: 43.6730 - custom_mse: 117698.5781\n",
            "reduce_confident_loss =  0.740805209\n",
            "\n",
            "reduce_mean_cls_loss =  1.25716853\n",
            "\n",
            "regression loss =  131.725723\n",
            "43/73 [================>.............] - ETA: 5:32 - loss: 45.7672 - custom_mse: 136693.8750\n",
            "reduce_confident_loss =  0.760663331\n",
            "\n",
            "reduce_mean_cls_loss =  1.28802657\n",
            "\n",
            "regression loss =  10.6462107\n",
            "44/73 [=================>............] - ETA: 5:19 - loss: 45.0156 - custom_mse: 139420.0781\n",
            "reduce_confident_loss =  0.729550302\n",
            "\n",
            "reduce_mean_cls_loss =  1.29404724\n",
            "\n",
            "regression loss =  15.1004915\n",
            "45/73 [=================>............] - ETA: 5:06 - loss: 44.3958 - custom_mse: 142743.3438\n",
            "reduce_confident_loss =  0.738022804\n",
            "\n",
            "reduce_mean_cls_loss =  1.29634893\n",
            "\n",
            "regression loss =  33.1331253\n",
            "46/73 [=================>............] - ETA: 4:56 - loss: 44.1952 - custom_mse: 148097.9375\n",
            "reduce_confident_loss =  0.740824342\n",
            "\n",
            "reduce_mean_cls_loss =  1.30034411\n",
            "\n",
            "regression loss =  56.7626114\n",
            "47/73 [==================>...........] - ETA: 4:44 - loss: 44.5060 - custom_mse: 156186.3750\n",
            "reduce_confident_loss =  0.722458\n",
            "\n",
            "reduce_mean_cls_loss =  0.750681221\n",
            "\n",
            "regression loss =  20.6778355\n",
            "48/73 [==================>...........] - ETA: 4:32 - loss: 44.0402 - custom_mse: 159467.9531\n",
            "reduce_confident_loss =  0.717085183\n",
            "\n",
            "reduce_mean_cls_loss =  0.320545942\n",
            "\n",
            "regression loss =  20.3071632\n",
            "49/73 [===================>..........] - ETA: 4:20 - loss: 43.5771 - custom_mse: 161221.0469\n",
            "reduce_confident_loss =  0.728775203\n",
            "\n",
            "reduce_mean_cls_loss =  0.315465271\n",
            "\n",
            "regression loss =  19.4444752\n",
            "50/73 [===================>..........] - ETA: 4:08 - loss: 43.1153 - custom_mse: 162465.1562\n",
            "reduce_confident_loss =  0.716379583\n",
            "\n",
            "reduce_mean_cls_loss =  0.367952526\n",
            "\n",
            "regression loss =  14.7033854\n",
            "51/73 [===================>..........] - ETA: 3:56 - loss: 42.5795 - custom_mse: 163526.3281\n",
            "reduce_confident_loss =  0.723564088\n",
            "\n",
            "reduce_mean_cls_loss =  0.325638324\n",
            "\n",
            "regression loss =  11.7988367\n",
            "52/73 [====================>.........] - ETA: 3:43 - loss: 42.0077 - custom_mse: 164470.7344\n",
            "reduce_confident_loss =  0.728882849\n",
            "\n",
            "reduce_mean_cls_loss =  0.330581963\n",
            "\n",
            "regression loss =  55.3844109\n",
            "53/73 [====================>.........] - ETA: 3:33 - loss: 42.2801 - custom_mse: 166678.5312\n",
            "reduce_confident_loss =  0.734196961\n",
            "\n",
            "reduce_mean_cls_loss =  1.23495448\n",
            "\n",
            "regression loss =  187.176056\n",
            "54/73 [=====================>........] - ETA: 3:21 - loss: 44.9998 - custom_mse: 184208.0000\n",
            "reduce_confident_loss =  0.736376107\n",
            "\n",
            "reduce_mean_cls_loss =  1.27490783\n",
            "\n",
            "regression loss =  178.740967\n",
            "55/73 [=====================>........] - ETA: 3:09 - loss: 47.4680 - custom_mse: 200185.4844\n",
            "reduce_confident_loss =  0.749042869\n",
            "\n",
            "reduce_mean_cls_loss =  1.22925186\n",
            "\n",
            "regression loss =  172.620804\n",
            "56/73 [======================>.......] - ETA: 2:57 - loss: 49.7382 - custom_mse: 215087.8906\n",
            "reduce_confident_loss =  0.737326741\n",
            "\n",
            "reduce_mean_cls_loss =  0.903310478\n",
            "\n",
            "regression loss =  60.182621\n",
            "57/73 [======================>.......] - ETA: 2:47 - loss: 49.9503 - custom_mse: 220439.8281\n",
            "reduce_confident_loss =  0.711551607\n",
            "\n",
            "reduce_mean_cls_loss =  0.347061902\n",
            "\n",
            "regression loss =  12.9672527\n",
            "58/73 [======================>.......] - ETA: 2:35 - loss: 49.3309 - custom_mse: 222029.0000\n",
            "reduce_confident_loss =  0.716827691\n",
            "\n",
            "reduce_mean_cls_loss =  0.455914527\n",
            "\n",
            "regression loss =  31.6325302\n",
            "59/73 [=======================>......] - ETA: 2:24 - loss: 49.0508 - custom_mse: 224684.4375\n",
            "reduce_confident_loss =  0.709779203\n",
            "\n",
            "reduce_mean_cls_loss =  0.6838606\n",
            "\n",
            "regression loss =  31.7660542\n",
            "60/73 [=======================>......] - ETA: 2:13 - loss: 48.7859 - custom_mse: 226215.3594\n",
            "reduce_confident_loss =  0.705927074\n",
            "\n",
            "reduce_mean_cls_loss =  0.354016304\n",
            "\n",
            "regression loss =  12.9506397\n",
            "61/73 [========================>.....] - ETA: 2:02 - loss: 48.2158 - custom_mse: 227139.3438\n",
            "reduce_confident_loss =  0.747963786\n",
            "\n",
            "reduce_mean_cls_loss =  0.314957112\n",
            "\n",
            "regression loss =  60.812706\n",
            "62/73 [========================>.....] - ETA: 1:51 - loss: 48.4362 - custom_mse: 229247.7500\n",
            "reduce_confident_loss =  0.718745649\n",
            "\n",
            "reduce_mean_cls_loss =  0.848294139\n",
            "\n",
            "regression loss =  84.9733887\n",
            "63/73 [========================>.....] - ETA: 1:41 - loss: 49.0410 - custom_mse: 231794.9531\n",
            "reduce_confident_loss =  0.716606557\n",
            "\n",
            "reduce_mean_cls_loss =  0.532638729\n",
            "\n",
            "regression loss =  26.7308769\n",
            "64/73 [=========================>....] - ETA: 1:30 - loss: 48.7119 - custom_mse: 233846.0312\n",
            "reduce_confident_loss =  0.732061267\n",
            "\n",
            "reduce_mean_cls_loss =  0.361900717\n",
            "\n",
            "regression loss =  7.36516905\n",
            "65/73 [=========================>....] - ETA: 1:20 - loss: 48.0926 - custom_mse: 236045.1719\n",
            "reduce_confident_loss =  0.744295299\n",
            "\n",
            "reduce_mean_cls_loss =  0.327713192\n",
            "\n",
            "regression loss =  12.550312\n",
            "66/73 [==========================>...] - ETA: 1:09 - loss: 47.5704 - custom_mse: 238216.5938\n",
            "reduce_confident_loss =  0.740421772\n",
            "\n",
            "reduce_mean_cls_loss =  0.337158233\n",
            "\n",
            "regression loss =  13.8483658\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 47.0831 - custom_mse: 241005.5938 \n",
            "reduce_confident_loss =  0.727569282\n",
            "\n",
            "reduce_mean_cls_loss =  0.407288581\n",
            "\n",
            "regression loss =  14.8297062\n",
            "68/73 [==========================>...] - ETA: 49s - loss: 46.6255 - custom_mse: 243662.6562\n",
            "reduce_confident_loss =  0.745588422\n",
            "\n",
            "reduce_mean_cls_loss =  0.317019\n",
            "\n",
            "regression loss =  8.3432312\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 46.0861 - custom_mse: 246342.9375\n",
            "reduce_confident_loss =  0.751513779\n",
            "\n",
            "reduce_mean_cls_loss =  0.317114919\n",
            "\n",
            "regression loss =  10.1685305\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 45.5882 - custom_mse: 249048.6094\n",
            "reduce_confident_loss =  0.745644927\n",
            "\n",
            "reduce_mean_cls_loss =  0.347856134\n",
            "\n",
            "regression loss =  12.9450006\n",
            "71/73 [============================>.] - ETA: 19s - loss: 45.1439 - custom_mse: 250885.2969\n",
            "reduce_confident_loss =  0.729928553\n",
            "\n",
            "reduce_mean_cls_loss =  0.315925509\n",
            "\n",
            "regression loss =  8.89806747\n",
            "72/73 [============================>.] - ETA: 9s - loss: 44.6550 - custom_mse: 252630.7500 \n",
            "reduce_confident_loss =  0.743988037\n",
            "\n",
            "reduce_mean_cls_loss =  0.31781584\n",
            "\n",
            "regression loss =  5.14646864\n",
            "73/73 [==============================] - ETA: 0s - loss: 44.3898 - custom_mse: 254693.5312\n",
            "reduce_confident_loss =  0.753091156\n",
            "\n",
            "reduce_mean_cls_loss =  0.314429849\n",
            "\n",
            "regression loss =  16.4039459\n",
            "\n",
            "reduce_confident_loss =  0.753337264\n",
            "\n",
            "reduce_mean_cls_loss =  0.314150423\n",
            "\n",
            "regression loss =  10.6259565\n",
            "\n",
            "reduce_confident_loss =  0.753609598\n",
            "\n",
            "reduce_mean_cls_loss =  0.314335853\n",
            "\n",
            "regression loss =  13.3678856\n",
            "\n",
            "reduce_confident_loss =  0.75405395\n",
            "\n",
            "reduce_mean_cls_loss =  0.314089984\n",
            "\n",
            "regression loss =  14.9664507\n",
            "\n",
            "reduce_confident_loss =  0.741271\n",
            "\n",
            "reduce_mean_cls_loss =  0.623096585\n",
            "\n",
            "regression loss =  52.1604424\n",
            "\n",
            "reduce_confident_loss =  0.741220772\n",
            "\n",
            "reduce_mean_cls_loss =  0.475090295\n",
            "\n",
            "regression loss =  30.6033535\n",
            "\n",
            "reduce_confident_loss =  0.752860725\n",
            "\n",
            "reduce_mean_cls_loss =  0.314533263\n",
            "\n",
            "regression loss =  14.2481146\n",
            "\n",
            "reduce_confident_loss =  0.752632141\n",
            "\n",
            "reduce_mean_cls_loss =  0.314522445\n",
            "\n",
            "regression loss =  14.302104\n",
            "\n",
            "reduce_confident_loss =  0.752371669\n",
            "\n",
            "reduce_mean_cls_loss =  0.3145096\n",
            "\n",
            "regression loss =  14.4859524\n",
            "\n",
            "reduce_confident_loss =  0.75258857\n",
            "\n",
            "reduce_mean_cls_loss =  0.314510643\n",
            "\n",
            "regression loss =  14.4840794\n",
            "\n",
            "reduce_confident_loss =  0.752793252\n",
            "\n",
            "reduce_mean_cls_loss =  0.314606488\n",
            "\n",
            "regression loss =  12.2380619\n",
            "\n",
            "reduce_confident_loss =  0.751225531\n",
            "\n",
            "reduce_mean_cls_loss =  0.314334869\n",
            "\n",
            "regression loss =  12.1191149\n",
            "\n",
            "reduce_confident_loss =  0.752455115\n",
            "\n",
            "reduce_mean_cls_loss =  0.314696103\n",
            "\n",
            "regression loss =  12.2674789\n",
            "\n",
            "reduce_confident_loss =  0.753358\n",
            "\n",
            "reduce_mean_cls_loss =  0.740979791\n",
            "\n",
            "regression loss =  75.0170746\n",
            "73/73 [==============================] - 840s 11s/step - loss: 44.3898 - custom_mse: 254693.5312 - val_loss: 21.3553 - val_custom_mse: 23399.3125\n",
            "Epoch 5/15\n",
            "\n",
            "reduce_confident_loss =  0.747437477\n",
            "\n",
            "reduce_mean_cls_loss =  1.11688423\n",
            "\n",
            "regression loss =  98.8959\n",
            " 1/73 [..............................] - ETA: 31:57 - loss: 100.7602 - custom_mse: 5186.3896\n",
            "reduce_confident_loss =  0.736678481\n",
            "\n",
            "reduce_mean_cls_loss =  0.652366936\n",
            "\n",
            "regression loss =  45.4002724\n",
            " 2/73 [..............................] - ETA: 11:57 - loss: 73.7748 - custom_mse: 6485.7285 \n",
            "reduce_confident_loss =  0.751146376\n",
            "\n",
            "reduce_mean_cls_loss =  0.646441579\n",
            "\n",
            "regression loss =  31.8681393\n",
            " 3/73 [>.............................] - ETA: 11:17 - loss: 60.2718 - custom_mse: 8153.9004\n",
            "reduce_confident_loss =  0.715552211\n",
            "\n",
            "reduce_mean_cls_loss =  0.650491059\n",
            "\n",
            "regression loss =  30.2892704\n",
            " 4/73 [>.............................] - ETA: 10:54 - loss: 53.1176 - custom_mse: 9880.6611\n",
            "reduce_confident_loss =  0.725756943\n",
            "\n",
            "reduce_mean_cls_loss =  0.649701178\n",
            "\n",
            "regression loss =  28.1279926\n",
            " 5/73 [=>............................] - ETA: 10:42 - loss: 48.3948 - custom_mse: 11503.1455\n",
            "reduce_confident_loss =  0.732389\n",
            "\n",
            "reduce_mean_cls_loss =  0.645301878\n",
            "\n",
            "regression loss =  28.6118622\n",
            " 6/73 [=>............................] - ETA: 10:28 - loss: 45.3273 - custom_mse: 13033.7031\n",
            "reduce_confident_loss =  0.728944659\n",
            "\n",
            "reduce_mean_cls_loss =  0.646068\n",
            "\n",
            "regression loss =  39.6276169\n",
            " 7/73 [=>............................] - ETA: 10:21 - loss: 44.7095 - custom_mse: 14679.8018\n",
            "reduce_confident_loss =  0.730857849\n",
            "\n",
            "reduce_mean_cls_loss =  0.423899084\n",
            "\n",
            "regression loss =  23.0702477\n",
            " 8/73 [==>...........................] - ETA: 10:28 - loss: 42.1489 - custom_mse: 15579.7158\n",
            "reduce_confident_loss =  0.738522\n",
            "\n",
            "reduce_mean_cls_loss =  0.313908577\n",
            "\n",
            "regression loss =  7.6148057\n",
            " 9/73 [==>...........................] - ETA: 10:12 - loss: 38.4287 - custom_mse: 16581.9668\n",
            "reduce_confident_loss =  0.734054387\n",
            "\n",
            "reduce_mean_cls_loss =  0.324312508\n",
            "\n",
            "regression loss =  7.21693134\n",
            "10/73 [===>..........................] - ETA: 9:58 - loss: 35.4134 - custom_mse: 17862.0820 \n",
            "reduce_confident_loss =  0.726179123\n",
            "\n",
            "reduce_mean_cls_loss =  0.345522314\n",
            "\n",
            "regression loss =  25.9421768\n",
            "11/73 [===>..........................] - ETA: 9:44 - loss: 34.6498 - custom_mse: 19118.5859\n",
            "reduce_confident_loss =  0.733609736\n",
            "\n",
            "reduce_mean_cls_loss =  0.320141971\n",
            "\n",
            "regression loss =  18.8524876\n",
            "12/73 [===>..........................] - ETA: 9:31 - loss: 33.4212 - custom_mse: 22871.5059\n",
            "reduce_confident_loss =  0.714744866\n",
            "\n",
            "reduce_mean_cls_loss =  0.316256613\n",
            "\n",
            "regression loss =  14.795516\n",
            "13/73 [====>.........................] - ETA: 9:19 - loss: 32.0677 - custom_mse: 25938.6191\n",
            "reduce_confident_loss =  0.711560309\n",
            "\n",
            "reduce_mean_cls_loss =  0.400884837\n",
            "\n",
            "regression loss =  9.8004694\n",
            "14/73 [====>.........................] - ETA: 9:07 - loss: 30.5567 - custom_mse: 28238.3008\n",
            "reduce_confident_loss =  0.69952625\n",
            "\n",
            "reduce_mean_cls_loss =  0.324880332\n",
            "\n",
            "regression loss =  5.87195492\n",
            "15/73 [=====>........................] - ETA: 8:56 - loss: 28.9793 - custom_mse: 30782.9727\n",
            "reduce_confident_loss =  0.713392317\n",
            "\n",
            "reduce_mean_cls_loss =  0.316355705\n",
            "\n",
            "regression loss =  4.42024803\n",
            "16/73 [=====>........................] - ETA: 8:45 - loss: 27.5087 - custom_mse: 33583.9883\n",
            "reduce_confident_loss =  0.727349401\n",
            "\n",
            "reduce_mean_cls_loss =  0.328327984\n",
            "\n",
            "regression loss =  21.6391335\n",
            "17/73 [=====>........................] - ETA: 8:36 - loss: 27.2256 - custom_mse: 36800.2305\n",
            "reduce_confident_loss =  0.726657033\n",
            "\n",
            "reduce_mean_cls_loss =  0.650836289\n",
            "\n",
            "regression loss =  50.2414436\n",
            "18/73 [======>.......................] - ETA: 8:26 - loss: 28.5807 - custom_mse: 39192.5039\n",
            "reduce_confident_loss =  0.708331\n",
            "\n",
            "reduce_mean_cls_loss =  0.658371031\n",
            "\n",
            "regression loss =  57.2328262\n",
            "19/73 [======>.......................] - ETA: 10:26 - loss: 30.1607 - custom_mse: 41084.7852\n",
            "reduce_confident_loss =  0.717530131\n",
            "\n",
            "reduce_mean_cls_loss =  0.641780913\n",
            "\n",
            "regression loss =  56.9196968\n",
            "20/73 [=======>......................] - ETA: 10:07 - loss: 31.5666 - custom_mse: 42401.5977\n",
            "reduce_confident_loss =  0.717058361\n",
            "\n",
            "reduce_mean_cls_loss =  0.63316834\n",
            "\n",
            "regression loss =  44.45084\n",
            "21/73 [=======>......................] - ETA: 9:50 - loss: 32.2444 - custom_mse: 43530.2305 \n",
            "reduce_confident_loss =  0.700985432\n",
            "\n",
            "reduce_mean_cls_loss =  0.622425616\n",
            "\n",
            "regression loss =  54.84198\n",
            "22/73 [========>.....................] - ETA: 9:33 - loss: 33.3317 - custom_mse: 45041.3477\n",
            "reduce_confident_loss =  0.720196187\n",
            "\n",
            "reduce_mean_cls_loss =  0.454835\n",
            "\n",
            "regression loss =  32.4933815\n",
            "23/73 [========>.....................] - ETA: 9:20 - loss: 33.3464 - custom_mse: 46090.7461\n",
            "reduce_confident_loss =  0.721134186\n",
            "\n",
            "reduce_mean_cls_loss =  0.316086531\n",
            "\n",
            "regression loss =  24.2266655\n",
            "24/73 [========>.....................] - ETA: 9:08 - loss: 33.0096 - custom_mse: 47379.7031\n",
            "reduce_confident_loss =  0.722642\n",
            "\n",
            "reduce_mean_cls_loss =  0.313884556\n",
            "\n",
            "regression loss =  21.8648968\n",
            "25/73 [=========>....................] - ETA: 9:10 - loss: 32.6053 - custom_mse: 48684.9648\n",
            "reduce_confident_loss =  0.708172381\n",
            "\n",
            "reduce_mean_cls_loss =  0.328854412\n",
            "\n",
            "regression loss =  28.7634945\n",
            "26/73 [=========>....................] - ETA: 8:52 - loss: 32.4974 - custom_mse: 52437.3945\n",
            "reduce_confident_loss =  0.713771403\n",
            "\n",
            "reduce_mean_cls_loss =  0.320382267\n",
            "\n",
            "regression loss =  17.2852402\n",
            "27/73 [==========>...................] - ETA: 8:36 - loss: 31.9723 - custom_mse: 56018.7969\n",
            "reduce_confident_loss =  0.706950068\n",
            "\n",
            "reduce_mean_cls_loss =  0.3154746\n",
            "\n",
            "regression loss =  9.40591335\n",
            "28/73 [==========>...................] - ETA: 8:20 - loss: 31.2029 - custom_mse: 57566.5820\n",
            "reduce_confident_loss =  0.71258229\n",
            "\n",
            "reduce_mean_cls_loss =  0.343841791\n",
            "\n",
            "regression loss =  7.24942589\n",
            "29/73 [==========>...................] - ETA: 8:05 - loss: 30.4133 - custom_mse: 58642.4453\n",
            "reduce_confident_loss =  0.699946105\n",
            "\n",
            "reduce_mean_cls_loss =  0.323076814\n",
            "\n",
            "regression loss =  12.4217958\n",
            "30/73 [===========>..................] - ETA: 7:50 - loss: 29.8477 - custom_mse: 60001.4609\n",
            "reduce_confident_loss =  0.695805192\n",
            "\n",
            "reduce_mean_cls_loss =  0.415745467\n",
            "\n",
            "regression loss =  18.5475025\n",
            "31/73 [===========>..................] - ETA: 7:36 - loss: 29.5190 - custom_mse: 61615.4258\n",
            "reduce_confident_loss =  0.701948225\n",
            "\n",
            "reduce_mean_cls_loss =  0.411501348\n",
            "\n",
            "regression loss =  20.7359924\n",
            "32/73 [============>.................] - ETA: 7:22 - loss: 29.2794 - custom_mse: 63330.9766\n",
            "reduce_confident_loss =  0.724472106\n",
            "\n",
            "reduce_mean_cls_loss =  0.313998193\n",
            "\n",
            "regression loss =  13.3598213\n",
            "33/73 [============>.................] - ETA: 7:08 - loss: 28.8284 - custom_mse: 64554.8750\n",
            "reduce_confident_loss =  0.707496345\n",
            "\n",
            "reduce_mean_cls_loss =  0.340841681\n",
            "\n",
            "regression loss =  4.97907448\n",
            "34/73 [============>.................] - ETA: 6:54 - loss: 28.1578 - custom_mse: 66455.6016\n",
            "reduce_confident_loss =  0.72419548\n",
            "\n",
            "reduce_mean_cls_loss =  0.343999088\n",
            "\n",
            "regression loss =  5.98763371\n",
            "35/73 [=============>................] - ETA: 6:41 - loss: 27.5549 - custom_mse: 68630.9609\n",
            "reduce_confident_loss =  0.701849401\n",
            "\n",
            "reduce_mean_cls_loss =  0.340495855\n",
            "\n",
            "regression loss =  59.337677\n",
            "36/73 [=============>................] - ETA: 6:34 - loss: 28.4667 - custom_mse: 73897.6562\n",
            "reduce_confident_loss =  0.724201202\n",
            "\n",
            "reduce_mean_cls_loss =  1.20454967\n",
            "\n",
            "regression loss =  97.4687805\n",
            "37/73 [==============>...............] - ETA: 6:21 - loss: 30.3837 - custom_mse: 79212.1172\n",
            "reduce_confident_loss =  0.716530919\n",
            "\n",
            "reduce_mean_cls_loss =  0.74560225\n",
            "\n",
            "regression loss =  74.2144928\n",
            "38/73 [==============>...............] - ETA: 6:14 - loss: 31.5757 - custom_mse: 81764.2656\n",
            "reduce_confident_loss =  0.707870305\n",
            "\n",
            "reduce_mean_cls_loss =  1.29020858\n",
            "\n",
            "regression loss =  103.498871\n",
            "39/73 [===============>..............] - ETA: 6:01 - loss: 33.4711 - custom_mse: 84980.5000\n",
            "reduce_confident_loss =  0.698873043\n",
            "\n",
            "reduce_mean_cls_loss =  1.28525841\n",
            "\n",
            "regression loss =  110.941269\n",
            "40/73 [===============>..............] - ETA: 5:49 - loss: 35.4574 - custom_mse: 87734.7188\n",
            "reduce_confident_loss =  0.70115608\n",
            "\n",
            "reduce_mean_cls_loss =  1.28418517\n",
            "\n",
            "regression loss =  139.024216\n",
            "41/73 [===============>..............] - ETA: 5:40 - loss: 38.0319 - custom_mse: 112135.1875\n",
            "reduce_confident_loss =  0.695433736\n",
            "\n",
            "reduce_mean_cls_loss =  1.2910955\n",
            "\n",
            "regression loss =  154.083908\n",
            "42/73 [================>.............] - ETA: 5:27 - loss: 40.8423 - custom_mse: 139692.2656\n",
            "reduce_confident_loss =  0.69667238\n",
            "\n",
            "reduce_mean_cls_loss =  1.19121814\n",
            "\n",
            "regression loss =  125.525383\n",
            "43/73 [================>.............] - ETA: 5:32 - loss: 42.8556 - custom_mse: 161883.0938\n",
            "reduce_confident_loss =  0.745793402\n",
            "\n",
            "reduce_mean_cls_loss =  1.26709926\n",
            "\n",
            "regression loss =  10.008523\n",
            "44/73 [=================>............] - ETA: 5:19 - loss: 42.1548 - custom_mse: 166078.3125\n",
            "reduce_confident_loss =  0.722450674\n",
            "\n",
            "reduce_mean_cls_loss =  1.26707971\n",
            "\n",
            "regression loss =  10.7670193\n",
            "45/73 [=================>............] - ETA: 5:06 - loss: 41.5015 - custom_mse: 170984.8906\n",
            "reduce_confident_loss =  0.707460046\n",
            "\n",
            "reduce_mean_cls_loss =  1.21123302\n",
            "\n",
            "regression loss =  25.0952148\n",
            "46/73 [=================>............] - ETA: 4:56 - loss: 41.1866 - custom_mse: 178579.6562\n",
            "reduce_confident_loss =  0.718672693\n",
            "\n",
            "reduce_mean_cls_loss =  1.25315821\n",
            "\n",
            "regression loss =  46.892025\n",
            "47/73 [==================>...........] - ETA: 4:44 - loss: 41.3499 - custom_mse: 189432.8281\n",
            "reduce_confident_loss =  0.73296839\n",
            "\n",
            "reduce_mean_cls_loss =  0.712889314\n",
            "\n",
            "regression loss =  18.7812614\n",
            "48/73 [==================>...........] - ETA: 4:33 - loss: 40.9099 - custom_mse: 194326.4062\n",
            "reduce_confident_loss =  0.696247518\n",
            "\n",
            "reduce_mean_cls_loss =  0.319436699\n",
            "\n",
            "regression loss =  18.6966801\n",
            "49/73 [===================>..........] - ETA: 4:20 - loss: 40.4773 - custom_mse: 197463.8750\n",
            "reduce_confident_loss =  0.700377882\n",
            "\n",
            "reduce_mean_cls_loss =  0.316440642\n",
            "\n",
            "regression loss =  14.8203497\n",
            "50/73 [===================>..........] - ETA: 4:09 - loss: 39.9845 - custom_mse: 199504.9375\n",
            "reduce_confident_loss =  0.731721222\n",
            "\n",
            "reduce_mean_cls_loss =  0.329699248\n",
            "\n",
            "regression loss =  4.94510031\n",
            "51/73 [===================>..........] - ETA: 3:56 - loss: 39.3182 - custom_mse: 201067.3281\n",
            "reduce_confident_loss =  0.723131239\n",
            "\n",
            "reduce_mean_cls_loss =  0.329732329\n",
            "\n",
            "regression loss =  4.0955925\n",
            "52/73 [====================>.........] - ETA: 3:44 - loss: 38.6611 - custom_mse: 202451.1562\n",
            "reduce_confident_loss =  0.720133841\n",
            "\n",
            "reduce_mean_cls_loss =  0.369206369\n",
            "\n",
            "regression loss =  50.6021271\n",
            "53/73 [====================>.........] - ETA: 3:33 - loss: 38.9070 - custom_mse: 205407.7344\n",
            "reduce_confident_loss =  0.745266795\n",
            "\n",
            "reduce_mean_cls_loss =  1.1711309\n",
            "\n",
            "regression loss =  159.524139\n",
            "54/73 [=====================>........] - ETA: 3:21 - loss: 41.1761 - custom_mse: 218144.3438\n",
            "reduce_confident_loss =  0.732693672\n",
            "\n",
            "reduce_mean_cls_loss =  1.13591874\n",
            "\n",
            "regression loss =  148.892761\n",
            "55/73 [=====================>........] - ETA: 3:09 - loss: 43.1686 - custom_mse: 229233.8438\n",
            "reduce_confident_loss =  0.739144862\n",
            "\n",
            "reduce_mean_cls_loss =  1.16120517\n",
            "\n",
            "regression loss =  143.464798\n",
            "56/73 [======================>.......] - ETA: 2:58 - loss: 44.9935 - custom_mse: 239529.7656\n",
            "reduce_confident_loss =  0.749770761\n",
            "\n",
            "reduce_mean_cls_loss =  0.878044724\n",
            "\n",
            "regression loss =  48.3845291\n",
            "57/73 [======================>.......] - ETA: 2:47 - loss: 45.0816 - custom_mse: 244227.3125\n",
            "reduce_confident_loss =  0.703273356\n",
            "\n",
            "reduce_mean_cls_loss =  0.388958037\n",
            "\n",
            "regression loss =  9.89193344\n",
            "58/73 [======================>.......] - ETA: 2:35 - loss: 44.4937 - custom_mse: 246818.9062\n",
            "reduce_confident_loss =  0.697791278\n",
            "\n",
            "reduce_mean_cls_loss =  0.468865663\n",
            "\n",
            "regression loss =  27.8679562\n",
            "59/73 [=======================>......] - ETA: 2:24 - loss: 44.2317 - custom_mse: 251252.3438\n",
            "reduce_confident_loss =  0.694988787\n",
            "\n",
            "reduce_mean_cls_loss =  0.745733798\n",
            "\n",
            "regression loss =  27.8463058\n",
            "60/73 [=======================>......] - ETA: 2:13 - loss: 43.9826 - custom_mse: 254088.2812\n",
            "reduce_confident_loss =  0.697052479\n",
            "\n",
            "reduce_mean_cls_loss =  0.40260765\n",
            "\n",
            "regression loss =  10.3922291\n",
            "61/73 [========================>.....] - ETA: 2:02 - loss: 43.4500 - custom_mse: 256325.2344\n",
            "reduce_confident_loss =  0.711222708\n",
            "\n",
            "reduce_mean_cls_loss =  0.377098978\n",
            "\n",
            "regression loss =  56.0226784\n",
            "62/73 [========================>.....] - ETA: 1:52 - loss: 43.6703 - custom_mse: 260889.6719\n",
            "reduce_confident_loss =  0.704042435\n",
            "\n",
            "reduce_mean_cls_loss =  0.878199279\n",
            "\n",
            "regression loss =  81.8687439\n",
            "63/73 [========================>.....] - ETA: 1:41 - loss: 44.3017 - custom_mse: 265605.7188\n",
            "reduce_confident_loss =  0.716604292\n",
            "\n",
            "reduce_mean_cls_loss =  0.575744748\n",
            "\n",
            "regression loss =  23.6535645\n",
            "64/73 [=========================>....] - ETA: 1:30 - loss: 43.9993 - custom_mse: 268946.9375\n",
            "reduce_confident_loss =  0.706701875\n",
            "\n",
            "reduce_mean_cls_loss =  0.398009777\n",
            "\n",
            "regression loss =  5.66818523\n",
            "65/73 [=========================>....] - ETA: 1:20 - loss: 43.4266 - custom_mse: 272288.0938\n",
            "reduce_confident_loss =  0.705581486\n",
            "\n",
            "reduce_mean_cls_loss =  0.38825205\n",
            "\n",
            "regression loss =  9.63154\n",
            "66/73 [==========================>...] - ETA: 1:09 - loss: 42.9311 - custom_mse: 276322.1875\n",
            "reduce_confident_loss =  0.723293602\n",
            "\n",
            "reduce_mean_cls_loss =  0.419077635\n",
            "\n",
            "regression loss =  14.6678524\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 42.5263 - custom_mse: 281947.6562 \n",
            "reduce_confident_loss =  0.708718359\n",
            "\n",
            "reduce_mean_cls_loss =  0.337335497\n",
            "\n",
            "regression loss =  15.1820011\n",
            "68/73 [==========================>...] - ETA: 49s - loss: 42.1396 - custom_mse: 286661.3125\n",
            "reduce_confident_loss =  0.722462177\n",
            "\n",
            "reduce_mean_cls_loss =  0.345433235\n",
            "\n",
            "regression loss =  6.0607729\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 41.6322 - custom_mse: 291273.6875\n",
            "reduce_confident_loss =  0.727915466\n",
            "\n",
            "reduce_mean_cls_loss =  0.36753884\n",
            "\n",
            "regression loss =  9.49605\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 41.1887 - custom_mse: 296864.3750\n",
            "reduce_confident_loss =  0.710613072\n",
            "\n",
            "reduce_mean_cls_loss =  0.350257963\n",
            "\n",
            "regression loss =  4.09243202\n",
            "71/73 [============================>.] - ETA: 19s - loss: 40.6812 - custom_mse: 300498.0312\n",
            "reduce_confident_loss =  0.703469872\n",
            "\n",
            "reduce_mean_cls_loss =  0.330332816\n",
            "\n",
            "regression loss =  11.1665964\n",
            "72/73 [============================>.] - ETA: 9s - loss: 40.2856 - custom_mse: 303672.5625 \n",
            "reduce_confident_loss =  0.714974701\n",
            "\n",
            "reduce_mean_cls_loss =  0.329193205\n",
            "\n",
            "regression loss =  7.06898642\n",
            "73/73 [==============================] - ETA: 0s - loss: 40.0638 - custom_mse: 306967.0625\n",
            "reduce_confident_loss =  0.707029939\n",
            "\n",
            "reduce_mean_cls_loss =  0.313823402\n",
            "\n",
            "regression loss =  18.1680737\n",
            "\n",
            "reduce_confident_loss =  0.706388831\n",
            "\n",
            "reduce_mean_cls_loss =  0.313707978\n",
            "\n",
            "regression loss =  12.956707\n",
            "\n",
            "reduce_confident_loss =  0.7081393\n",
            "\n",
            "reduce_mean_cls_loss =  0.313938797\n",
            "\n",
            "regression loss =  15.0265684\n",
            "\n",
            "reduce_confident_loss =  0.707809806\n",
            "\n",
            "reduce_mean_cls_loss =  0.313785851\n",
            "\n",
            "regression loss =  14.3406763\n",
            "\n",
            "reduce_confident_loss =  0.701223969\n",
            "\n",
            "reduce_mean_cls_loss =  0.62202692\n",
            "\n",
            "regression loss =  50.5180855\n",
            "\n",
            "reduce_confident_loss =  0.701996148\n",
            "\n",
            "reduce_mean_cls_loss =  0.475112438\n",
            "\n",
            "regression loss =  30.6655159\n",
            "\n",
            "reduce_confident_loss =  0.708017766\n",
            "\n",
            "reduce_mean_cls_loss =  0.314426452\n",
            "\n",
            "regression loss =  15.8619967\n",
            "\n",
            "reduce_confident_loss =  0.707537115\n",
            "\n",
            "reduce_mean_cls_loss =  0.314180702\n",
            "\n",
            "regression loss =  15.4616833\n",
            "\n",
            "reduce_confident_loss =  0.707803547\n",
            "\n",
            "reduce_mean_cls_loss =  0.314049304\n",
            "\n",
            "regression loss =  15.7315502\n",
            "\n",
            "reduce_confident_loss =  0.70784837\n",
            "\n",
            "reduce_mean_cls_loss =  0.3139157\n",
            "\n",
            "regression loss =  15.4350405\n",
            "\n",
            "reduce_confident_loss =  0.709439278\n",
            "\n",
            "reduce_mean_cls_loss =  0.314397871\n",
            "\n",
            "regression loss =  12.1153822\n",
            "\n",
            "reduce_confident_loss =  0.707919478\n",
            "\n",
            "reduce_mean_cls_loss =  0.31459707\n",
            "\n",
            "regression loss =  12.8918409\n",
            "\n",
            "reduce_confident_loss =  0.705898166\n",
            "\n",
            "reduce_mean_cls_loss =  0.314035326\n",
            "\n",
            "regression loss =  13.9753571\n",
            "\n",
            "reduce_confident_loss =  0.706062257\n",
            "\n",
            "reduce_mean_cls_loss =  0.741329\n",
            "\n",
            "regression loss =  73.6017838\n",
            "73/73 [==============================] - 842s 11s/step - loss: 40.0638 - custom_mse: 306967.0625 - val_loss: 22.0537 - val_custom_mse: 41414.7695\n",
            "Epoch 6/15\n",
            "\n",
            "reduce_confident_loss =  0.708167255\n",
            "\n",
            "reduce_mean_cls_loss =  1.11532402\n",
            "\n",
            "regression loss =  96.0729752\n",
            " 1/73 [..............................] - ETA: 32:04 - loss: 97.8965 - custom_mse: 7838.9600\n",
            "reduce_confident_loss =  0.694540918\n",
            "\n",
            "reduce_mean_cls_loss =  0.685916245\n",
            "\n",
            "regression loss =  46.2622337\n",
            " 2/73 [..............................] - ETA: 11:50 - loss: 72.7696 - custom_mse: 10495.9727\n",
            "reduce_confident_loss =  0.714110255\n",
            "\n",
            "reduce_mean_cls_loss =  0.657026112\n",
            "\n",
            "regression loss =  30.6449356\n",
            " 3/73 [>.............................] - ETA: 11:12 - loss: 59.1851 - custom_mse: 14337.1602\n",
            "reduce_confident_loss =  0.718919337\n",
            "\n",
            "reduce_mean_cls_loss =  0.647555649\n",
            "\n",
            "regression loss =  26.9540501\n",
            " 4/73 [>.............................] - ETA: 10:52 - loss: 51.4689 - custom_mse: 18361.6348\n",
            "reduce_confident_loss =  0.717748702\n",
            "\n",
            "reduce_mean_cls_loss =  0.638501465\n",
            "\n",
            "regression loss =  25.5977974\n",
            " 5/73 [=>............................] - ETA: 10:39 - loss: 46.5660 - custom_mse: 22124.0801\n",
            "reduce_confident_loss =  0.70557785\n",
            "\n",
            "reduce_mean_cls_loss =  0.641780078\n",
            "\n",
            "regression loss =  24.9859295\n",
            " 6/73 [=>............................] - ETA: 10:26 - loss: 43.1938 - custom_mse: 25949.5371\n",
            "reduce_confident_loss =  0.71599406\n",
            "\n",
            "reduce_mean_cls_loss =  0.630130768\n",
            "\n",
            "regression loss =  37.1578751\n",
            " 7/73 [=>............................] - ETA: 10:15 - loss: 42.5239 - custom_mse: 29568.6816\n",
            "reduce_confident_loss =  0.70260483\n",
            "\n",
            "reduce_mean_cls_loss =  0.409938097\n",
            "\n",
            "regression loss =  25.8042889\n",
            " 8/73 [==>...........................] - ETA: 10:22 - loss: 40.5730 - custom_mse: 31400.8516\n",
            "reduce_confident_loss =  0.711351752\n",
            "\n",
            "reduce_mean_cls_loss =  0.316265166\n",
            "\n",
            "regression loss =  9.94827366\n",
            " 9/73 [==>...........................] - ETA: 10:10 - loss: 37.2844 - custom_mse: 33371.8867\n",
            "reduce_confident_loss =  0.704037607\n",
            "\n",
            "reduce_mean_cls_loss =  0.317649156\n",
            "\n",
            "regression loss =  9.59555435\n",
            "10/73 [===>..........................] - ETA: 9:56 - loss: 34.6177 - custom_mse: 35792.9414 \n",
            "reduce_confident_loss =  0.703910291\n",
            "\n",
            "reduce_mean_cls_loss =  0.316554964\n",
            "\n",
            "regression loss =  26.9160614\n",
            "11/73 [===>..........................] - ETA: 9:43 - loss: 34.0103 - custom_mse: 38259.8438\n",
            "reduce_confident_loss =  0.71703434\n",
            "\n",
            "reduce_mean_cls_loss =  0.314880162\n",
            "\n",
            "regression loss =  34.5054169\n",
            "12/73 [===>..........................] - ETA: 9:30 - loss: 34.1376 - custom_mse: 45003.9883\n",
            "reduce_confident_loss =  0.711524\n",
            "\n",
            "reduce_mean_cls_loss =  0.313995391\n",
            "\n",
            "regression loss =  14.0798407\n",
            "13/73 [====>.........................] - ETA: 9:18 - loss: 32.6736 - custom_mse: 50247.6914\n",
            "reduce_confident_loss =  0.703380167\n",
            "\n",
            "reduce_mean_cls_loss =  0.327322721\n",
            "\n",
            "regression loss =  3.86256599\n",
            "14/73 [====>.........................] - ETA: 9:07 - loss: 30.6893 - custom_mse: 54576.5391\n",
            "reduce_confident_loss =  0.706139\n",
            "\n",
            "reduce_mean_cls_loss =  0.319767386\n",
            "\n",
            "regression loss =  5.20799494\n",
            "15/73 [=====>........................] - ETA: 8:56 - loss: 29.0589 - custom_mse: 59030.3242\n",
            "reduce_confident_loss =  0.696682632\n",
            "\n",
            "reduce_mean_cls_loss =  0.325347841\n",
            "\n",
            "regression loss =  4.71860838\n",
            "16/73 [=====>........................] - ETA: 8:45 - loss: 27.6015 - custom_mse: 63768.9648\n",
            "reduce_confident_loss =  0.716895759\n",
            "\n",
            "reduce_mean_cls_loss =  0.316014975\n",
            "\n",
            "regression loss =  17.9486275\n",
            "17/73 [=====>........................] - ETA: 8:34 - loss: 27.0944 - custom_mse: 69413.7969\n",
            "reduce_confident_loss =  0.696716189\n",
            "\n",
            "reduce_mean_cls_loss =  0.646240294\n",
            "\n",
            "regression loss =  47.063343\n",
            "18/73 [======>.......................] - ETA: 8:24 - loss: 28.2784 - custom_mse: 73685.9141\n",
            "reduce_confident_loss =  0.695108116\n",
            "\n",
            "reduce_mean_cls_loss =  0.659416616\n",
            "\n",
            "regression loss =  51.0143661\n",
            "19/73 [======>.......................] - ETA: 10:23 - loss: 29.5464 - custom_mse: 77117.7578\n",
            "reduce_confident_loss =  0.692139149\n",
            "\n",
            "reduce_mean_cls_loss =  0.650761783\n",
            "\n",
            "regression loss =  48.864666\n",
            "20/73 [=======>......................] - ETA: 10:06 - loss: 30.5794 - custom_mse: 79581.4219\n",
            "reduce_confident_loss =  0.680337429\n",
            "\n",
            "reduce_mean_cls_loss =  0.646687448\n",
            "\n",
            "regression loss =  37.6549149\n",
            "21/73 [=======>......................] - ETA: 9:48 - loss: 30.9795 - custom_mse: 81620.5156 \n",
            "reduce_confident_loss =  0.677785516\n",
            "\n",
            "reduce_mean_cls_loss =  0.639636636\n",
            "\n",
            "regression loss =  47.5996666\n",
            "22/73 [========>.....................] - ETA: 9:31 - loss: 31.7949 - custom_mse: 84289.9062\n",
            "reduce_confident_loss =  0.704572678\n",
            "\n",
            "reduce_mean_cls_loss =  0.457517445\n",
            "\n",
            "regression loss =  33.6657562\n",
            "23/73 [========>.....................] - ETA: 9:18 - loss: 31.9268 - custom_mse: 86100.6797\n",
            "reduce_confident_loss =  0.698080719\n",
            "\n",
            "reduce_mean_cls_loss =  0.315800339\n",
            "\n",
            "regression loss =  21.7598076\n",
            "24/73 [========>.....................] - ETA: 9:06 - loss: 31.5454 - custom_mse: 88582.8750\n",
            "reduce_confident_loss =  0.710637093\n",
            "\n",
            "reduce_mean_cls_loss =  0.316379547\n",
            "\n",
            "regression loss =  20.6557064\n",
            "25/73 [=========>....................] - ETA: 9:08 - loss: 31.1509 - custom_mse: 91166.0391\n",
            "reduce_confident_loss =  0.701195419\n",
            "\n",
            "reduce_mean_cls_loss =  0.31868124\n",
            "\n",
            "regression loss =  28.1205463\n",
            "26/73 [=========>....................] - ETA: 8:50 - loss: 31.0735 - custom_mse: 97283.7031\n",
            "reduce_confident_loss =  0.695013225\n",
            "\n",
            "reduce_mean_cls_loss =  0.324708313\n",
            "\n",
            "regression loss =  16.8730659\n",
            "27/73 [==========>...................] - ETA: 8:34 - loss: 30.5854 - custom_mse: 102929.0234\n",
            "reduce_confident_loss =  0.70328\n",
            "\n",
            "reduce_mean_cls_loss =  0.314656436\n",
            "\n",
            "regression loss =  9.56307697\n",
            "28/73 [==========>...................] - ETA: 8:18 - loss: 29.8709 - custom_mse: 104898.2969\n",
            "reduce_confident_loss =  0.741142452\n",
            "\n",
            "reduce_mean_cls_loss =  0.319814026\n",
            "\n",
            "regression loss =  8.44743347\n",
            "29/73 [==========>...................] - ETA: 8:02 - loss: 29.1688 - custom_mse: 106226.5547\n",
            "reduce_confident_loss =  0.718555\n",
            "\n",
            "reduce_mean_cls_loss =  0.318379223\n",
            "\n",
            "regression loss =  12.6431379\n",
            "30/73 [===========>..................] - ETA: 7:48 - loss: 28.6525 - custom_mse: 107747.1406\n",
            "reduce_confident_loss =  0.677672088\n",
            "\n",
            "reduce_mean_cls_loss =  0.413759738\n",
            "\n",
            "regression loss =  17.5629292\n",
            "31/73 [===========>..................] - ETA: 7:34 - loss: 28.3300 - custom_mse: 109398.1250\n",
            "reduce_confident_loss =  0.695604801\n",
            "\n",
            "reduce_mean_cls_loss =  0.410094678\n",
            "\n",
            "regression loss =  20.829464\n",
            "32/73 [============>.................] - ETA: 7:20 - loss: 28.1301 - custom_mse: 111464.7969\n",
            "reduce_confident_loss =  0.700292766\n",
            "\n",
            "reduce_mean_cls_loss =  0.31346491\n",
            "\n",
            "regression loss =  7.67999\n",
            "33/73 [============>.................] - ETA: 7:06 - loss: 27.5411 - custom_mse: 113860.2656\n",
            "reduce_confident_loss =  0.700157\n",
            "\n",
            "reduce_mean_cls_loss =  0.342299879\n",
            "\n",
            "regression loss =  7.66180038\n",
            "34/73 [============>.................] - ETA: 6:53 - loss: 26.9871 - custom_mse: 117791.0703\n",
            "reduce_confident_loss =  0.710114896\n",
            "\n",
            "reduce_mean_cls_loss =  0.329294056\n",
            "\n",
            "regression loss =  6.61712074\n",
            "35/73 [=============>................] - ETA: 6:39 - loss: 26.4348 - custom_mse: 121774.7969\n",
            "reduce_confident_loss =  0.710116327\n",
            "\n",
            "reduce_mean_cls_loss =  0.340072691\n",
            "\n",
            "regression loss =  50.612\n",
            "36/73 [=============>................] - ETA: 6:33 - loss: 27.1356 - custom_mse: 126490.6562\n",
            "reduce_confident_loss =  0.716874778\n",
            "\n",
            "reduce_mean_cls_loss =  1.13198674\n",
            "\n",
            "regression loss =  81.7424393\n",
            "37/73 [==============>...............] - ETA: 6:20 - loss: 28.6614 - custom_mse: 131830.7500\n",
            "reduce_confident_loss =  0.704056919\n",
            "\n",
            "reduce_mean_cls_loss =  0.778728068\n",
            "\n",
            "regression loss =  56.9806061\n",
            "38/73 [==============>...............] - ETA: 6:12 - loss: 29.4457 - custom_mse: 134844.5156\n",
            "reduce_confident_loss =  0.701287925\n",
            "\n",
            "reduce_mean_cls_loss =  1.26903069\n",
            "\n",
            "regression loss =  86.9690628\n",
            "39/73 [===============>..............] - ETA: 6:00 - loss: 30.9711 - custom_mse: 136357.4062\n",
            "reduce_confident_loss =  0.684513\n",
            "\n",
            "reduce_mean_cls_loss =  1.22071517\n",
            "\n",
            "regression loss =  99.1035461\n",
            "40/73 [===============>..............] - ETA: 5:47 - loss: 32.7221 - custom_mse: 137857.0781\n",
            "reduce_confident_loss =  0.701803207\n",
            "\n",
            "reduce_mean_cls_loss =  1.20935977\n",
            "\n",
            "regression loss =  127.753128\n",
            "41/73 [===============>..............] - ETA: 5:39 - loss: 35.0865 - custom_mse: 170044.2188\n",
            "reduce_confident_loss =  0.69990015\n",
            "\n",
            "reduce_mean_cls_loss =  1.20846772\n",
            "\n",
            "regression loss =  152.396545\n",
            "42/73 [================>.............] - ETA: 5:27 - loss: 37.9251 - custom_mse: 203928.0469\n",
            "reduce_confident_loss =  0.697931767\n",
            "\n",
            "reduce_mean_cls_loss =  1.00084734\n",
            "\n",
            "regression loss =  117.385521\n",
            "43/73 [================>.............] - ETA: 5:31 - loss: 39.8125 - custom_mse: 231536.2969\n",
            "reduce_confident_loss =  0.770636499\n",
            "\n",
            "reduce_mean_cls_loss =  1.09769976\n",
            "\n",
            "regression loss =  10.443181\n",
            "44/73 [=================>............] - ETA: 5:18 - loss: 39.1875 - custom_mse: 238640.0938\n",
            "reduce_confident_loss =  0.722381651\n",
            "\n",
            "reduce_mean_cls_loss =  1.18864441\n",
            "\n",
            "regression loss =  12.4890852\n",
            "45/73 [=================>............] - ETA: 5:05 - loss: 38.6366 - custom_mse: 246776.8594\n",
            "reduce_confident_loss =  0.704950809\n",
            "\n",
            "reduce_mean_cls_loss =  1.11622834\n",
            "\n",
            "regression loss =  23.4370823\n",
            "46/73 [=================>............] - ETA: 4:55 - loss: 38.3458 - custom_mse: 258802.6562\n",
            "reduce_confident_loss =  0.745949328\n",
            "\n",
            "reduce_mean_cls_loss =  1.02248895\n",
            "\n",
            "regression loss =  39.4146\n",
            "47/73 [==================>...........] - ETA: 4:43 - loss: 38.4062 - custom_mse: 276437.8750\n",
            "reduce_confident_loss =  0.726714313\n",
            "\n",
            "reduce_mean_cls_loss =  0.714813828\n",
            "\n",
            "regression loss =  23.691\n",
            "48/73 [==================>...........] - ETA: 4:31 - loss: 38.1296 - custom_mse: 284256.1562\n",
            "reduce_confident_loss =  0.677913606\n",
            "\n",
            "reduce_mean_cls_loss =  0.376737058\n",
            "\n",
            "regression loss =  20.6494331\n",
            "49/73 [===================>..........] - ETA: 4:19 - loss: 37.7944 - custom_mse: 289884.5000\n",
            "reduce_confident_loss =  0.671536505\n",
            "\n",
            "reduce_mean_cls_loss =  0.327193975\n",
            "\n",
            "regression loss =  12.3111162\n",
            "50/73 [===================>..........] - ETA: 4:08 - loss: 37.3047 - custom_mse: 293091.4375\n",
            "reduce_confident_loss =  0.759787261\n",
            "\n",
            "reduce_mean_cls_loss =  0.416560978\n",
            "\n",
            "regression loss =  5.07100058\n",
            "51/73 [===================>..........] - ETA: 3:55 - loss: 36.6958 - custom_mse: 295416.5000\n",
            "reduce_confident_loss =  0.735928118\n",
            "\n",
            "reduce_mean_cls_loss =  0.360237688\n",
            "\n",
            "regression loss =  8.18234634\n",
            "52/73 [====================>.........] - ETA: 3:43 - loss: 36.1685 - custom_mse: 297165.4062\n",
            "reduce_confident_loss =  0.732517779\n",
            "\n",
            "reduce_mean_cls_loss =  0.61983794\n",
            "\n",
            "regression loss =  42.5731277\n",
            "53/73 [====================>.........] - ETA: 3:33 - loss: 36.3149 - custom_mse: 302098.7500\n",
            "reduce_confident_loss =  0.748884141\n",
            "\n",
            "reduce_mean_cls_loss =  0.86576879\n",
            "\n",
            "regression loss =  122.017731\n",
            "54/73 [=====================>........] - ETA: 3:21 - loss: 37.9319 - custom_mse: 309584.1250\n",
            "reduce_confident_loss =  0.754092216\n",
            "\n",
            "reduce_mean_cls_loss =  0.825745583\n",
            "\n",
            "regression loss =  115.021416\n",
            "55/73 [=====================>........] - ETA: 3:09 - loss: 39.3622 - custom_mse: 316207.9688\n",
            "reduce_confident_loss =  0.751235664\n",
            "\n",
            "reduce_mean_cls_loss =  0.807462394\n",
            "\n",
            "regression loss =  108.481575\n",
            "56/73 [======================>.......] - ETA: 2:57 - loss: 40.6243 - custom_mse: 322102.2188\n",
            "reduce_confident_loss =  0.752717793\n",
            "\n",
            "reduce_mean_cls_loss =  0.706619143\n",
            "\n",
            "regression loss =  37.6815758\n",
            "57/73 [======================>.......] - ETA: 2:46 - loss: 40.5983 - custom_mse: 326335.5000\n",
            "reduce_confident_loss =  0.69835\n",
            "\n",
            "reduce_mean_cls_loss =  0.439963609\n",
            "\n",
            "regression loss =  9.55863762\n",
            "58/73 [======================>.......] - ETA: 2:35 - loss: 40.0828 - custom_mse: 330756.9688\n",
            "reduce_confident_loss =  0.697770715\n",
            "\n",
            "reduce_mean_cls_loss =  0.520735681\n",
            "\n",
            "regression loss =  26.7091846\n",
            "59/73 [=======================>......] - ETA: 2:24 - loss: 39.8767 - custom_mse: 338681.0312\n",
            "reduce_confident_loss =  0.706778228\n",
            "\n",
            "reduce_mean_cls_loss =  0.763612926\n",
            "\n",
            "regression loss =  30.3791065\n",
            "60/73 [=======================>......] - ETA: 2:13 - loss: 39.7429 - custom_mse: 344484.0000\n",
            "reduce_confident_loss =  0.698844969\n",
            "\n",
            "reduce_mean_cls_loss =  0.502160132\n",
            "\n",
            "regression loss =  10.4034081\n",
            "61/73 [========================>.....] - ETA: 2:02 - loss: 39.2817 - custom_mse: 349390.7188\n",
            "reduce_confident_loss =  0.73066318\n",
            "\n",
            "reduce_mean_cls_loss =  0.514707088\n",
            "\n",
            "regression loss =  53.8010635\n",
            "62/73 [========================>.....] - ETA: 1:51 - loss: 39.5359 - custom_mse: 358567.9375\n",
            "reduce_confident_loss =  0.68102175\n",
            "\n",
            "reduce_mean_cls_loss =  0.866694033\n",
            "\n",
            "regression loss =  81.1304474\n",
            "63/73 [========================>.....] - ETA: 1:41 - loss: 40.2207 - custom_mse: 366328.8750\n",
            "reduce_confident_loss =  0.729098499\n",
            "\n",
            "reduce_mean_cls_loss =  0.578817248\n",
            "\n",
            "regression loss =  25.5963345\n",
            "64/73 [=========================>....] - ETA: 1:30 - loss: 40.0127 - custom_mse: 371671.5000\n",
            "reduce_confident_loss =  0.72774595\n",
            "\n",
            "reduce_mean_cls_loss =  0.496424615\n",
            "\n",
            "regression loss =  9.72369862\n",
            "65/73 [=========================>....] - ETA: 1:20 - loss: 39.5655 - custom_mse: 376992.1875\n",
            "reduce_confident_loss =  0.728756428\n",
            "\n",
            "reduce_mean_cls_loss =  0.421021223\n",
            "\n",
            "regression loss =  9.68863392\n",
            "66/73 [==========================>...] - ETA: 1:09 - loss: 39.1302 - custom_mse: 383427.1562\n",
            "reduce_confident_loss =  0.728335798\n",
            "\n",
            "reduce_mean_cls_loss =  0.509456515\n",
            "\n",
            "regression loss =  8.65367889\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 38.6938 - custom_mse: 393945.5312 \n",
            "reduce_confident_loss =  0.71008873\n",
            "\n",
            "reduce_mean_cls_loss =  0.559820116\n",
            "\n",
            "regression loss =  10.8874979\n",
            "68/73 [==========================>...] - ETA: 49s - loss: 38.3036 - custom_mse: 403487.9688\n",
            "reduce_confident_loss =  0.715616822\n",
            "\n",
            "reduce_mean_cls_loss =  0.393908113\n",
            "\n",
            "regression loss =  5.81301308\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 37.8488 - custom_mse: 412300.6250\n",
            "reduce_confident_loss =  0.744246483\n",
            "\n",
            "reduce_mean_cls_loss =  0.572200954\n",
            "\n",
            "regression loss =  11.7665367\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 37.4950 - custom_mse: 422300.7500\n",
            "reduce_confident_loss =  0.714809239\n",
            "\n",
            "reduce_mean_cls_loss =  0.414465\n",
            "\n",
            "regression loss =  6.38334894\n",
            "71/73 [============================>.] - ETA: 19s - loss: 37.0727 - custom_mse: 428107.4062\n",
            "reduce_confident_loss =  0.731399715\n",
            "\n",
            "reduce_mean_cls_loss =  0.383152515\n",
            "\n",
            "regression loss =  13.6997633\n",
            "72/73 [============================>.] - ETA: 9s - loss: 36.7636 - custom_mse: 433644.0000 \n",
            "reduce_confident_loss =  0.710555494\n",
            "\n",
            "reduce_mean_cls_loss =  0.344101429\n",
            "\n",
            "regression loss =  10.7995853\n",
            "73/73 [==============================] - ETA: 0s - loss: 36.5918 - custom_mse: 438616.5938\n",
            "reduce_confident_loss =  0.700041771\n",
            "\n",
            "reduce_mean_cls_loss =  0.318861783\n",
            "\n",
            "regression loss =  6.95009089\n",
            "\n",
            "reduce_confident_loss =  0.698173523\n",
            "\n",
            "reduce_mean_cls_loss =  0.31643644\n",
            "\n",
            "regression loss =  10.7935266\n",
            "\n",
            "reduce_confident_loss =  0.712047577\n",
            "\n",
            "reduce_mean_cls_loss =  0.316041648\n",
            "\n",
            "regression loss =  15.3629818\n",
            "\n",
            "reduce_confident_loss =  0.702083886\n",
            "\n",
            "reduce_mean_cls_loss =  0.317366511\n",
            "\n",
            "regression loss =  15.5022936\n",
            "\n",
            "reduce_confident_loss =  0.694717884\n",
            "\n",
            "reduce_mean_cls_loss =  0.620240748\n",
            "\n",
            "regression loss =  47.9300728\n",
            "\n",
            "reduce_confident_loss =  0.693767548\n",
            "\n",
            "reduce_mean_cls_loss =  0.477625787\n",
            "\n",
            "regression loss =  30.307909\n",
            "\n",
            "reduce_confident_loss =  0.699150383\n",
            "\n",
            "reduce_mean_cls_loss =  0.340082228\n",
            "\n",
            "regression loss =  18.2869034\n",
            "\n",
            "reduce_confident_loss =  0.699337184\n",
            "\n",
            "reduce_mean_cls_loss =  0.317773\n",
            "\n",
            "regression loss =  16.4823685\n",
            "\n",
            "reduce_confident_loss =  0.700282097\n",
            "\n",
            "reduce_mean_cls_loss =  0.315664619\n",
            "\n",
            "regression loss =  14.59233\n",
            "\n",
            "reduce_confident_loss =  0.699578285\n",
            "\n",
            "reduce_mean_cls_loss =  0.317411959\n",
            "\n",
            "regression loss =  13.181324\n",
            "\n",
            "reduce_confident_loss =  0.71948117\n",
            "\n",
            "reduce_mean_cls_loss =  0.316245\n",
            "\n",
            "regression loss =  10.0134068\n",
            "\n",
            "reduce_confident_loss =  0.752964854\n",
            "\n",
            "reduce_mean_cls_loss =  0.319423109\n",
            "\n",
            "regression loss =  12.362421\n",
            "\n",
            "reduce_confident_loss =  0.724127293\n",
            "\n",
            "reduce_mean_cls_loss =  0.327712297\n",
            "\n",
            "regression loss =  16.4218483\n",
            "\n",
            "reduce_confident_loss =  0.698244\n",
            "\n",
            "reduce_mean_cls_loss =  0.736622334\n",
            "\n",
            "regression loss =  71.5551147\n",
            "73/73 [==============================] - 839s 11s/step - loss: 36.5918 - custom_mse: 438616.5938 - val_loss: 20.8709 - val_custom_mse: 84460.4922\n",
            "Epoch 7/15\n",
            "\n",
            "reduce_confident_loss =  0.699124932\n",
            "\n",
            "reduce_mean_cls_loss =  1.08268297\n",
            "\n",
            "regression loss =  95.6928\n",
            " 1/73 [..............................] - ETA: 32:55 - loss: 97.4746 - custom_mse: 13039.1172\n",
            "reduce_confident_loss =  0.694744647\n",
            "\n",
            "reduce_mean_cls_loss =  0.745511949\n",
            "\n",
            "regression loss =  48.1733742\n",
            " 2/73 [..............................] - ETA: 11:54 - loss: 73.5441 - custom_mse: 19283.8750\n",
            "reduce_confident_loss =  0.708672822\n",
            "\n",
            "reduce_mean_cls_loss =  0.692608297\n",
            "\n",
            "regression loss =  28.6680489\n",
            " 3/73 [>.............................] - ETA: 11:11 - loss: 59.0525 - custom_mse: 28514.8418\n",
            "reduce_confident_loss =  0.715209186\n",
            "\n",
            "reduce_mean_cls_loss =  0.61715728\n",
            "\n",
            "regression loss =  27.4226971\n",
            " 4/73 [>.............................] - ETA: 10:54 - loss: 51.4782 - custom_mse: 37511.8789\n",
            "reduce_confident_loss =  0.698634207\n",
            "\n",
            "reduce_mean_cls_loss =  0.626217306\n",
            "\n",
            "regression loss =  25.3156929\n",
            " 5/73 [=>............................] - ETA: 10:38 - loss: 46.5106 - custom_mse: 46650.8125\n",
            "reduce_confident_loss =  0.699573398\n",
            "\n",
            "reduce_mean_cls_loss =  0.639001191\n",
            "\n",
            "regression loss =  24.8508453\n",
            " 6/73 [=>............................] - ETA: 10:25 - loss: 43.1238 - custom_mse: 55209.7461\n",
            "reduce_confident_loss =  0.703876\n",
            "\n",
            "reduce_mean_cls_loss =  0.66274178\n",
            "\n",
            "regression loss =  37.3987694\n",
            " 7/73 [=>............................] - ETA: 10:15 - loss: 42.5011 - custom_mse: 63700.9219\n",
            "reduce_confident_loss =  0.696086824\n",
            "\n",
            "reduce_mean_cls_loss =  0.423962742\n",
            "\n",
            "regression loss =  25.1299896\n",
            " 8/73 [==>...........................] - ETA: 10:23 - loss: 40.4698 - custom_mse: 67014.5781\n",
            "reduce_confident_loss =  0.69549495\n",
            "\n",
            "reduce_mean_cls_loss =  0.431690782\n",
            "\n",
            "regression loss =  10.9767122\n",
            " 9/73 [==>...........................] - ETA: 10:09 - loss: 37.3180 - custom_mse: 71000.6562\n",
            "reduce_confident_loss =  0.703324\n",
            "\n",
            "reduce_mean_cls_loss =  0.387699872\n",
            "\n",
            "regression loss =  7.85608387\n",
            "10/73 [===>..........................] - ETA: 9:55 - loss: 34.4809 - custom_mse: 75641.8594 \n",
            "reduce_confident_loss =  0.720731795\n",
            "\n",
            "reduce_mean_cls_loss =  0.420192957\n",
            "\n",
            "regression loss =  24.6740837\n",
            "11/73 [===>..........................] - ETA: 9:45 - loss: 33.6931 - custom_mse: 81026.7656\n",
            "reduce_confident_loss =  0.704310119\n",
            "\n",
            "reduce_mean_cls_loss =  0.527028918\n",
            "\n",
            "regression loss =  28.2594166\n",
            "12/73 [===>..........................] - ETA: 9:32 - loss: 33.3429 - custom_mse: 92470.3047\n",
            "reduce_confident_loss =  0.706017\n",
            "\n",
            "reduce_mean_cls_loss =  0.344409406\n",
            "\n",
            "regression loss =  7.45669842\n",
            "13/73 [====>.........................] - ETA: 9:20 - loss: 31.4325 - custom_mse: 101153.3594\n",
            "reduce_confident_loss =  0.698942602\n",
            "\n",
            "reduce_mean_cls_loss =  0.460440159\n",
            "\n",
            "regression loss =  8.91189\n",
            "14/73 [====>.........................] - ETA: 9:08 - loss: 29.9067 - custom_mse: 108126.5234\n",
            "reduce_confident_loss =  0.694754958\n",
            "\n",
            "reduce_mean_cls_loss =  0.361380637\n",
            "\n",
            "regression loss =  8.63268375\n",
            "15/73 [=====>........................] - ETA: 8:57 - loss: 28.5588 - custom_mse: 116068.0938\n",
            "reduce_confident_loss =  0.695463061\n",
            "\n",
            "reduce_mean_cls_loss =  0.361098081\n",
            "\n",
            "regression loss =  6.5920682\n",
            "16/73 [=====>........................] - ETA: 8:46 - loss: 27.2519 - custom_mse: 123859.2031\n",
            "reduce_confident_loss =  0.699175179\n",
            "\n",
            "reduce_mean_cls_loss =  0.409760088\n",
            "\n",
            "regression loss =  19.6686096\n",
            "17/73 [=====>........................] - ETA: 8:35 - loss: 26.8711 - custom_mse: 134767.9844\n",
            "reduce_confident_loss =  0.703168571\n",
            "\n",
            "reduce_mean_cls_loss =  0.660964787\n",
            "\n",
            "regression loss =  42.255558\n",
            "18/73 [======>.......................] - ETA: 8:25 - loss: 27.8015 - custom_mse: 142307.4531\n",
            "reduce_confident_loss =  0.696219\n",
            "\n",
            "reduce_mean_cls_loss =  0.659181535\n",
            "\n",
            "regression loss =  44.3527069\n",
            "19/73 [======>.......................] - ETA: 10:27 - loss: 28.7440 - custom_mse: 148724.1875\n",
            "reduce_confident_loss =  0.701410472\n",
            "\n",
            "reduce_mean_cls_loss =  0.703171551\n",
            "\n",
            "regression loss =  34.4194717\n",
            "20/73 [=======>......................] - ETA: 10:09 - loss: 29.0980 - custom_mse: 156221.7656\n",
            "reduce_confident_loss =  0.685225129\n",
            "\n",
            "reduce_mean_cls_loss =  0.62948668\n",
            "\n",
            "regression loss =  29.6285782\n",
            "21/73 [=======>......................] - ETA: 9:52 - loss: 29.1859 - custom_mse: 162166.4531 \n",
            "reduce_confident_loss =  0.678829491\n",
            "\n",
            "reduce_mean_cls_loss =  0.60622853\n",
            "\n",
            "regression loss =  33.7566566\n",
            "22/73 [========>.....................] - ETA: 9:35 - loss: 29.4520 - custom_mse: 169207.2031\n",
            "reduce_confident_loss =  0.70831871\n",
            "\n",
            "reduce_mean_cls_loss =  0.458670139\n",
            "\n",
            "regression loss =  25.437809\n",
            "23/73 [========>.....................] - ETA: 9:21 - loss: 29.3283 - custom_mse: 172414.0625\n",
            "reduce_confident_loss =  0.710094631\n",
            "\n",
            "reduce_mean_cls_loss =  0.318424255\n",
            "\n",
            "regression loss =  20.1558\n",
            "24/73 [========>.....................] - ETA: 9:09 - loss: 28.9889 - custom_mse: 176798.7656\n",
            "reduce_confident_loss =  0.714718878\n",
            "\n",
            "reduce_mean_cls_loss =  0.317838937\n",
            "\n",
            "regression loss =  18.9106388\n",
            "25/73 [=========>....................] - ETA: 9:11 - loss: 28.6271 - custom_mse: 181524.0938\n",
            "reduce_confident_loss =  0.70639658\n",
            "\n",
            "reduce_mean_cls_loss =  0.458502084\n",
            "\n",
            "regression loss =  26.5703468\n",
            "26/73 [=========>....................] - ETA: 8:53 - loss: 28.5928 - custom_mse: 193417.9688\n",
            "reduce_confident_loss =  0.712171257\n",
            "\n",
            "reduce_mean_cls_loss =  0.359277725\n",
            "\n",
            "regression loss =  19.0725956\n",
            "27/73 [==========>...................] - ETA: 8:37 - loss: 28.2799 - custom_mse: 202170.5938\n",
            "reduce_confident_loss =  0.705908775\n",
            "\n",
            "reduce_mean_cls_loss =  0.318722248\n",
            "\n",
            "regression loss =  7.43323374\n",
            "28/73 [==========>...................] - ETA: 8:21 - loss: 27.5719 - custom_mse: 204454.7656\n",
            "reduce_confident_loss =  0.725031555\n",
            "\n",
            "reduce_mean_cls_loss =  0.319738537\n",
            "\n",
            "regression loss =  6.01454592\n",
            "29/73 [==========>...................] - ETA: 8:06 - loss: 26.8646 - custom_mse: 205844.9844\n",
            "reduce_confident_loss =  0.715892851\n",
            "\n",
            "reduce_mean_cls_loss =  0.344637185\n",
            "\n",
            "regression loss =  10.4000378\n",
            "30/73 [===========>..................] - ETA: 7:50 - loss: 26.3511 - custom_mse: 207505.2500\n",
            "reduce_confident_loss =  0.676529\n",
            "\n",
            "reduce_mean_cls_loss =  0.423116416\n",
            "\n",
            "regression loss =  15.5955839\n",
            "31/73 [===========>..................] - ETA: 7:36 - loss: 26.0397 - custom_mse: 208938.5000\n",
            "reduce_confident_loss =  0.693866849\n",
            "\n",
            "reduce_mean_cls_loss =  0.414280087\n",
            "\n",
            "regression loss =  23.1344891\n",
            "32/73 [============>.................] - ETA: 7:23 - loss: 25.9835 - custom_mse: 211656.9844\n",
            "reduce_confident_loss =  0.709621549\n",
            "\n",
            "reduce_mean_cls_loss =  0.314697\n",
            "\n",
            "regression loss =  14.3096199\n",
            "33/73 [============>.................] - ETA: 7:09 - loss: 25.6608 - custom_mse: 217235.3906\n",
            "reduce_confident_loss =  0.70753479\n",
            "\n",
            "reduce_mean_cls_loss =  0.324660242\n",
            "\n",
            "regression loss =  18.4424438\n",
            "34/73 [============>.................] - ETA: 6:55 - loss: 25.4788 - custom_mse: 226229.6562\n",
            "reduce_confident_loss =  0.732868075\n",
            "\n",
            "reduce_mean_cls_loss =  0.406173795\n",
            "\n",
            "regression loss =  19.1636086\n",
            "35/73 [=============>................] - ETA: 6:42 - loss: 25.3310 - custom_mse: 235706.9375\n",
            "reduce_confident_loss =  0.710128367\n",
            "\n",
            "reduce_mean_cls_loss =  0.33148554\n",
            "\n",
            "regression loss =  40.8359375\n",
            "36/73 [=============>................] - ETA: 6:35 - loss: 25.7906 - custom_mse: 241513.3594\n",
            "reduce_confident_loss =  0.706708729\n",
            "\n",
            "reduce_mean_cls_loss =  0.997566879\n",
            "\n",
            "regression loss =  61.0886955\n",
            "37/73 [==============>...............] - ETA: 6:22 - loss: 26.7906 - custom_mse: 249791.9844\n",
            "reduce_confident_loss =  0.735786557\n",
            "\n",
            "reduce_mean_cls_loss =  0.845791519\n",
            "\n",
            "regression loss =  46.8726845\n",
            "38/73 [==============>...............] - ETA: 6:14 - loss: 27.3607 - custom_mse: 256766.3281\n",
            "reduce_confident_loss =  0.715443075\n",
            "\n",
            "reduce_mean_cls_loss =  1.26429021\n",
            "\n",
            "regression loss =  81.3022156\n",
            "39/73 [===============>..............] - ETA: 6:02 - loss: 28.7946 - custom_mse: 258391.4844\n",
            "reduce_confident_loss =  0.699186325\n",
            "\n",
            "reduce_mean_cls_loss =  1.24455178\n",
            "\n",
            "regression loss =  101.212646\n",
            "40/73 [===============>..............] - ETA: 5:49 - loss: 30.6537 - custom_mse: 261051.5000\n",
            "reduce_confident_loss =  0.7416026\n",
            "\n",
            "reduce_mean_cls_loss =  1.09079337\n",
            "\n",
            "regression loss =  114.955345\n",
            "41/73 [===============>..............] - ETA: 5:40 - loss: 32.7545 - custom_mse: 307566.1562\n",
            "reduce_confident_loss =  0.704668343\n",
            "\n",
            "reduce_mean_cls_loss =  1.26792991\n",
            "\n",
            "regression loss =  141.113342\n",
            "42/73 [================>.............] - ETA: 5:28 - loss: 35.3814 - custom_mse: 353298.0000\n",
            "reduce_confident_loss =  0.712085426\n",
            "\n",
            "reduce_mean_cls_loss =  1.15003717\n",
            "\n",
            "regression loss =  116.352768\n",
            "43/73 [================>.............] - ETA: 5:32 - loss: 37.3078 - custom_mse: 391250.1875\n",
            "reduce_confident_loss =  0.754069448\n",
            "\n",
            "reduce_mean_cls_loss =  1.09544408\n",
            "\n",
            "regression loss =  13.6591787\n",
            "44/73 [=================>............] - ETA: 5:19 - loss: 36.8124 - custom_mse: 401011.2812\n",
            "reduce_confident_loss =  0.743528664\n",
            "\n",
            "reduce_mean_cls_loss =  1.13032436\n",
            "\n",
            "regression loss =  14.0603943\n",
            "45/73 [=================>............] - ETA: 5:06 - loss: 36.3484 - custom_mse: 412104.6875\n",
            "reduce_confident_loss =  0.711946666\n",
            "\n",
            "reduce_mean_cls_loss =  1.19238341\n",
            "\n",
            "regression loss =  24.070673\n",
            "46/73 [=================>............] - ETA: 4:56 - loss: 36.1229 - custom_mse: 426625.8750\n",
            "reduce_confident_loss =  0.729043901\n",
            "\n",
            "reduce_mean_cls_loss =  1.16557205\n",
            "\n",
            "regression loss =  20.2885857\n",
            "47/73 [==================>...........] - ETA: 4:44 - loss: 35.8263 - custom_mse: 449609.0000\n",
            "reduce_confident_loss =  0.721919119\n",
            "\n",
            "reduce_mean_cls_loss =  0.723014832\n",
            "\n",
            "regression loss =  18.8032379\n",
            "48/73 [==================>...........] - ETA: 4:33 - loss: 35.5018 - custom_mse: 459904.0938\n",
            "reduce_confident_loss =  0.697356641\n",
            "\n",
            "reduce_mean_cls_loss =  0.342474192\n",
            "\n",
            "regression loss =  15.4528656\n",
            "49/73 [===================>..........] - ETA: 4:20 - loss: 35.1138 - custom_mse: 465552.6562\n",
            "reduce_confident_loss =  0.692590714\n",
            "\n",
            "reduce_mean_cls_loss =  0.32994625\n",
            "\n",
            "regression loss =  9.14184093\n",
            "50/73 [===================>..........] - ETA: 4:09 - loss: 34.6148 - custom_mse: 468870.3750\n",
            "reduce_confident_loss =  0.739225447\n",
            "\n",
            "reduce_mean_cls_loss =  0.332066208\n",
            "\n",
            "regression loss =  6.54439068\n",
            "51/73 [===================>..........] - ETA: 3:56 - loss: 34.0854 - custom_mse: 471184.8438\n",
            "reduce_confident_loss =  0.75909\n",
            "\n",
            "reduce_mean_cls_loss =  0.346833229\n",
            "\n",
            "regression loss =  4.91906404\n",
            "52/73 [====================>.........] - ETA: 3:44 - loss: 33.5458 - custom_mse: 472159.9375\n",
            "reduce_confident_loss =  0.749908149\n",
            "\n",
            "reduce_mean_cls_loss =  0.366504729\n",
            "\n",
            "regression loss =  31.1368847\n",
            "53/73 [====================>.........] - ETA: 3:33 - loss: 33.5214 - custom_mse: 480534.3750\n",
            "reduce_confident_loss =  0.750639558\n",
            "\n",
            "reduce_mean_cls_loss =  1.02134502\n",
            "\n",
            "regression loss =  87.220726\n",
            "54/73 [=====================>........] - ETA: 3:21 - loss: 34.5487 - custom_mse: 484363.0938\n",
            "reduce_confident_loss =  0.743104279\n",
            "\n",
            "reduce_mean_cls_loss =  0.915605724\n",
            "\n",
            "regression loss =  81.9854431\n",
            "55/73 [=====================>........] - ETA: 3:09 - loss: 35.4413 - custom_mse: 487734.0938\n",
            "reduce_confident_loss =  0.741681635\n",
            "\n",
            "reduce_mean_cls_loss =  0.910130799\n",
            "\n",
            "regression loss =  81.0539093\n",
            "56/73 [======================>.......] - ETA: 2:58 - loss: 36.2853 - custom_mse: 491040.4688\n",
            "reduce_confident_loss =  0.75077641\n",
            "\n",
            "reduce_mean_cls_loss =  0.843123376\n",
            "\n",
            "regression loss =  31.5813293\n",
            "57/73 [======================>.......] - ETA: 2:47 - loss: 36.2308 - custom_mse: 493890.7812\n",
            "reduce_confident_loss =  0.697727561\n",
            "\n",
            "reduce_mean_cls_loss =  0.318676203\n",
            "\n",
            "regression loss =  10.0492659\n",
            "58/73 [======================>.......] - ETA: 2:35 - loss: 35.7969 - custom_mse: 497865.3750\n",
            "reduce_confident_loss =  0.698525131\n",
            "\n",
            "reduce_mean_cls_loss =  0.439804554\n",
            "\n",
            "regression loss =  23.8069572\n",
            "59/73 [=======================>......] - ETA: 2:24 - loss: 35.6130 - custom_mse: 507408.6875\n",
            "reduce_confident_loss =  0.707928717\n",
            "\n",
            "reduce_mean_cls_loss =  0.651667237\n",
            "\n",
            "regression loss =  29.9240932\n",
            "60/73 [=======================>......] - ETA: 2:13 - loss: 35.5408 - custom_mse: 514093.1562\n",
            "reduce_confident_loss =  0.709991\n",
            "\n",
            "reduce_mean_cls_loss =  0.349973619\n",
            "\n",
            "regression loss =  9.67102146\n",
            "61/73 [========================>.....] - ETA: 2:02 - loss: 35.1341 - custom_mse: 520194.3438\n",
            "reduce_confident_loss =  0.710081041\n",
            "\n",
            "reduce_mean_cls_loss =  0.391176462\n",
            "\n",
            "regression loss =  49.6699333\n",
            "62/73 [========================>.....] - ETA: 1:51 - loss: 35.3863 - custom_mse: 531455.7500\n",
            "reduce_confident_loss =  0.685834527\n",
            "\n",
            "reduce_mean_cls_loss =  0.802127421\n",
            "\n",
            "regression loss =  79.2968597\n",
            "63/73 [========================>.....] - ETA: 1:41 - loss: 36.1069 - custom_mse: 540108.0000\n",
            "reduce_confident_loss =  0.708228886\n",
            "\n",
            "reduce_mean_cls_loss =  0.572204471\n",
            "\n",
            "regression loss =  21.6189251\n",
            "64/73 [=========================>....] - ETA: 1:30 - loss: 35.9005 - custom_mse: 546485.7500\n",
            "reduce_confident_loss =  0.736289084\n",
            "\n",
            "reduce_mean_cls_loss =  0.363391876\n",
            "\n",
            "regression loss =  6.04448509\n",
            "65/73 [=========================>....] - ETA: 1:20 - loss: 35.4581 - custom_mse: 552752.4375\n",
            "reduce_confident_loss =  0.704366\n",
            "\n",
            "reduce_mean_cls_loss =  0.324322522\n",
            "\n",
            "regression loss =  4.3031\n",
            "66/73 [==========================>...] - ETA: 1:09 - loss: 35.0017 - custom_mse: 559776.6875\n",
            "reduce_confident_loss =  0.713747561\n",
            "\n",
            "reduce_mean_cls_loss =  0.423072964\n",
            "\n",
            "regression loss =  4.12606335\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 34.5578 - custom_mse: 570594.4375 \n",
            "reduce_confident_loss =  0.715657294\n",
            "\n",
            "reduce_mean_cls_loss =  0.364656925\n",
            "\n",
            "regression loss =  5.64276457\n",
            "68/73 [==========================>...] - ETA: 49s - loss: 34.1485 - custom_mse: 579435.8125\n",
            "reduce_confident_loss =  0.703264117\n",
            "\n",
            "reduce_mean_cls_loss =  0.381603777\n",
            "\n",
            "regression loss =  6.388381\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 33.7619 - custom_mse: 587168.9375\n",
            "reduce_confident_loss =  0.745144486\n",
            "\n",
            "reduce_mean_cls_loss =  0.387387425\n",
            "\n",
            "regression loss =  5.71187639\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 33.3773 - custom_mse: 594339.5000\n",
            "reduce_confident_loss =  0.707530439\n",
            "\n",
            "reduce_mean_cls_loss =  0.355996519\n",
            "\n",
            "regression loss =  5.53866196\n",
            "71/73 [============================>.] - ETA: 19s - loss: 33.0002 - custom_mse: 596462.9375\n",
            "reduce_confident_loss =  0.698822856\n",
            "\n",
            "reduce_mean_cls_loss =  0.31756106\n",
            "\n",
            "regression loss =  10.1155281\n",
            "72/73 [============================>.] - ETA: 9s - loss: 32.6965 - custom_mse: 598201.4375 \n",
            "reduce_confident_loss =  0.711213648\n",
            "\n",
            "reduce_mean_cls_loss =  0.320852906\n",
            "\n",
            "regression loss =  12.5792627\n",
            "73/73 [==============================] - ETA: 0s - loss: 32.5649 - custom_mse: 599780.1250\n",
            "reduce_confident_loss =  0.70676744\n",
            "\n",
            "reduce_mean_cls_loss =  0.321394295\n",
            "\n",
            "regression loss =  6.55202675\n",
            "\n",
            "reduce_confident_loss =  0.70755291\n",
            "\n",
            "reduce_mean_cls_loss =  0.318302542\n",
            "\n",
            "regression loss =  8.69547081\n",
            "\n",
            "reduce_confident_loss =  0.736808717\n",
            "\n",
            "reduce_mean_cls_loss =  0.318164259\n",
            "\n",
            "regression loss =  9.22863674\n",
            "\n",
            "reduce_confident_loss =  0.709603131\n",
            "\n",
            "reduce_mean_cls_loss =  0.324291497\n",
            "\n",
            "regression loss =  13.4624882\n",
            "\n",
            "reduce_confident_loss =  0.680978417\n",
            "\n",
            "reduce_mean_cls_loss =  0.539513588\n",
            "\n",
            "regression loss =  45.8655472\n",
            "\n",
            "reduce_confident_loss =  0.685830772\n",
            "\n",
            "reduce_mean_cls_loss =  0.473893553\n",
            "\n",
            "regression loss =  31.8960743\n",
            "\n",
            "reduce_confident_loss =  0.700520217\n",
            "\n",
            "reduce_mean_cls_loss =  0.337257236\n",
            "\n",
            "regression loss =  14.3977814\n",
            "\n",
            "reduce_confident_loss =  0.698497117\n",
            "\n",
            "reduce_mean_cls_loss =  0.317638248\n",
            "\n",
            "regression loss =  13.5625381\n",
            "\n",
            "reduce_confident_loss =  0.698190093\n",
            "\n",
            "reduce_mean_cls_loss =  0.31524995\n",
            "\n",
            "regression loss =  11.1194544\n",
            "\n",
            "reduce_confident_loss =  0.715627074\n",
            "\n",
            "reduce_mean_cls_loss =  0.31599617\n",
            "\n",
            "regression loss =  8.88303566\n",
            "\n",
            "reduce_confident_loss =  0.745466232\n",
            "\n",
            "reduce_mean_cls_loss =  0.317621738\n",
            "\n",
            "regression loss =  7.75882053\n",
            "\n",
            "reduce_confident_loss =  0.780948281\n",
            "\n",
            "reduce_mean_cls_loss =  0.323027194\n",
            "\n",
            "regression loss =  9.69583416\n",
            "\n",
            "reduce_confident_loss =  0.730792\n",
            "\n",
            "reduce_mean_cls_loss =  0.343300253\n",
            "\n",
            "regression loss =  10.2961283\n",
            "\n",
            "reduce_confident_loss =  0.704133749\n",
            "\n",
            "reduce_mean_cls_loss =  0.718699\n",
            "\n",
            "regression loss =  68.0172348\n",
            "73/73 [==============================] - 843s 11s/step - loss: 32.5649 - custom_mse: 599780.1250 - val_loss: 18.0156 - val_custom_mse: 78979.2656\n",
            "Epoch 8/15\n",
            "\n",
            "reduce_confident_loss =  0.695104659\n",
            "\n",
            "reduce_mean_cls_loss =  1.00974238\n",
            "\n",
            "regression loss =  97.1175385\n",
            " 1/73 [..............................] - ETA: 32:13 - loss: 98.8224 - custom_mse: 13063.0215\n",
            "reduce_confident_loss =  0.697015464\n",
            "\n",
            "reduce_mean_cls_loss =  0.668008924\n",
            "\n",
            "regression loss =  48.8238335\n",
            " 2/73 [..............................] - ETA: 12:32 - loss: 74.5056 - custom_mse: 17736.3477\n",
            "reduce_confident_loss =  0.693546236\n",
            "\n",
            "reduce_mean_cls_loss =  0.650135636\n",
            "\n",
            "regression loss =  31.194479\n",
            " 3/73 [>.............................] - ETA: 11:31 - loss: 60.5165 - custom_mse: 26480.2031\n",
            "reduce_confident_loss =  0.693097413\n",
            "\n",
            "reduce_mean_cls_loss =  0.605805576\n",
            "\n",
            "regression loss =  27.8973274\n",
            " 4/73 [>.............................] - ETA: 11:12 - loss: 52.6864 - custom_mse: 34369.8086\n",
            "reduce_confident_loss =  0.695103526\n",
            "\n",
            "reduce_mean_cls_loss =  0.582616925\n",
            "\n",
            "regression loss =  26.3431416\n",
            " 5/73 [=>............................] - ETA: 10:52 - loss: 47.6733 - custom_mse: 42046.1680\n",
            "reduce_confident_loss =  0.69381088\n",
            "\n",
            "reduce_mean_cls_loss =  0.593838096\n",
            "\n",
            "regression loss =  26.243763\n",
            " 6/73 [=>............................] - ETA: 10:37 - loss: 44.3163 - custom_mse: 49970.2695\n",
            "reduce_confident_loss =  0.699947476\n",
            "\n",
            "reduce_mean_cls_loss =  0.559229672\n",
            "\n",
            "regression loss =  37.170517\n",
            " 7/73 [=>............................] - ETA: 10:25 - loss: 43.4754 - custom_mse: 58295.6641\n",
            "reduce_confident_loss =  0.70305568\n",
            "\n",
            "reduce_mean_cls_loss =  0.449383318\n",
            "\n",
            "regression loss =  24.1493\n",
            " 8/73 [==>...........................] - ETA: 10:30 - loss: 41.2037 - custom_mse: 60717.4219\n",
            "reduce_confident_loss =  0.69613713\n",
            "\n",
            "reduce_mean_cls_loss =  0.419593483\n",
            "\n",
            "regression loss =  14.7192678\n",
            " 9/73 [==>...........................] - ETA: 10:18 - loss: 38.3849 - custom_mse: 64580.7422\n",
            "reduce_confident_loss =  0.703937411\n",
            "\n",
            "reduce_mean_cls_loss =  0.42552039\n",
            "\n",
            "regression loss =  7.07816315\n",
            "10/73 [===>..........................] - ETA: 10:02 - loss: 35.3672 - custom_mse: 68795.3672\n",
            "reduce_confident_loss =  0.709716141\n",
            "\n",
            "reduce_mean_cls_loss =  0.376013\n",
            "\n",
            "regression loss =  22.5980415\n",
            "11/73 [===>..........................] - ETA: 9:48 - loss: 34.3051 - custom_mse: 73983.3750 \n",
            "reduce_confident_loss =  0.702209413\n",
            "\n",
            "reduce_mean_cls_loss =  0.425510883\n",
            "\n",
            "regression loss =  25.35182\n",
            "12/73 [===>..........................] - ETA: 9:34 - loss: 33.6529 - custom_mse: 85057.2734\n",
            "reduce_confident_loss =  0.69639641\n",
            "\n",
            "reduce_mean_cls_loss =  0.435272962\n",
            "\n",
            "regression loss =  8.20896816\n",
            "13/73 [====>.........................] - ETA: 9:24 - loss: 31.7828 - custom_mse: 93130.4062\n",
            "reduce_confident_loss =  0.694926679\n",
            "\n",
            "reduce_mean_cls_loss =  0.427494109\n",
            "\n",
            "regression loss =  10.922718\n",
            "14/73 [====>.........................] - ETA: 9:13 - loss: 30.3729 - custom_mse: 100267.6016\n",
            "reduce_confident_loss =  0.695603848\n",
            "\n",
            "reduce_mean_cls_loss =  0.377613544\n",
            "\n",
            "regression loss =  7.72543049\n",
            "15/73 [=====>........................] - ETA: 9:01 - loss: 28.9346 - custom_mse: 107851.5000\n",
            "reduce_confident_loss =  0.694431\n",
            "\n",
            "reduce_mean_cls_loss =  0.420797884\n",
            "\n",
            "regression loss =  8.00888157\n",
            "16/73 [=====>........................] - ETA: 8:50 - loss: 27.6965 - custom_mse: 115866.8438\n",
            "reduce_confident_loss =  0.694844246\n",
            "\n",
            "reduce_mean_cls_loss =  0.41852966\n",
            "\n",
            "regression loss =  19.6503582\n",
            "17/73 [=====>........................] - ETA: 8:39 - loss: 27.2887 - custom_mse: 126615.2734\n",
            "reduce_confident_loss =  0.692269444\n",
            "\n",
            "reduce_mean_cls_loss =  0.634256065\n",
            "\n",
            "regression loss =  40.5611382\n",
            "18/73 [======>.......................] - ETA: 8:29 - loss: 28.0997 - custom_mse: 133664.6719\n",
            "reduce_confident_loss =  0.693766594\n",
            "\n",
            "reduce_mean_cls_loss =  0.652590096\n",
            "\n",
            "regression loss =  44.2632561\n",
            "19/73 [======>.......................] - ETA: 10:28 - loss: 29.0213 - custom_mse: 139384.9375\n",
            "reduce_confident_loss =  0.676698744\n",
            "\n",
            "reduce_mean_cls_loss =  0.571482778\n",
            "\n",
            "regression loss =  28.4530182\n",
            "20/73 [=======>......................] - ETA: 10:09 - loss: 29.0553 - custom_mse: 148691.7812\n",
            "reduce_confident_loss =  0.6981951\n",
            "\n",
            "reduce_mean_cls_loss =  0.616834641\n",
            "\n",
            "regression loss =  29.0989113\n",
            "21/73 [=======>......................] - ETA: 9:51 - loss: 29.1200 - custom_mse: 154730.3125 \n",
            "reduce_confident_loss =  0.683281362\n",
            "\n",
            "reduce_mean_cls_loss =  0.515555739\n",
            "\n",
            "regression loss =  29.8904629\n",
            "22/73 [========>.....................] - ETA: 9:34 - loss: 29.2095 - custom_mse: 162715.6875\n",
            "reduce_confident_loss =  0.704649448\n",
            "\n",
            "reduce_mean_cls_loss =  0.408470094\n",
            "\n",
            "regression loss =  21.9824162\n",
            "23/73 [========>.....................] - ETA: 9:22 - loss: 28.9437 - custom_mse: 165608.0781\n",
            "reduce_confident_loss =  0.693752885\n",
            "\n",
            "reduce_mean_cls_loss =  0.339095294\n",
            "\n",
            "regression loss =  16.836998\n",
            "24/73 [========>.....................] - ETA: 9:10 - loss: 28.4823 - custom_mse: 168878.1875\n",
            "reduce_confident_loss =  0.705145895\n",
            "\n",
            "reduce_mean_cls_loss =  0.314758331\n",
            "\n",
            "regression loss =  15.5963478\n",
            "25/73 [=========>....................] - ETA: 9:12 - loss: 28.0076 - custom_mse: 172486.5469\n",
            "reduce_confident_loss =  0.706791937\n",
            "\n",
            "reduce_mean_cls_loss =  0.459133208\n",
            "\n",
            "regression loss =  26.1311302\n",
            "26/73 [=========>....................] - ETA: 8:54 - loss: 27.9803 - custom_mse: 183098.4844\n",
            "reduce_confident_loss =  0.69607228\n",
            "\n",
            "reduce_mean_cls_loss =  0.373592705\n",
            "\n",
            "regression loss =  18.933176\n",
            "27/73 [==========>...................] - ETA: 8:37 - loss: 27.6848 - custom_mse: 190259.7031\n",
            "reduce_confident_loss =  0.700802505\n",
            "\n",
            "reduce_mean_cls_loss =  0.318713158\n",
            "\n",
            "regression loss =  6.37048197\n",
            "28/73 [==========>...................] - ETA: 8:21 - loss: 26.9600 - custom_mse: 191582.6406\n",
            "reduce_confident_loss =  0.786719322\n",
            "\n",
            "reduce_mean_cls_loss =  0.322966903\n",
            "\n",
            "regression loss =  5.99309826\n",
            "29/73 [==========>...................] - ETA: 8:06 - loss: 26.2753 - custom_mse: 192005.2656\n",
            "reduce_confident_loss =  0.770773947\n",
            "\n",
            "reduce_mean_cls_loss =  0.331005722\n",
            "\n",
            "regression loss =  8.04070568\n",
            "30/73 [===========>..................] - ETA: 7:51 - loss: 25.7042 - custom_mse: 192642.2344\n",
            "reduce_confident_loss =  0.626519322\n",
            "\n",
            "reduce_mean_cls_loss =  0.415366858\n",
            "\n",
            "regression loss =  10.0652075\n",
            "31/73 [===========>..................] - ETA: 7:37 - loss: 25.2333 - custom_mse: 193051.8438\n",
            "reduce_confident_loss =  0.660502791\n",
            "\n",
            "reduce_mean_cls_loss =  0.417485714\n",
            "\n",
            "regression loss =  16.3613415\n",
            "32/73 [============>.................] - ETA: 7:23 - loss: 24.9898 - custom_mse: 194620.3281\n",
            "reduce_confident_loss =  0.695312321\n",
            "\n",
            "reduce_mean_cls_loss =  0.314988822\n",
            "\n",
            "regression loss =  12.1988401\n",
            "33/73 [============>.................] - ETA: 7:09 - loss: 24.6328 - custom_mse: 199146.1875\n",
            "reduce_confident_loss =  0.694866061\n",
            "\n",
            "reduce_mean_cls_loss =  0.358267546\n",
            "\n",
            "regression loss =  14.3845854\n",
            "34/73 [============>.................] - ETA: 6:56 - loss: 24.3623 - custom_mse: 207171.6719\n",
            "reduce_confident_loss =  0.698930323\n",
            "\n",
            "reduce_mean_cls_loss =  0.387546957\n",
            "\n",
            "regression loss =  20.3672104\n",
            "35/73 [=============>................] - ETA: 6:42 - loss: 24.2792 - custom_mse: 216625.5156\n",
            "reduce_confident_loss =  0.704336584\n",
            "\n",
            "reduce_mean_cls_loss =  0.383839667\n",
            "\n",
            "regression loss =  36.0216751\n",
            "36/73 [=============>................] - ETA: 6:36 - loss: 24.6356 - custom_mse: 221876.0312\n",
            "reduce_confident_loss =  0.717530191\n",
            "\n",
            "reduce_mean_cls_loss =  1.03091526\n",
            "\n",
            "regression loss =  58.4492569\n",
            "37/73 [==============>...............] - ETA: 6:23 - loss: 25.5968 - custom_mse: 230846.7969\n",
            "reduce_confident_loss =  0.710264742\n",
            "\n",
            "reduce_mean_cls_loss =  0.800665379\n",
            "\n",
            "regression loss =  41.4358521\n",
            "38/73 [==============>...............] - ETA: 6:15 - loss: 26.0534 - custom_mse: 237990.3594\n",
            "reduce_confident_loss =  0.70334655\n",
            "\n",
            "reduce_mean_cls_loss =  1.246894\n",
            "\n",
            "regression loss =  80.8090744\n",
            "39/73 [===============>..............] - ETA: 6:02 - loss: 27.5074 - custom_mse: 239564.5469\n",
            "reduce_confident_loss =  0.670670331\n",
            "\n",
            "reduce_mean_cls_loss =  1.12436676\n",
            "\n",
            "regression loss =  101.027725\n",
            "40/73 [===============>..............] - ETA: 5:50 - loss: 29.3902 - custom_mse: 242517.5625\n",
            "reduce_confident_loss =  0.728274047\n",
            "\n",
            "reduce_mean_cls_loss =  1.09055054\n",
            "\n",
            "regression loss =  102.833359\n",
            "41/73 [===============>..............] - ETA: 5:41 - loss: 31.2259 - custom_mse: 292862.8438\n",
            "reduce_confident_loss =  0.707831442\n",
            "\n",
            "reduce_mean_cls_loss =  1.18427396\n",
            "\n",
            "regression loss =  135.593658\n",
            "42/73 [================>.............] - ETA: 5:29 - loss: 33.7559 - custom_mse: 342549.1562\n",
            "reduce_confident_loss =  0.706091702\n",
            "\n",
            "reduce_mean_cls_loss =  1.0406127\n",
            "\n",
            "regression loss =  101.62188\n",
            "43/73 [================>.............] - ETA: 5:33 - loss: 35.3748 - custom_mse: 383349.2812\n",
            "reduce_confident_loss =  0.751300037\n",
            "\n",
            "reduce_mean_cls_loss =  1.14202988\n",
            "\n",
            "regression loss =  12.5519247\n",
            "44/73 [=================>............] - ETA: 5:20 - loss: 34.8991 - custom_mse: 393609.3750\n",
            "reduce_confident_loss =  0.741912186\n",
            "\n",
            "reduce_mean_cls_loss =  1.29117644\n",
            "\n",
            "regression loss =  14.2784138\n",
            "45/73 [=================>............] - ETA: 5:07 - loss: 34.4861 - custom_mse: 405193.3125\n",
            "reduce_confident_loss =  0.724851787\n",
            "\n",
            "reduce_mean_cls_loss =  1.05856597\n",
            "\n",
            "regression loss =  21.9350338\n",
            "46/73 [=================>............] - ETA: 4:57 - loss: 34.2520 - custom_mse: 421390.2812\n",
            "reduce_confident_loss =  0.746436656\n",
            "\n",
            "reduce_mean_cls_loss =  0.905627728\n",
            "\n",
            "regression loss =  18.110569\n",
            "47/73 [==================>...........] - ETA: 4:45 - loss: 33.9437 - custom_mse: 446404.5625\n",
            "reduce_confident_loss =  0.723087251\n",
            "\n",
            "reduce_mean_cls_loss =  0.724597037\n",
            "\n",
            "regression loss =  18.3756924\n",
            "48/73 [==================>...........] - ETA: 4:33 - loss: 33.6495 - custom_mse: 456574.8125\n",
            "reduce_confident_loss =  0.699831307\n",
            "\n",
            "reduce_mean_cls_loss =  0.365528911\n",
            "\n",
            "regression loss =  14.6275911\n",
            "49/73 [===================>..........] - ETA: 4:21 - loss: 33.2831 - custom_mse: 462069.3750\n",
            "reduce_confident_loss =  0.661344469\n",
            "\n",
            "reduce_mean_cls_loss =  0.336813509\n",
            "\n",
            "regression loss =  8.4467392\n",
            "50/73 [===================>..........] - ETA: 4:09 - loss: 32.8063 - custom_mse: 465047.5938\n",
            "reduce_confident_loss =  0.742312729\n",
            "\n",
            "reduce_mean_cls_loss =  0.32662943\n",
            "\n",
            "regression loss =  3.8785665\n",
            "51/73 [===================>..........] - ETA: 3:57 - loss: 32.2601 - custom_mse: 466942.7500\n",
            "reduce_confident_loss =  0.768320918\n",
            "\n",
            "reduce_mean_cls_loss =  0.339866847\n",
            "\n",
            "regression loss =  5.81082106\n",
            "52/73 [====================>.........] - ETA: 3:44 - loss: 31.7727 - custom_mse: 467657.7188\n",
            "reduce_confident_loss =  0.739187896\n",
            "\n",
            "reduce_mean_cls_loss =  0.444519699\n",
            "\n",
            "regression loss =  29.3223038\n",
            "53/73 [====================>.........] - ETA: 3:34 - loss: 31.7488 - custom_mse: 477599.1250\n",
            "reduce_confident_loss =  0.776116192\n",
            "\n",
            "reduce_mean_cls_loss =  0.796795487\n",
            "\n",
            "regression loss =  80.5632248\n",
            "54/73 [=====================>........] - ETA: 3:22 - loss: 32.6819 - custom_mse: 480862.4375\n",
            "reduce_confident_loss =  0.76423347\n",
            "\n",
            "reduce_mean_cls_loss =  0.874367416\n",
            "\n",
            "regression loss =  72.6887817\n",
            "55/73 [=====================>........] - ETA: 3:10 - loss: 33.4391 - custom_mse: 483518.8438\n",
            "reduce_confident_loss =  0.757611811\n",
            "\n",
            "reduce_mean_cls_loss =  0.98488152\n",
            "\n",
            "regression loss =  71.5945053\n",
            "56/73 [======================>.......] - ETA: 2:58 - loss: 34.1516 - custom_mse: 486090.5625\n",
            "reduce_confident_loss =  0.744085312\n",
            "\n",
            "reduce_mean_cls_loss =  0.846974492\n",
            "\n",
            "regression loss =  28.4009781\n",
            "57/73 [======================>.......] - ETA: 2:47 - loss: 34.0786 - custom_mse: 489060.1875\n",
            "reduce_confident_loss =  0.69803375\n",
            "\n",
            "reduce_mean_cls_loss =  0.350993186\n",
            "\n",
            "regression loss =  8.58009052\n",
            "58/73 [======================>.......] - ETA: 2:36 - loss: 33.6571 - custom_mse: 493887.1250\n",
            "reduce_confident_loss =  0.697911799\n",
            "\n",
            "reduce_mean_cls_loss =  0.469201893\n",
            "\n",
            "regression loss =  22.4458427\n",
            "59/73 [=======================>......] - ETA: 2:25 - loss: 33.4868 - custom_mse: 505752.2500\n",
            "reduce_confident_loss =  0.691304505\n",
            "\n",
            "reduce_mean_cls_loss =  0.672789156\n",
            "\n",
            "regression loss =  26.7353344\n",
            "60/73 [=======================>......] - ETA: 2:14 - loss: 33.3970 - custom_mse: 513599.5312\n",
            "reduce_confident_loss =  0.701613486\n",
            "\n",
            "reduce_mean_cls_loss =  0.417828739\n",
            "\n",
            "regression loss =  9.7567091\n",
            "61/73 [========================>.....] - ETA: 2:03 - loss: 33.0278 - custom_mse: 521126.9688\n",
            "reduce_confident_loss =  0.705881715\n",
            "\n",
            "reduce_mean_cls_loss =  0.443282664\n",
            "\n",
            "regression loss =  48.4708939\n",
            "62/73 [========================>.....] - ETA: 1:52 - loss: 33.2955 - custom_mse: 534342.0000\n",
            "reduce_confident_loss =  0.67805326\n",
            "\n",
            "reduce_mean_cls_loss =  0.76394254\n",
            "\n",
            "regression loss =  75.2209244\n",
            "63/73 [========================>.....] - ETA: 1:41 - loss: 33.9838 - custom_mse: 544417.8125\n",
            "reduce_confident_loss =  0.706692815\n",
            "\n",
            "reduce_mean_cls_loss =  0.564373314\n",
            "\n",
            "regression loss =  20.0107193\n",
            "64/73 [=========================>....] - ETA: 1:31 - loss: 33.7854 - custom_mse: 551871.2500\n",
            "reduce_confident_loss =  0.708367825\n",
            "\n",
            "reduce_mean_cls_loss =  0.395062953\n",
            "\n",
            "regression loss =  4.12052298\n",
            "65/73 [=========================>....] - ETA: 1:20 - loss: 33.3459 - custom_mse: 559187.0000\n",
            "reduce_confident_loss =  0.704853714\n",
            "\n",
            "reduce_mean_cls_loss =  0.377446324\n",
            "\n",
            "regression loss =  3.46073937\n",
            "66/73 [==========================>...] - ETA: 1:10 - loss: 32.9095 - custom_mse: 567462.8750\n",
            "reduce_confident_loss =  0.723216593\n",
            "\n",
            "reduce_mean_cls_loss =  0.421318084\n",
            "\n",
            "regression loss =  6.62841368\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 32.5344 - custom_mse: 579685.3750 \n",
            "reduce_confident_loss =  0.705489635\n",
            "\n",
            "reduce_mean_cls_loss =  0.37748161\n",
            "\n",
            "regression loss =  6.20800209\n",
            "68/73 [==========================>...] - ETA: 49s - loss: 32.1631 - custom_mse: 589516.6875\n",
            "reduce_confident_loss =  0.709839284\n",
            "\n",
            "reduce_mean_cls_loss =  0.357863575\n",
            "\n",
            "regression loss =  5.02657461\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 31.7853 - custom_mse: 598087.1250\n",
            "reduce_confident_loss =  0.741774678\n",
            "\n",
            "reduce_mean_cls_loss =  0.376179457\n",
            "\n",
            "regression loss =  5.18843603\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 31.4213 - custom_mse: 606518.7500\n",
            "reduce_confident_loss =  0.703833282\n",
            "\n",
            "reduce_mean_cls_loss =  0.320863694\n",
            "\n",
            "regression loss =  5.34808731\n",
            "71/73 [============================>.] - ETA: 19s - loss: 31.0685 - custom_mse: 609676.0000\n",
            "reduce_confident_loss =  0.700403154\n",
            "\n",
            "reduce_mean_cls_loss =  0.317019701\n",
            "\n",
            "regression loss =  9.60189819\n",
            "72/73 [============================>.] - ETA: 9s - loss: 30.7845 - custom_mse: 612402.2500 \n",
            "reduce_confident_loss =  0.703050733\n",
            "\n",
            "reduce_mean_cls_loss =  0.318833888\n",
            "\n",
            "regression loss =  9.48614693\n",
            "73/73 [==============================] - ETA: 0s - loss: 30.6447 - custom_mse: 615116.0625\n",
            "reduce_confident_loss =  0.702396929\n",
            "\n",
            "reduce_mean_cls_loss =  0.320348054\n",
            "\n",
            "regression loss =  7.35831213\n",
            "\n",
            "reduce_confident_loss =  0.711683691\n",
            "\n",
            "reduce_mean_cls_loss =  0.319682747\n",
            "\n",
            "regression loss =  8.79542923\n",
            "\n",
            "reduce_confident_loss =  0.742065489\n",
            "\n",
            "reduce_mean_cls_loss =  0.318528831\n",
            "\n",
            "regression loss =  7.44091225\n",
            "\n",
            "reduce_confident_loss =  0.715326607\n",
            "\n",
            "reduce_mean_cls_loss =  0.328212023\n",
            "\n",
            "regression loss =  12.9781523\n",
            "\n",
            "reduce_confident_loss =  0.678090155\n",
            "\n",
            "reduce_mean_cls_loss =  0.510680676\n",
            "\n",
            "regression loss =  44.3046303\n",
            "\n",
            "reduce_confident_loss =  0.684882343\n",
            "\n",
            "reduce_mean_cls_loss =  0.471910059\n",
            "\n",
            "regression loss =  30.7182617\n",
            "\n",
            "reduce_confident_loss =  0.701103687\n",
            "\n",
            "reduce_mean_cls_loss =  0.332940668\n",
            "\n",
            "regression loss =  16.2150784\n",
            "\n",
            "reduce_confident_loss =  0.697155476\n",
            "\n",
            "reduce_mean_cls_loss =  0.315724045\n",
            "\n",
            "regression loss =  15.0079842\n",
            "\n",
            "reduce_confident_loss =  0.696428418\n",
            "\n",
            "reduce_mean_cls_loss =  0.315113842\n",
            "\n",
            "regression loss =  11.5443945\n",
            "\n",
            "reduce_confident_loss =  0.730326593\n",
            "\n",
            "reduce_mean_cls_loss =  0.316674709\n",
            "\n",
            "regression loss =  7.66503906\n",
            "\n",
            "reduce_confident_loss =  0.754985869\n",
            "\n",
            "reduce_mean_cls_loss =  0.318830401\n",
            "\n",
            "regression loss =  7.15303516\n",
            "\n",
            "reduce_confident_loss =  0.783348382\n",
            "\n",
            "reduce_mean_cls_loss =  0.326632649\n",
            "\n",
            "regression loss =  8.97864532\n",
            "\n",
            "reduce_confident_loss =  0.719280899\n",
            "\n",
            "reduce_mean_cls_loss =  0.355956703\n",
            "\n",
            "regression loss =  10.7289925\n",
            "\n",
            "reduce_confident_loss =  0.705876887\n",
            "\n",
            "reduce_mean_cls_loss =  0.710432\n",
            "\n",
            "regression loss =  67.5974426\n",
            "73/73 [==============================] - 845s 11s/step - loss: 30.6447 - custom_mse: 615116.0625 - val_loss: 17.8122 - val_custom_mse: 87561.9922\n",
            "Epoch 9/15\n",
            "\n",
            "reduce_confident_loss =  0.693236768\n",
            "\n",
            "reduce_mean_cls_loss =  1.07890344\n",
            "\n",
            "regression loss =  95.9883041\n",
            " 1/73 [..............................] - ETA: 32:19 - loss: 97.7604 - custom_mse: 14106.0098\n",
            "reduce_confident_loss =  0.69744581\n",
            "\n",
            "reduce_mean_cls_loss =  0.658215344\n",
            "\n",
            "regression loss =  48.1372032\n",
            " 2/73 [..............................] - ETA: 11:52 - loss: 73.6267 - custom_mse: 19507.2891\n",
            "reduce_confident_loss =  0.696210921\n",
            "\n",
            "reduce_mean_cls_loss =  0.578076661\n",
            "\n",
            "regression loss =  29.9685307\n",
            " 3/73 [>.............................] - ETA: 11:12 - loss: 59.4987 - custom_mse: 29819.5039\n",
            "reduce_confident_loss =  0.69503206\n",
            "\n",
            "reduce_mean_cls_loss =  0.534864306\n",
            "\n",
            "regression loss =  25.981617\n",
            " 4/73 [>.............................] - ETA: 10:54 - loss: 51.4269 - custom_mse: 39948.5938\n",
            "reduce_confident_loss =  0.684511721\n",
            "\n",
            "reduce_mean_cls_loss =  0.536418676\n",
            "\n",
            "regression loss =  24.726984\n",
            " 5/73 [=>............................] - ETA: 10:49 - loss: 46.3311 - custom_mse: 49728.1602\n",
            "reduce_confident_loss =  0.695208609\n",
            "\n",
            "reduce_mean_cls_loss =  0.554184377\n",
            "\n",
            "regression loss =  24.2513084\n",
            " 6/73 [=>............................] - ETA: 10:35 - loss: 42.8594 - custom_mse: 59141.1641\n",
            "reduce_confident_loss =  0.706167042\n",
            "\n",
            "reduce_mean_cls_loss =  0.516517282\n",
            "\n",
            "regression loss =  36.5794563\n",
            " 7/73 [=>............................] - ETA: 10:22 - loss: 42.1369 - custom_mse: 68717.5078\n",
            "reduce_confident_loss =  0.691794753\n",
            "\n",
            "reduce_mean_cls_loss =  0.432247192\n",
            "\n",
            "regression loss =  25.0423203\n",
            " 8/73 [==>...........................] - ETA: 10:29 - loss: 40.1406 - custom_mse: 70803.5938\n",
            "reduce_confident_loss =  0.698843896\n",
            "\n",
            "reduce_mean_cls_loss =  0.361295372\n",
            "\n",
            "regression loss =  10.4519815\n",
            " 9/73 [==>...........................] - ETA: 10:13 - loss: 36.9596 - custom_mse: 74401.0703\n",
            "reduce_confident_loss =  0.702240527\n",
            "\n",
            "reduce_mean_cls_loss =  0.384289205\n",
            "\n",
            "regression loss =  6.48660374\n",
            "10/73 [===>..........................] - ETA: 9:59 - loss: 34.0210 - custom_mse: 79067.2266 \n",
            "reduce_confident_loss =  0.706021786\n",
            "\n",
            "reduce_mean_cls_loss =  0.387859672\n",
            "\n",
            "regression loss =  21.1817684\n",
            "11/73 [===>..........................] - ETA: 9:46 - loss: 32.9532 - custom_mse: 84611.6719\n",
            "reduce_confident_loss =  0.710851371\n",
            "\n",
            "reduce_mean_cls_loss =  0.466176271\n",
            "\n",
            "regression loss =  24.5783672\n",
            "12/73 [===>..........................] - ETA: 9:33 - loss: 32.3534 - custom_mse: 96524.1016\n",
            "reduce_confident_loss =  0.698208928\n",
            "\n",
            "reduce_mean_cls_loss =  0.368405789\n",
            "\n",
            "regression loss =  8.51452732\n",
            "13/73 [====>.........................] - ETA: 9:20 - loss: 30.6017 - custom_mse: 104869.2188\n",
            "reduce_confident_loss =  0.7015329\n",
            "\n",
            "reduce_mean_cls_loss =  0.41327402\n",
            "\n",
            "regression loss =  9.3398447\n",
            "14/73 [====>.........................] - ETA: 9:09 - loss: 29.1626 - custom_mse: 112501.8984\n",
            "reduce_confident_loss =  0.695798099\n",
            "\n",
            "reduce_mean_cls_loss =  0.353484392\n",
            "\n",
            "regression loss =  5.56749725\n",
            "15/73 [=====>........................] - ETA: 9:00 - loss: 27.6596 - custom_mse: 120235.4609\n",
            "reduce_confident_loss =  0.694014907\n",
            "\n",
            "reduce_mean_cls_loss =  0.412906\n",
            "\n",
            "regression loss =  5.18974829\n",
            "16/73 [=====>........................] - ETA: 8:48 - loss: 26.3244 - custom_mse: 128531.0000\n",
            "reduce_confident_loss =  0.696815789\n",
            "\n",
            "reduce_mean_cls_loss =  0.40563345\n",
            "\n",
            "regression loss =  21.2528763\n",
            "17/73 [=====>........................] - ETA: 8:37 - loss: 26.0909 - custom_mse: 140547.9219\n",
            "reduce_confident_loss =  0.69246763\n",
            "\n",
            "reduce_mean_cls_loss =  0.622880578\n",
            "\n",
            "regression loss =  39.7978134\n",
            "18/73 [======>.......................] - ETA: 8:27 - loss: 26.9255 - custom_mse: 147233.1719\n",
            "reduce_confident_loss =  0.693256676\n",
            "\n",
            "reduce_mean_cls_loss =  0.643491209\n",
            "\n",
            "regression loss =  42.8140526\n",
            "19/73 [======>.......................] - ETA: 10:25 - loss: 27.8321 - custom_mse: 152910.8594\n",
            "reduce_confident_loss =  0.678085446\n",
            "\n",
            "reduce_mean_cls_loss =  0.570112348\n",
            "\n",
            "regression loss =  24.2635345\n",
            "20/73 [=======>......................] - ETA: 10:06 - loss: 27.7161 - custom_mse: 164385.1719\n",
            "reduce_confident_loss =  0.678310275\n",
            "\n",
            "reduce_mean_cls_loss =  0.587535203\n",
            "\n",
            "regression loss =  26.602541\n",
            "21/73 [=======>......................] - ETA: 9:49 - loss: 27.7233 - custom_mse: 171495.0312 \n",
            "reduce_confident_loss =  0.659608245\n",
            "\n",
            "reduce_mean_cls_loss =  0.498329967\n",
            "\n",
            "regression loss =  25.8225307\n",
            "22/73 [========>.....................] - ETA: 9:32 - loss: 27.6896 - custom_mse: 181416.5781\n",
            "reduce_confident_loss =  0.674000323\n",
            "\n",
            "reduce_mean_cls_loss =  0.441173941\n",
            "\n",
            "regression loss =  16.1398544\n",
            "23/73 [========>.....................] - ETA: 9:19 - loss: 27.2359 - custom_mse: 184660.5469\n",
            "reduce_confident_loss =  0.693172038\n",
            "\n",
            "reduce_mean_cls_loss =  0.318117797\n",
            "\n",
            "regression loss =  14.0920401\n",
            "24/73 [========>.....................] - ETA: 9:07 - loss: 26.7304 - custom_mse: 187114.0000\n",
            "reduce_confident_loss =  0.706456304\n",
            "\n",
            "reduce_mean_cls_loss =  0.314576775\n",
            "\n",
            "regression loss =  12.3230896\n",
            "25/73 [=========>....................] - ETA: 9:09 - loss: 26.1949 - custom_mse: 190227.8750\n",
            "reduce_confident_loss =  0.695438862\n",
            "\n",
            "reduce_mean_cls_loss =  0.350179106\n",
            "\n",
            "regression loss =  28.0934372\n",
            "26/73 [=========>....................] - ETA: 8:52 - loss: 26.3081 - custom_mse: 200948.2188\n",
            "reduce_confident_loss =  0.697766185\n",
            "\n",
            "reduce_mean_cls_loss =  0.390618563\n",
            "\n",
            "regression loss =  16.5584888\n",
            "27/73 [==========>...................] - ETA: 8:35 - loss: 25.9874 - custom_mse: 206572.1406\n",
            "reduce_confident_loss =  0.72741127\n",
            "\n",
            "reduce_mean_cls_loss =  0.321186751\n",
            "\n",
            "regression loss =  6.36070585\n",
            "28/73 [==========>...................] - ETA: 8:19 - loss: 25.3238 - custom_mse: 207321.9844\n",
            "reduce_confident_loss =  0.765450597\n",
            "\n",
            "reduce_mean_cls_loss =  0.330913305\n",
            "\n",
            "regression loss =  7.05237\n",
            "29/73 [==========>...................] - ETA: 8:04 - loss: 24.7316 - custom_mse: 207448.7031\n",
            "reduce_confident_loss =  0.774570525\n",
            "\n",
            "reduce_mean_cls_loss =  0.324329764\n",
            "\n",
            "regression loss =  9.10860443\n",
            "30/73 [===========>..................] - ETA: 7:49 - loss: 24.2475 - custom_mse: 207703.1562\n",
            "reduce_confident_loss =  0.618993223\n",
            "\n",
            "reduce_mean_cls_loss =  0.420749724\n",
            "\n",
            "regression loss =  9.72613239\n",
            "31/73 [===========>..................] - ETA: 7:35 - loss: 23.8126 - custom_mse: 207838.0469\n",
            "reduce_confident_loss =  0.633543551\n",
            "\n",
            "reduce_mean_cls_loss =  0.403190821\n",
            "\n",
            "regression loss =  12.9191151\n",
            "32/73 [============>.................] - ETA: 7:21 - loss: 23.5046 - custom_mse: 208814.1094\n",
            "reduce_confident_loss =  0.697197\n",
            "\n",
            "reduce_mean_cls_loss =  0.32749933\n",
            "\n",
            "regression loss =  9.83725262\n",
            "33/73 [============>.................] - ETA: 7:07 - loss: 23.1214 - custom_mse: 212835.4375\n",
            "reduce_confident_loss =  0.698354065\n",
            "\n",
            "reduce_mean_cls_loss =  0.344943136\n",
            "\n",
            "regression loss =  15.963912\n",
            "34/73 [============>.................] - ETA: 6:54 - loss: 22.9416 - custom_mse: 219773.3125\n",
            "reduce_confident_loss =  0.6991629\n",
            "\n",
            "reduce_mean_cls_loss =  0.431308478\n",
            "\n",
            "regression loss =  20.6905899\n",
            "35/73 [=============>................] - ETA: 6:40 - loss: 22.9096 - custom_mse: 227887.7188\n",
            "reduce_confident_loss =  0.694441378\n",
            "\n",
            "reduce_mean_cls_loss =  0.386630476\n",
            "\n",
            "regression loss =  40.6791878\n",
            "36/73 [=============>................] - ETA: 6:34 - loss: 23.4332 - custom_mse: 232628.2031\n",
            "reduce_confident_loss =  0.715937614\n",
            "\n",
            "reduce_mean_cls_loss =  1.17938721\n",
            "\n",
            "regression loss =  55.8319092\n",
            "37/73 [==============>...............] - ETA: 6:21 - loss: 24.3601 - custom_mse: 242124.9531\n",
            "reduce_confident_loss =  0.731445312\n",
            "\n",
            "reduce_mean_cls_loss =  0.833236039\n",
            "\n",
            "regression loss =  44.0650063\n",
            "38/73 [==============>...............] - ETA: 6:14 - loss: 24.9198 - custom_mse: 249982.8125\n",
            "reduce_confident_loss =  0.707719\n",
            "\n",
            "reduce_mean_cls_loss =  1.2504698\n",
            "\n",
            "regression loss =  79.4442062\n",
            "39/73 [===============>..............] - ETA: 6:01 - loss: 26.3681 - custom_mse: 251504.2656\n",
            "reduce_confident_loss =  0.682027698\n",
            "\n",
            "reduce_mean_cls_loss =  1.1245836\n",
            "\n",
            "regression loss =  98.7489395\n",
            "40/73 [===============>..............] - ETA: 5:48 - loss: 28.2228 - custom_mse: 254109.8594\n",
            "reduce_confident_loss =  0.726247311\n",
            "\n",
            "reduce_mean_cls_loss =  1.09835017\n",
            "\n",
            "regression loss =  97.128952\n",
            "41/73 [===============>..............] - ETA: 5:40 - loss: 29.9479 - custom_mse: 307448.4062\n",
            "reduce_confident_loss =  0.698185384\n",
            "\n",
            "reduce_mean_cls_loss =  1.16973591\n",
            "\n",
            "regression loss =  132.420822\n",
            "42/73 [================>.............] - ETA: 5:27 - loss: 32.4322 - custom_mse: 361451.9688\n",
            "reduce_confident_loss =  0.700444221\n",
            "\n",
            "reduce_mean_cls_loss =  1.07102573\n",
            "\n",
            "regression loss =  99.4813843\n",
            "43/73 [================>.............] - ETA: 5:31 - loss: 34.0327 - custom_mse: 403693.9375\n",
            "reduce_confident_loss =  0.748708069\n",
            "\n",
            "reduce_mean_cls_loss =  0.936484158\n",
            "\n",
            "regression loss =  11.3168077\n",
            "44/73 [=================>............] - ETA: 5:18 - loss: 33.5547 - custom_mse: 414145.5938\n",
            "reduce_confident_loss =  0.732331395\n",
            "\n",
            "reduce_mean_cls_loss =  1.18910348\n",
            "\n",
            "regression loss =  13.4828634\n",
            "45/73 [=================>............] - ETA: 5:05 - loss: 33.1514 - custom_mse: 425369.0312\n",
            "reduce_confident_loss =  0.733505249\n",
            "\n",
            "reduce_mean_cls_loss =  1.05201483\n",
            "\n",
            "regression loss =  19.7358704\n",
            "46/73 [=================>............] - ETA: 4:56 - loss: 32.8986 - custom_mse: 441810.7812\n",
            "reduce_confident_loss =  0.752829432\n",
            "\n",
            "reduce_mean_cls_loss =  0.945096135\n",
            "\n",
            "regression loss =  17.0953617\n",
            "47/73 [==================>...........] - ETA: 4:43 - loss: 32.5984 - custom_mse: 467630.2188\n",
            "reduce_confident_loss =  0.710897267\n",
            "\n",
            "reduce_mean_cls_loss =  0.761545181\n",
            "\n",
            "regression loss =  18.6507244\n",
            "48/73 [==================>...........] - ETA: 4:32 - loss: 32.3385 - custom_mse: 477703.0312\n",
            "reduce_confident_loss =  0.686660409\n",
            "\n",
            "reduce_mean_cls_loss =  0.363883287\n",
            "\n",
            "regression loss =  13.8926468\n",
            "49/73 [===================>..........] - ETA: 4:20 - loss: 31.9835 - custom_mse: 482931.9375\n",
            "reduce_confident_loss =  0.664517283\n",
            "\n",
            "reduce_mean_cls_loss =  0.34366557\n",
            "\n",
            "regression loss =  7.8323679\n",
            "50/73 [===================>..........] - ETA: 4:08 - loss: 31.5207 - custom_mse: 485408.0938\n",
            "reduce_confident_loss =  0.720045209\n",
            "\n",
            "reduce_mean_cls_loss =  0.348199099\n",
            "\n",
            "regression loss =  3.95769238\n",
            "51/73 [===================>..........] - ETA: 3:56 - loss: 31.0012 - custom_mse: 486906.8438\n",
            "reduce_confident_loss =  0.794488549\n",
            "\n",
            "reduce_mean_cls_loss =  0.337464541\n",
            "\n",
            "regression loss =  6.29406595\n",
            "52/73 [====================>.........] - ETA: 3:43 - loss: 30.5478 - custom_mse: 487202.8438\n",
            "reduce_confident_loss =  0.732581437\n",
            "\n",
            "reduce_mean_cls_loss =  0.54612869\n",
            "\n",
            "regression loss =  29.2287655\n",
            "53/73 [====================>.........] - ETA: 3:33 - loss: 30.5470 - custom_mse: 497374.4062\n",
            "reduce_confident_loss =  0.777727544\n",
            "\n",
            "reduce_mean_cls_loss =  0.875161588\n",
            "\n",
            "regression loss =  74.6136246\n",
            "54/73 [=====================>........] - ETA: 3:21 - loss: 31.3937 - custom_mse: 500174.5312\n",
            "reduce_confident_loss =  0.780593932\n",
            "\n",
            "reduce_mean_cls_loss =  0.850730777\n",
            "\n",
            "regression loss =  70.4429245\n",
            "55/73 [=====================>........] - ETA: 3:09 - loss: 32.1333 - custom_mse: 502670.6250\n",
            "reduce_confident_loss =  0.764350057\n",
            "\n",
            "reduce_mean_cls_loss =  0.805018\n",
            "\n",
            "regression loss =  69.375824\n",
            "56/73 [======================>.......] - ETA: 2:57 - loss: 32.8264 - custom_mse: 505085.0000\n",
            "reduce_confident_loss =  0.753550231\n",
            "\n",
            "reduce_mean_cls_loss =  0.891906798\n",
            "\n",
            "regression loss =  25.8402672\n",
            "57/73 [======================>.......] - ETA: 2:46 - loss: 32.7327 - custom_mse: 508247.2188\n",
            "reduce_confident_loss =  0.710705638\n",
            "\n",
            "reduce_mean_cls_loss =  0.329064637\n",
            "\n",
            "regression loss =  9.02298069\n",
            "58/73 [======================>.......] - ETA: 2:35 - loss: 32.3419 - custom_mse: 512507.0625\n",
            "reduce_confident_loss =  0.716001153\n",
            "\n",
            "reduce_mean_cls_loss =  0.469332933\n",
            "\n",
            "regression loss =  21.3351097\n",
            "59/73 [=======================>......] - ETA: 2:24 - loss: 32.1754 - custom_mse: 525448.1875\n",
            "reduce_confident_loss =  0.710497737\n",
            "\n",
            "reduce_mean_cls_loss =  0.665891707\n",
            "\n",
            "regression loss =  25.4591064\n",
            "60/73 [=======================>......] - ETA: 2:13 - loss: 32.0864 - custom_mse: 534685.1250\n",
            "reduce_confident_loss =  0.697808266\n",
            "\n",
            "reduce_mean_cls_loss =  0.358057082\n",
            "\n",
            "regression loss =  6.64486694\n",
            "61/73 [========================>.....] - ETA: 2:02 - loss: 31.6866 - custom_mse: 542516.3750\n",
            "reduce_confident_loss =  0.730612755\n",
            "\n",
            "reduce_mean_cls_loss =  0.444966763\n",
            "\n",
            "regression loss =  48.6688385\n",
            "62/73 [========================>.....] - ETA: 1:51 - loss: 31.9795 - custom_mse: 556313.5000\n",
            "reduce_confident_loss =  0.678817689\n",
            "\n",
            "reduce_mean_cls_loss =  0.726893246\n",
            "\n",
            "regression loss =  73.315773\n",
            "63/73 [========================>.....] - ETA: 1:41 - loss: 32.6579 - custom_mse: 567598.5625\n",
            "reduce_confident_loss =  0.718162954\n",
            "\n",
            "reduce_mean_cls_loss =  0.5625\n",
            "\n",
            "regression loss =  18.779604\n",
            "64/73 [=========================>....] - ETA: 1:30 - loss: 32.4611 - custom_mse: 575598.9375\n",
            "reduce_confident_loss =  0.749890327\n",
            "\n",
            "reduce_mean_cls_loss =  0.385760844\n",
            "\n",
            "regression loss =  4.7437849\n",
            "65/73 [=========================>....] - ETA: 1:20 - loss: 32.0521 - custom_mse: 583187.0000\n",
            "reduce_confident_loss =  0.71044153\n",
            "\n",
            "reduce_mean_cls_loss =  0.372114629\n",
            "\n",
            "regression loss =  2.97430205\n",
            "66/73 [==========================>...] - ETA: 1:09 - loss: 31.6280 - custom_mse: 591929.6875\n",
            "reduce_confident_loss =  0.7219491\n",
            "\n",
            "reduce_mean_cls_loss =  0.388894826\n",
            "\n",
            "regression loss =  3.74311042\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 31.2284 - custom_mse: 604461.8125 \n",
            "reduce_confident_loss =  0.699816704\n",
            "\n",
            "reduce_mean_cls_loss =  0.343805045\n",
            "\n",
            "regression loss =  4.19732237\n",
            "68/73 [==========================>...] - ETA: 49s - loss: 30.8462 - custom_mse: 614270.7500\n",
            "reduce_confident_loss =  0.712165952\n",
            "\n",
            "reduce_mean_cls_loss =  0.354786366\n",
            "\n",
            "regression loss =  5.08877277\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 30.4884 - custom_mse: 623008.4375\n",
            "reduce_confident_loss =  0.742218792\n",
            "\n",
            "reduce_mean_cls_loss =  0.381637782\n",
            "\n",
            "regression loss =  5.65312481\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 30.1496 - custom_mse: 631496.7500\n",
            "reduce_confident_loss =  0.704855502\n",
            "\n",
            "reduce_mean_cls_loss =  0.320406497\n",
            "\n",
            "regression loss =  4.27718496\n",
            "71/73 [============================>.] - ETA: 19s - loss: 29.7997 - custom_mse: 634958.9375\n",
            "reduce_confident_loss =  0.697192311\n",
            "\n",
            "reduce_mean_cls_loss =  0.315887123\n",
            "\n",
            "regression loss =  10.6796951\n",
            "72/73 [============================>.] - ETA: 9s - loss: 29.5482 - custom_mse: 638116.6875 \n",
            "reduce_confident_loss =  0.697178662\n",
            "\n",
            "reduce_mean_cls_loss =  0.320159\n",
            "\n",
            "regression loss =  8.16905594\n",
            "73/73 [==============================] - ETA: 0s - loss: 29.4078 - custom_mse: 641403.2500\n",
            "reduce_confident_loss =  0.698449075\n",
            "\n",
            "reduce_mean_cls_loss =  0.31699881\n",
            "\n",
            "regression loss =  7.01706791\n",
            "\n",
            "reduce_confident_loss =  0.703801215\n",
            "\n",
            "reduce_mean_cls_loss =  0.31712\n",
            "\n",
            "regression loss =  8.18580818\n",
            "\n",
            "reduce_confident_loss =  0.735402703\n",
            "\n",
            "reduce_mean_cls_loss =  0.31752345\n",
            "\n",
            "regression loss =  4.19843149\n",
            "\n",
            "reduce_confident_loss =  0.707167923\n",
            "\n",
            "reduce_mean_cls_loss =  0.321234643\n",
            "\n",
            "regression loss =  12.0833111\n",
            "\n",
            "reduce_confident_loss =  0.686477661\n",
            "\n",
            "reduce_mean_cls_loss =  0.5066064\n",
            "\n",
            "regression loss =  44.7813797\n",
            "\n",
            "reduce_confident_loss =  0.683403\n",
            "\n",
            "reduce_mean_cls_loss =  0.476635605\n",
            "\n",
            "regression loss =  31.0458069\n",
            "\n",
            "reduce_confident_loss =  0.698069036\n",
            "\n",
            "reduce_mean_cls_loss =  0.32696715\n",
            "\n",
            "regression loss =  14.4284134\n",
            "\n",
            "reduce_confident_loss =  0.699875712\n",
            "\n",
            "reduce_mean_cls_loss =  0.314823\n",
            "\n",
            "regression loss =  15.7092142\n",
            "\n",
            "reduce_confident_loss =  0.696964383\n",
            "\n",
            "reduce_mean_cls_loss =  0.314531237\n",
            "\n",
            "regression loss =  11.2532234\n",
            "\n",
            "reduce_confident_loss =  0.719463825\n",
            "\n",
            "reduce_mean_cls_loss =  0.31515339\n",
            "\n",
            "regression loss =  7.04234743\n",
            "\n",
            "reduce_confident_loss =  0.754236639\n",
            "\n",
            "reduce_mean_cls_loss =  0.318688154\n",
            "\n",
            "regression loss =  7.1750989\n",
            "\n",
            "reduce_confident_loss =  0.781347156\n",
            "\n",
            "reduce_mean_cls_loss =  0.326783478\n",
            "\n",
            "regression loss =  7.89498234\n",
            "\n",
            "reduce_confident_loss =  0.71477741\n",
            "\n",
            "reduce_mean_cls_loss =  0.348234594\n",
            "\n",
            "regression loss =  11.0414162\n",
            "\n",
            "reduce_confident_loss =  0.70452708\n",
            "\n",
            "reduce_mean_cls_loss =  0.714630723\n",
            "\n",
            "regression loss =  67.7962799\n",
            "73/73 [==============================] - 837s 11s/step - loss: 29.4078 - custom_mse: 641403.2500 - val_loss: 17.2970 - val_custom_mse: 82344.5312\n",
            "Epoch 10/15\n",
            "\n",
            "reduce_confident_loss =  0.689699\n",
            "\n",
            "reduce_mean_cls_loss =  1.01927161\n",
            "\n",
            "regression loss =  96.5025787\n",
            " 1/73 [..............................] - ETA: 31:52 - loss: 98.2115 - custom_mse: 14619.1855\n",
            "reduce_confident_loss =  0.693561137\n",
            "\n",
            "reduce_mean_cls_loss =  0.643216252\n",
            "\n",
            "regression loss =  47.0564651\n",
            " 2/73 [..............................] - ETA: 11:48 - loss: 73.3024 - custom_mse: 20061.1211\n",
            "reduce_confident_loss =  0.694160521\n",
            "\n",
            "reduce_mean_cls_loss =  0.489211142\n",
            "\n",
            "regression loss =  29.0952129\n",
            " 3/73 [>.............................] - ETA: 11:05 - loss: 58.9611 - custom_mse: 31470.1074\n",
            "reduce_confident_loss =  0.689332962\n",
            "\n",
            "reduce_mean_cls_loss =  0.536245823\n",
            "\n",
            "regression loss =  24.0829277\n",
            " 4/73 [>.............................] - ETA: 10:42 - loss: 50.5480 - custom_mse: 42433.1758\n",
            "reduce_confident_loss =  0.69236958\n",
            "\n",
            "reduce_mean_cls_loss =  0.511957\n",
            "\n",
            "regression loss =  22.8552475\n",
            " 5/73 [=>............................] - ETA: 10:30 - loss: 45.2503 - custom_mse: 52554.3906\n",
            "reduce_confident_loss =  0.689003944\n",
            "\n",
            "reduce_mean_cls_loss =  0.508470118\n",
            "\n",
            "regression loss =  23.1807747\n",
            " 6/73 [=>............................] - ETA: 10:18 - loss: 41.7716 - custom_mse: 63034.8828\n",
            "reduce_confident_loss =  0.695410192\n",
            "\n",
            "reduce_mean_cls_loss =  0.518800557\n",
            "\n",
            "regression loss =  35.7089233\n",
            " 7/73 [=>............................] - ETA: 10:11 - loss: 41.0790 - custom_mse: 72718.1719\n",
            "reduce_confident_loss =  0.690054238\n",
            "\n",
            "reduce_mean_cls_loss =  0.392622888\n",
            "\n",
            "regression loss =  21.0037174\n",
            " 8/73 [==>...........................] - ETA: 10:18 - loss: 38.7049 - custom_mse: 74669.2969\n",
            "reduce_confident_loss =  0.696184933\n",
            "\n",
            "reduce_mean_cls_loss =  0.340573758\n",
            "\n",
            "regression loss =  10.8723249\n",
            " 9/73 [==>...........................] - ETA: 10:02 - loss: 35.7276 - custom_mse: 78256.3281\n",
            "reduce_confident_loss =  0.698187768\n",
            "\n",
            "reduce_mean_cls_loss =  0.408156693\n",
            "\n",
            "regression loss =  6.84663391\n",
            "10/73 [===>..........................] - ETA: 9:47 - loss: 32.9501 - custom_mse: 82674.9219 \n",
            "reduce_confident_loss =  0.703660309\n",
            "\n",
            "reduce_mean_cls_loss =  0.325933248\n",
            "\n",
            "regression loss =  24.0485439\n",
            "11/73 [===>..........................] - ETA: 9:34 - loss: 32.2345 - custom_mse: 87550.6875\n",
            "reduce_confident_loss =  0.720725\n",
            "\n",
            "reduce_mean_cls_loss =  0.481586158\n",
            "\n",
            "regression loss =  23.6643276\n",
            "12/73 [===>..........................] - ETA: 9:21 - loss: 31.6205 - custom_mse: 99973.8516\n",
            "reduce_confident_loss =  0.693777263\n",
            "\n",
            "reduce_mean_cls_loss =  0.370544434\n",
            "\n",
            "regression loss =  7.39180279\n",
            "13/73 [====>.........................] - ETA: 9:10 - loss: 29.8386 - custom_mse: 108461.8047\n",
            "reduce_confident_loss =  0.695334733\n",
            "\n",
            "reduce_mean_cls_loss =  0.385974348\n",
            "\n",
            "regression loss =  7.87692833\n",
            "14/73 [====>.........................] - ETA: 8:58 - loss: 28.3472 - custom_mse: 115578.3047\n",
            "reduce_confident_loss =  0.694272041\n",
            "\n",
            "reduce_mean_cls_loss =  0.368714958\n",
            "\n",
            "regression loss =  3.32982278\n",
            "15/73 [=====>........................] - ETA: 8:47 - loss: 26.7502 - custom_mse: 122598.8906\n",
            "reduce_confident_loss =  0.69485\n",
            "\n",
            "reduce_mean_cls_loss =  0.458808094\n",
            "\n",
            "regression loss =  6.31042242\n",
            "16/73 [=====>........................] - ETA: 8:36 - loss: 25.5448 - custom_mse: 130264.5781\n",
            "reduce_confident_loss =  0.694516182\n",
            "\n",
            "reduce_mean_cls_loss =  0.426468849\n",
            "\n",
            "regression loss =  18.6101665\n",
            "17/73 [=====>........................] - ETA: 8:26 - loss: 25.2029 - custom_mse: 141719.0625\n",
            "reduce_confident_loss =  0.692955315\n",
            "\n",
            "reduce_mean_cls_loss =  0.614381671\n",
            "\n",
            "regression loss =  40.0681458\n",
            "18/73 [======>.......................] - ETA: 8:17 - loss: 26.1013 - custom_mse: 148199.9062\n",
            "reduce_confident_loss =  0.692641377\n",
            "\n",
            "reduce_mean_cls_loss =  0.641401112\n",
            "\n",
            "regression loss =  43.2604179\n",
            "19/73 [======>.......................] - ETA: 10:14 - loss: 27.0747 - custom_mse: 153034.6406\n",
            "reduce_confident_loss =  0.659136832\n",
            "\n",
            "reduce_mean_cls_loss =  0.568210959\n",
            "\n",
            "regression loss =  22.0052795\n",
            "20/73 [=======>......................] - ETA: 9:56 - loss: 26.8825 - custom_mse: 165805.3125 \n",
            "reduce_confident_loss =  0.668853223\n",
            "\n",
            "reduce_mean_cls_loss =  0.549663961\n",
            "\n",
            "regression loss =  24.4880848\n",
            "21/73 [=======>......................] - ETA: 9:39 - loss: 26.8266 - custom_mse: 173642.8438\n",
            "reduce_confident_loss =  0.657466888\n",
            "\n",
            "reduce_mean_cls_loss =  0.478520155\n",
            "\n",
            "regression loss =  20.2771606\n",
            "22/73 [========>.....................] - ETA: 9:22 - loss: 26.5805 - custom_mse: 185681.4844\n",
            "reduce_confident_loss =  0.714810252\n",
            "\n",
            "reduce_mean_cls_loss =  0.436177909\n",
            "\n",
            "regression loss =  17.9057369\n",
            "23/73 [========>.....................] - ETA: 9:09 - loss: 26.2534 - custom_mse: 189092.6094\n",
            "reduce_confident_loss =  0.696851552\n",
            "\n",
            "reduce_mean_cls_loss =  0.317419559\n",
            "\n",
            "regression loss =  11.3268013\n",
            "24/73 [========>.....................] - ETA: 8:58 - loss: 25.6737 - custom_mse: 190883.9688\n",
            "reduce_confident_loss =  0.706096947\n",
            "\n",
            "reduce_mean_cls_loss =  0.314566761\n",
            "\n",
            "regression loss =  10.7019911\n",
            "25/73 [=========>....................] - ETA: 8:59 - loss: 25.1156 - custom_mse: 193407.6094\n",
            "reduce_confident_loss =  0.696828\n",
            "\n",
            "reduce_mean_cls_loss =  0.454619467\n",
            "\n",
            "regression loss =  25.0473423\n",
            "26/73 [=========>....................] - ETA: 8:42 - loss: 25.1573 - custom_mse: 202851.3438\n",
            "reduce_confident_loss =  0.695861399\n",
            "\n",
            "reduce_mean_cls_loss =  0.410162479\n",
            "\n",
            "regression loss =  15.3347263\n",
            "27/73 [==========>...................] - ETA: 8:25 - loss: 24.8345 - custom_mse: 207041.4375\n",
            "reduce_confident_loss =  0.762950063\n",
            "\n",
            "reduce_mean_cls_loss =  0.317189634\n",
            "\n",
            "regression loss =  7.09433365\n",
            "28/73 [==========>...................] - ETA: 8:10 - loss: 24.2395 - custom_mse: 207216.3750\n",
            "reduce_confident_loss =  0.792786181\n",
            "\n",
            "reduce_mean_cls_loss =  0.340714365\n",
            "\n",
            "regression loss =  7.63889074\n",
            "29/73 [==========>...................] - ETA: 7:55 - loss: 23.7061 - custom_mse: 207302.6250\n",
            "reduce_confident_loss =  0.797502458\n",
            "\n",
            "reduce_mean_cls_loss =  0.324113041\n",
            "\n",
            "regression loss =  8.15610123\n",
            "30/73 [===========>..................] - ETA: 7:41 - loss: 23.2252 - custom_mse: 207338.2969\n",
            "reduce_confident_loss =  0.622206032\n",
            "\n",
            "reduce_mean_cls_loss =  0.379442155\n",
            "\n",
            "regression loss =  9.84578419\n",
            "31/73 [===========>..................] - ETA: 7:27 - loss: 22.8259 - custom_mse: 207391.9375\n",
            "reduce_confident_loss =  0.604058325\n",
            "\n",
            "reduce_mean_cls_loss =  0.379743338\n",
            "\n",
            "regression loss =  13.4205332\n",
            "32/73 [============>.................] - ETA: 7:13 - loss: 22.5627 - custom_mse: 207919.2188\n",
            "reduce_confident_loss =  0.694986284\n",
            "\n",
            "reduce_mean_cls_loss =  0.342524499\n",
            "\n",
            "regression loss =  9.9844389\n",
            "33/73 [============>.................] - ETA: 6:59 - loss: 22.2130 - custom_mse: 211791.5625\n",
            "reduce_confident_loss =  0.694509625\n",
            "\n",
            "reduce_mean_cls_loss =  0.346097618\n",
            "\n",
            "regression loss =  8.87752342\n",
            "34/73 [============>.................] - ETA: 6:46 - loss: 21.8514 - custom_mse: 217476.6875\n",
            "reduce_confident_loss =  0.703492641\n",
            "\n",
            "reduce_mean_cls_loss =  0.336302817\n",
            "\n",
            "regression loss =  12.7068806\n",
            "35/73 [=============>................] - ETA: 6:33 - loss: 21.6198 - custom_mse: 223979.2344\n",
            "reduce_confident_loss =  0.693921685\n",
            "\n",
            "reduce_mean_cls_loss =  0.32270512\n",
            "\n",
            "regression loss =  34.9387283\n",
            "36/73 [=============>................] - ETA: 6:26 - loss: 22.0180 - custom_mse: 228185.7188\n",
            "reduce_confident_loss =  0.714692235\n",
            "\n",
            "reduce_mean_cls_loss =  1.05346406\n",
            "\n",
            "regression loss =  55.8054657\n",
            "37/73 [==============>...............] - ETA: 6:14 - loss: 22.9790 - custom_mse: 238073.7812\n",
            "reduce_confident_loss =  0.706852615\n",
            "\n",
            "reduce_mean_cls_loss =  0.804208815\n",
            "\n",
            "regression loss =  42.0242043\n",
            "38/73 [==============>...............] - ETA: 6:07 - loss: 23.5200 - custom_mse: 246130.5312\n",
            "reduce_confident_loss =  0.710242271\n",
            "\n",
            "reduce_mean_cls_loss =  1.1978395\n",
            "\n",
            "regression loss =  78.8190613\n",
            "39/73 [===============>..............] - ETA: 5:54 - loss: 24.9868 - custom_mse: 247534.8281\n",
            "reduce_confident_loss =  0.696278\n",
            "\n",
            "reduce_mean_cls_loss =  1.17941391\n",
            "\n",
            "regression loss =  98.0074234\n",
            "40/73 [===============>..............] - ETA: 5:42 - loss: 26.8592 - custom_mse: 250160.6562\n",
            "reduce_confident_loss =  0.722467721\n",
            "\n",
            "reduce_mean_cls_loss =  1.14207947\n",
            "\n",
            "regression loss =  94.4089279\n",
            "41/73 [===============>..............] - ETA: 5:34 - loss: 28.5522 - custom_mse: 306588.6562\n",
            "reduce_confident_loss =  0.696914315\n",
            "\n",
            "reduce_mean_cls_loss =  1.16964519\n",
            "\n",
            "regression loss =  129.041931\n",
            "42/73 [================>.............] - ETA: 5:21 - loss: 30.9893 - custom_mse: 362901.1250\n",
            "reduce_confident_loss =  0.716983199\n",
            "\n",
            "reduce_mean_cls_loss =  1.0185653\n",
            "\n",
            "regression loss =  97.8648224\n",
            "43/73 [================>.............] - ETA: 5:26 - loss: 32.5849 - custom_mse: 408445.1250\n",
            "reduce_confident_loss =  0.731126487\n",
            "\n",
            "reduce_mean_cls_loss =  0.986633897\n",
            "\n",
            "regression loss =  9.28179932\n",
            "44/73 [=================>............] - ETA: 5:13 - loss: 32.0943 - custom_mse: 419674.6562\n",
            "reduce_confident_loss =  0.742933393\n",
            "\n",
            "reduce_mean_cls_loss =  1.17252886\n",
            "\n",
            "regression loss =  13.2319717\n",
            "45/73 [=================>............] - ETA: 5:00 - loss: 31.7177 - custom_mse: 431318.0000\n",
            "reduce_confident_loss =  0.739246547\n",
            "\n",
            "reduce_mean_cls_loss =  0.986002386\n",
            "\n",
            "regression loss =  16.8690071\n",
            "46/73 [=================>............] - ETA: 4:51 - loss: 31.4324 - custom_mse: 448439.2188\n",
            "reduce_confident_loss =  0.734408677\n",
            "\n",
            "reduce_mean_cls_loss =  0.956411302\n",
            "\n",
            "regression loss =  14.9989223\n",
            "47/73 [==================>...........] - ETA: 4:38 - loss: 31.1188 - custom_mse: 476051.5312\n",
            "reduce_confident_loss =  0.723006248\n",
            "\n",
            "reduce_mean_cls_loss =  0.72967279\n",
            "\n",
            "regression loss =  16.7900238\n",
            "48/73 [==================>...........] - ETA: 4:27 - loss: 30.8505 - custom_mse: 486327.0625\n",
            "reduce_confident_loss =  0.677435219\n",
            "\n",
            "reduce_mean_cls_loss =  0.38932398\n",
            "\n",
            "regression loss =  13.5629711\n",
            "49/73 [===================>..........] - ETA: 4:16 - loss: 30.5195 - custom_mse: 491397.8125\n",
            "reduce_confident_loss =  0.663395345\n",
            "\n",
            "reduce_mean_cls_loss =  0.374208182\n",
            "\n",
            "regression loss =  6.5975728\n",
            "50/73 [===================>..........] - ETA: 4:04 - loss: 30.0618 - custom_mse: 493865.6250\n",
            "reduce_confident_loss =  0.721591413\n",
            "\n",
            "reduce_mean_cls_loss =  0.418063372\n",
            "\n",
            "regression loss =  3.7171886\n",
            "51/73 [===================>..........] - ETA: 3:52 - loss: 29.5676 - custom_mse: 495212.5312\n",
            "reduce_confident_loss =  0.789225519\n",
            "\n",
            "reduce_mean_cls_loss =  0.322112888\n",
            "\n",
            "regression loss =  4.30719471\n",
            "52/73 [====================>.........] - ETA: 3:40 - loss: 29.1032 - custom_mse: 495531.3125\n",
            "reduce_confident_loss =  0.734425426\n",
            "\n",
            "reduce_mean_cls_loss =  0.543385863\n",
            "\n",
            "regression loss =  26.7999973\n",
            "53/73 [====================>.........] - ETA: 3:29 - loss: 29.0838 - custom_mse: 506916.5625\n",
            "reduce_confident_loss =  0.759495616\n",
            "\n",
            "reduce_mean_cls_loss =  0.844412625\n",
            "\n",
            "regression loss =  68.7250671\n",
            "54/73 [=====================>........] - ETA: 3:18 - loss: 29.8476 - custom_mse: 509294.8750\n",
            "reduce_confident_loss =  0.764011919\n",
            "\n",
            "reduce_mean_cls_loss =  0.829352915\n",
            "\n",
            "regression loss =  62.764164\n",
            "55/73 [=====================>........] - ETA: 3:06 - loss: 30.4751 - custom_mse: 511268.6562\n",
            "reduce_confident_loss =  0.754282832\n",
            "\n",
            "reduce_mean_cls_loss =  0.819064677\n",
            "\n",
            "regression loss =  61.856266\n",
            "56/73 [======================>.......] - ETA: 2:54 - loss: 31.0635 - custom_mse: 513188.5625\n",
            "reduce_confident_loss =  0.754641652\n",
            "\n",
            "reduce_mean_cls_loss =  0.833220065\n",
            "\n",
            "regression loss =  23.6733532\n",
            "57/73 [======================>.......] - ETA: 2:44 - loss: 30.9617 - custom_mse: 516107.7812\n",
            "reduce_confident_loss =  0.705086887\n",
            "\n",
            "reduce_mean_cls_loss =  0.331076056\n",
            "\n",
            "regression loss =  8.20487\n",
            "58/73 [======================>.......] - ETA: 2:32 - loss: 30.5873 - custom_mse: 521188.5938\n",
            "reduce_confident_loss =  0.704453886\n",
            "\n",
            "reduce_mean_cls_loss =  0.459299\n",
            "\n",
            "regression loss =  20.2645397\n",
            "59/73 [=======================>......] - ETA: 2:22 - loss: 30.4320 - custom_mse: 535438.0000\n",
            "reduce_confident_loss =  0.704587936\n",
            "\n",
            "reduce_mean_cls_loss =  0.694634438\n",
            "\n",
            "regression loss =  24.3838081\n",
            "60/73 [=======================>......] - ETA: 2:11 - loss: 30.3545 - custom_mse: 545594.2500\n",
            "reduce_confident_loss =  0.69618237\n",
            "\n",
            "reduce_mean_cls_loss =  0.376739204\n",
            "\n",
            "regression loss =  6.77427435\n",
            "61/73 [========================>.....] - ETA: 2:00 - loss: 29.9856 - custom_mse: 554205.0625\n",
            "reduce_confident_loss =  0.700134397\n",
            "\n",
            "reduce_mean_cls_loss =  0.441885531\n",
            "\n",
            "regression loss =  46.4030457\n",
            "62/73 [========================>.....] - ETA: 1:50 - loss: 30.2688 - custom_mse: 569100.3125\n",
            "reduce_confident_loss =  0.673855722\n",
            "\n",
            "reduce_mean_cls_loss =  0.755636215\n",
            "\n",
            "regression loss =  69.5586319\n",
            "63/73 [========================>.....] - ETA: 1:39 - loss: 30.9151 - custom_mse: 582396.9375\n",
            "reduce_confident_loss =  0.700233161\n",
            "\n",
            "reduce_mean_cls_loss =  0.649054706\n",
            "\n",
            "regression loss =  18.8121872\n",
            "64/73 [=========================>....] - ETA: 1:29 - loss: 30.7471 - custom_mse: 591130.7500\n",
            "reduce_confident_loss =  0.722284615\n",
            "\n",
            "reduce_mean_cls_loss =  0.407689542\n",
            "\n",
            "regression loss =  5.06687212\n",
            "65/73 [=========================>....] - ETA: 1:18 - loss: 30.3694 - custom_mse: 599565.6875\n",
            "reduce_confident_loss =  0.725032687\n",
            "\n",
            "reduce_mean_cls_loss =  0.367881924\n",
            "\n",
            "regression loss =  3.99745345\n",
            "66/73 [==========================>...] - ETA: 1:08 - loss: 29.9864 - custom_mse: 608958.8125\n",
            "reduce_confident_loss =  0.705638587\n",
            "\n",
            "reduce_mean_cls_loss =  0.44289422\n",
            "\n",
            "regression loss =  3.00104213\n",
            "67/73 [==========================>...] - ETA: 58s - loss: 29.6008 - custom_mse: 621077.6250 \n",
            "reduce_confident_loss =  0.710084736\n",
            "\n",
            "reduce_mean_cls_loss =  0.357309967\n",
            "\n",
            "regression loss =  4.03040552\n",
            "68/73 [==========================>...] - ETA: 48s - loss: 29.2404 - custom_mse: 630585.2500\n",
            "reduce_confident_loss =  0.702826202\n",
            "\n",
            "reduce_mean_cls_loss =  0.364423811\n",
            "\n",
            "regression loss =  3.56956649\n",
            "69/73 [===========================>..] - ETA: 38s - loss: 28.8838 - custom_mse: 639803.9375\n",
            "reduce_confident_loss =  0.715189636\n",
            "\n",
            "reduce_mean_cls_loss =  0.340454906\n",
            "\n",
            "regression loss =  5.66100693\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 28.5672 - custom_mse: 647410.0625\n",
            "reduce_confident_loss =  0.703097403\n",
            "\n",
            "reduce_mean_cls_loss =  0.320893526\n",
            "\n",
            "regression loss =  4.85021782\n",
            "71/73 [============================>.] - ETA: 19s - loss: 28.2476 - custom_mse: 649134.4375\n",
            "reduce_confident_loss =  0.696991801\n",
            "\n",
            "reduce_mean_cls_loss =  0.317595065\n",
            "\n",
            "regression loss =  10.113678\n",
            "72/73 [============================>.] - ETA: 9s - loss: 28.0098 - custom_mse: 650671.9375 \n",
            "reduce_confident_loss =  0.701578557\n",
            "\n",
            "reduce_mean_cls_loss =  0.316865712\n",
            "\n",
            "regression loss =  10.4651089\n",
            "73/73 [==============================] - ETA: 0s - loss: 27.8958 - custom_mse: 652392.2500\n",
            "reduce_confident_loss =  0.698848724\n",
            "\n",
            "reduce_mean_cls_loss =  0.316457361\n",
            "\n",
            "regression loss =  3.68347335\n",
            "\n",
            "reduce_confident_loss =  0.704629064\n",
            "\n",
            "reduce_mean_cls_loss =  0.317256123\n",
            "\n",
            "regression loss =  7.38971233\n",
            "\n",
            "reduce_confident_loss =  0.733589947\n",
            "\n",
            "reduce_mean_cls_loss =  0.317749172\n",
            "\n",
            "regression loss =  4.41005\n",
            "\n",
            "reduce_confident_loss =  0.704422057\n",
            "\n",
            "reduce_mean_cls_loss =  0.320232\n",
            "\n",
            "regression loss =  10.9558296\n",
            "\n",
            "reduce_confident_loss =  0.689391553\n",
            "\n",
            "reduce_mean_cls_loss =  0.505031049\n",
            "\n",
            "regression loss =  44.5803566\n",
            "\n",
            "reduce_confident_loss =  0.679751873\n",
            "\n",
            "reduce_mean_cls_loss =  0.476048678\n",
            "\n",
            "regression loss =  30.9506969\n",
            "\n",
            "reduce_confident_loss =  0.698251367\n",
            "\n",
            "reduce_mean_cls_loss =  0.323755324\n",
            "\n",
            "regression loss =  13.1064377\n",
            "\n",
            "reduce_confident_loss =  0.699849427\n",
            "\n",
            "reduce_mean_cls_loss =  0.314733356\n",
            "\n",
            "regression loss =  15.2991028\n",
            "\n",
            "reduce_confident_loss =  0.697556138\n",
            "\n",
            "reduce_mean_cls_loss =  0.314399183\n",
            "\n",
            "regression loss =  10.0095835\n",
            "\n",
            "reduce_confident_loss =  0.712117255\n",
            "\n",
            "reduce_mean_cls_loss =  0.314595968\n",
            "\n",
            "regression loss =  6.4063077\n",
            "\n",
            "reduce_confident_loss =  0.729757905\n",
            "\n",
            "reduce_mean_cls_loss =  0.316776842\n",
            "\n",
            "regression loss =  7.57391357\n",
            "\n",
            "reduce_confident_loss =  0.797129571\n",
            "\n",
            "reduce_mean_cls_loss =  0.322374821\n",
            "\n",
            "regression loss =  6.98147631\n",
            "\n",
            "reduce_confident_loss =  0.710112154\n",
            "\n",
            "reduce_mean_cls_loss =  0.346753269\n",
            "\n",
            "regression loss =  9.2791605\n",
            "\n",
            "reduce_confident_loss =  0.703524\n",
            "\n",
            "reduce_mean_cls_loss =  0.716441095\n",
            "\n",
            "regression loss =  64.4725647\n",
            "73/73 [==============================] - 827s 11s/step - loss: 27.8958 - custom_mse: 652392.2500 - val_loss: 16.3283 - val_custom_mse: 80521.9609\n",
            "Epoch 11/15\n",
            "\n",
            "reduce_confident_loss =  0.693000138\n",
            "\n",
            "reduce_mean_cls_loss =  1.08494747\n",
            "\n",
            "regression loss =  95.085495\n",
            " 1/73 [..............................] - ETA: 31:43 - loss: 96.8634 - custom_mse: 14625.4805\n",
            "reduce_confident_loss =  0.693609595\n",
            "\n",
            "reduce_mean_cls_loss =  0.64627\n",
            "\n",
            "regression loss =  46.6593475\n",
            " 2/73 [..............................] - ETA: 11:56 - loss: 72.4313 - custom_mse: 19911.5762\n",
            "reduce_confident_loss =  0.681783736\n",
            "\n",
            "reduce_mean_cls_loss =  0.507971108\n",
            "\n",
            "regression loss =  28.3494587\n",
            " 3/73 [>.............................] - ETA: 11:09 - loss: 58.1340 - custom_mse: 32128.6270\n",
            "reduce_confident_loss =  0.69128567\n",
            "\n",
            "reduce_mean_cls_loss =  0.478634626\n",
            "\n",
            "regression loss =  23.769165\n",
            " 4/73 [>.............................] - ETA: 10:49 - loss: 49.8352 - custom_mse: 44296.7500\n",
            "reduce_confident_loss =  0.691820085\n",
            "\n",
            "reduce_mean_cls_loss =  0.504812479\n",
            "\n",
            "regression loss =  22.1821461\n",
            " 5/73 [=>............................] - ETA: 10:35 - loss: 44.5440 - custom_mse: 55281.3477\n",
            "reduce_confident_loss =  0.695939362\n",
            "\n",
            "reduce_mean_cls_loss =  0.489083052\n",
            "\n",
            "regression loss =  22.0121498\n",
            " 6/73 [=>............................] - ETA: 10:22 - loss: 40.9862 - custom_mse: 65778.1953\n",
            "reduce_confident_loss =  0.690669656\n",
            "\n",
            "reduce_mean_cls_loss =  0.511429131\n",
            "\n",
            "regression loss =  35.4577827\n",
            " 7/73 [=>............................] - ETA: 10:13 - loss: 40.3681 - custom_mse: 77103.8047\n",
            "reduce_confident_loss =  0.679121077\n",
            "\n",
            "reduce_mean_cls_loss =  0.409018248\n",
            "\n",
            "regression loss =  17.1293354\n",
            " 8/73 [==>...........................] - ETA: 10:20 - loss: 37.5993 - custom_mse: 78857.2969\n",
            "reduce_confident_loss =  0.695841491\n",
            "\n",
            "reduce_mean_cls_loss =  0.328547806\n",
            "\n",
            "regression loss =  6.29385662\n",
            " 9/73 [==>...........................] - ETA: 10:08 - loss: 34.2347 - custom_mse: 82065.3203\n",
            "reduce_confident_loss =  0.703605294\n",
            "\n",
            "reduce_mean_cls_loss =  0.348847926\n",
            "\n",
            "regression loss =  9.24159431\n",
            "10/73 [===>..........................] - ETA: 9:53 - loss: 31.8407 - custom_mse: 86361.3281 \n",
            "reduce_confident_loss =  0.702324271\n",
            "\n",
            "reduce_mean_cls_loss =  0.417656451\n",
            "\n",
            "regression loss =  23.3431759\n",
            "11/73 [===>..........................] - ETA: 9:39 - loss: 31.1700 - custom_mse: 91720.3516\n",
            "reduce_confident_loss =  0.703851342\n",
            "\n",
            "reduce_mean_cls_loss =  0.445316076\n",
            "\n",
            "regression loss =  22.1721191\n",
            "12/73 [===>..........................] - ETA: 9:28 - loss: 30.5159 - custom_mse: 104562.0547\n",
            "reduce_confident_loss =  0.695927501\n",
            "\n",
            "reduce_mean_cls_loss =  0.390941411\n",
            "\n",
            "regression loss =  7.49109316\n",
            "13/73 [====>.........................] - ETA: 9:16 - loss: 28.8284 - custom_mse: 113236.8828\n",
            "reduce_confident_loss =  0.69740665\n",
            "\n",
            "reduce_mean_cls_loss =  0.384295881\n",
            "\n",
            "regression loss =  6.96858215\n",
            "14/73 [====>.........................] - ETA: 9:04 - loss: 27.3442 - custom_mse: 121359.7344\n",
            "reduce_confident_loss =  0.693717182\n",
            "\n",
            "reduce_mean_cls_loss =  0.374594718\n",
            "\n",
            "regression loss =  2.90576649\n",
            "15/73 [=====>........................] - ETA: 8:53 - loss: 25.7862 - custom_mse: 128332.1094\n",
            "reduce_confident_loss =  0.693515182\n",
            "\n",
            "reduce_mean_cls_loss =  0.323727101\n",
            "\n",
            "regression loss =  3.97964835\n",
            "16/73 [=====>........................] - ETA: 8:42 - loss: 24.4869 - custom_mse: 135814.2031\n",
            "reduce_confident_loss =  0.694512069\n",
            "\n",
            "reduce_mean_cls_loss =  0.375219345\n",
            "\n",
            "regression loss =  17.7917767\n",
            "17/73 [=====>........................] - ETA: 8:32 - loss: 24.1560 - custom_mse: 147448.3750\n",
            "reduce_confident_loss =  0.686780512\n",
            "\n",
            "reduce_mean_cls_loss =  0.588001788\n",
            "\n",
            "regression loss =  38.9070511\n",
            "18/73 [======>.......................] - ETA: 8:22 - loss: 25.0463 - custom_mse: 154802.9219\n",
            "reduce_confident_loss =  0.689577758\n",
            "\n",
            "reduce_mean_cls_loss =  0.608297348\n",
            "\n",
            "regression loss =  42.0346\n",
            "19/73 [======>.......................] - ETA: 10:23 - loss: 26.0087 - custom_mse: 160397.8594\n",
            "reduce_confident_loss =  0.654009819\n",
            "\n",
            "reduce_mean_cls_loss =  0.471357554\n",
            "\n",
            "regression loss =  18.5461273\n",
            "20/73 [=======>......................] - ETA: 10:04 - loss: 25.6919 - custom_mse: 175333.1250\n",
            "reduce_confident_loss =  0.677198827\n",
            "\n",
            "reduce_mean_cls_loss =  0.529556215\n",
            "\n",
            "regression loss =  22.2606106\n",
            "21/73 [=======>......................] - ETA: 9:46 - loss: 25.5859 - custom_mse: 184830.5000 \n",
            "reduce_confident_loss =  0.66275245\n",
            "\n",
            "reduce_mean_cls_loss =  0.487877667\n",
            "\n",
            "regression loss =  15.2827711\n",
            "22/73 [========>.....................] - ETA: 9:29 - loss: 25.1699 - custom_mse: 198429.5312\n",
            "reduce_confident_loss =  0.721491277\n",
            "\n",
            "reduce_mean_cls_loss =  0.445544481\n",
            "\n",
            "regression loss =  15.120636\n",
            "23/73 [========>.....................] - ETA: 9:17 - loss: 24.7837 - custom_mse: 202108.8906\n",
            "reduce_confident_loss =  0.724008203\n",
            "\n",
            "reduce_mean_cls_loss =  0.318061024\n",
            "\n",
            "regression loss =  13.1110106\n",
            "24/73 [========>.....................] - ETA: 9:05 - loss: 24.3408 - custom_mse: 203338.9844\n",
            "reduce_confident_loss =  0.709304452\n",
            "\n",
            "reduce_mean_cls_loss =  0.331610382\n",
            "\n",
            "regression loss =  10.6617651\n",
            "25/73 [=========>....................] - ETA: 9:07 - loss: 23.8353 - custom_mse: 205748.5312\n",
            "reduce_confident_loss =  0.698451459\n",
            "\n",
            "reduce_mean_cls_loss =  0.3346183\n",
            "\n",
            "regression loss =  25.268837\n",
            "26/73 [=========>....................] - ETA: 8:49 - loss: 23.9301 - custom_mse: 214468.4688\n",
            "reduce_confident_loss =  0.701540351\n",
            "\n",
            "reduce_mean_cls_loss =  0.396114528\n",
            "\n",
            "regression loss =  15.1800308\n",
            "27/73 [==========>...................] - ETA: 8:33 - loss: 23.6467 - custom_mse: 217939.8125\n",
            "reduce_confident_loss =  0.733877838\n",
            "\n",
            "reduce_mean_cls_loss =  0.316743612\n",
            "\n",
            "regression loss =  6.17537498\n",
            "28/73 [==========>...................] - ETA: 8:17 - loss: 23.0603 - custom_mse: 218024.1406\n",
            "reduce_confident_loss =  0.76527071\n",
            "\n",
            "reduce_mean_cls_loss =  0.352246195\n",
            "\n",
            "regression loss =  7.18601131\n",
            "29/73 [==========>...................] - ETA: 8:02 - loss: 22.5514 - custom_mse: 218135.9219\n",
            "reduce_confident_loss =  0.768711507\n",
            "\n",
            "reduce_mean_cls_loss =  0.351249546\n",
            "\n",
            "regression loss =  9.07136536\n",
            "30/73 [===========>..................] - ETA: 7:47 - loss: 22.1394 - custom_mse: 218224.5625\n",
            "reduce_confident_loss =  0.665044904\n",
            "\n",
            "reduce_mean_cls_loss =  0.387503177\n",
            "\n",
            "regression loss =  9.77885\n",
            "31/73 [===========>..................] - ETA: 7:33 - loss: 21.7746 - custom_mse: 218312.0625\n",
            "reduce_confident_loss =  0.601074159\n",
            "\n",
            "reduce_mean_cls_loss =  0.362996072\n",
            "\n",
            "regression loss =  11.7320452\n",
            "32/73 [============>.................] - ETA: 7:19 - loss: 21.4909 - custom_mse: 218610.2969\n",
            "reduce_confident_loss =  0.695442379\n",
            "\n",
            "reduce_mean_cls_loss =  0.314161211\n",
            "\n",
            "regression loss =  8.23032379\n",
            "33/73 [============>.................] - ETA: 7:05 - loss: 21.1197 - custom_mse: 221837.6719\n",
            "reduce_confident_loss =  0.698767781\n",
            "\n",
            "reduce_mean_cls_loss =  0.369051427\n",
            "\n",
            "regression loss =  9.22326851\n",
            "34/73 [============>.................] - ETA: 6:52 - loss: 20.8012 - custom_mse: 227099.9844\n",
            "reduce_confident_loss =  0.699485183\n",
            "\n",
            "reduce_mean_cls_loss =  0.36968857\n",
            "\n",
            "regression loss =  10.4032907\n",
            "35/73 [=============>................] - ETA: 6:39 - loss: 20.5347 - custom_mse: 233035.3438\n",
            "reduce_confident_loss =  0.694989622\n",
            "\n",
            "reduce_mean_cls_loss =  0.315020263\n",
            "\n",
            "regression loss =  35.98283\n",
            "36/73 [=============>................] - ETA: 6:32 - loss: 20.9918 - custom_mse: 236731.4375\n",
            "reduce_confident_loss =  0.716795444\n",
            "\n",
            "reduce_mean_cls_loss =  0.965686619\n",
            "\n",
            "regression loss =  53.8290977\n",
            "37/73 [==============>...............] - ETA: 6:19 - loss: 21.9248 - custom_mse: 246669.4062\n",
            "reduce_confident_loss =  0.70782125\n",
            "\n",
            "reduce_mean_cls_loss =  0.808803\n",
            "\n",
            "regression loss =  40.6870461\n",
            "38/73 [==============>...............] - ETA: 6:11 - loss: 22.4585 - custom_mse: 254517.1406\n",
            "reduce_confident_loss =  0.70451504\n",
            "\n",
            "reduce_mean_cls_loss =  1.17959189\n",
            "\n",
            "regression loss =  75.3181152\n",
            "39/73 [===============>..............] - ETA: 5:59 - loss: 23.8621 - custom_mse: 255706.6406\n",
            "reduce_confident_loss =  0.694863498\n",
            "\n",
            "reduce_mean_cls_loss =  1.1156168\n",
            "\n",
            "regression loss =  92.3316116\n",
            "40/73 [===============>..............] - ETA: 5:46 - loss: 25.6191 - custom_mse: 257412.4219\n",
            "reduce_confident_loss =  0.744607925\n",
            "\n",
            "reduce_mean_cls_loss =  1.08475137\n",
            "\n",
            "regression loss =  88.3097458\n",
            "41/73 [===============>..............] - ETA: 5:38 - loss: 27.1928 - custom_mse: 316694.2188\n",
            "reduce_confident_loss =  0.694116533\n",
            "\n",
            "reduce_mean_cls_loss =  1.13995278\n",
            "\n",
            "regression loss =  126.120888\n",
            "42/73 [================>.............] - ETA: 5:25 - loss: 29.5919 - custom_mse: 376269.1250\n",
            "reduce_confident_loss =  0.713641226\n",
            "\n",
            "reduce_mean_cls_loss =  0.994986951\n",
            "\n",
            "regression loss =  90.1686\n",
            "43/73 [================>.............] - ETA: 5:29 - loss: 31.0404 - custom_mse: 423850.1875\n",
            "reduce_confident_loss =  0.762767315\n",
            "\n",
            "reduce_mean_cls_loss =  1.0122292\n",
            "\n",
            "regression loss =  6.26246262\n",
            "44/73 [=================>............] - ETA: 5:16 - loss: 30.5176 - custom_mse: 436613.6250\n",
            "reduce_confident_loss =  0.766464353\n",
            "\n",
            "reduce_mean_cls_loss =  1.17836165\n",
            "\n",
            "regression loss =  12.7508163\n",
            "45/73 [=================>............] - ETA: 5:04 - loss: 30.1660 - custom_mse: 447766.0000\n",
            "reduce_confident_loss =  0.727203846\n",
            "\n",
            "reduce_mean_cls_loss =  1.11374879\n",
            "\n",
            "regression loss =  15.0944529\n",
            "46/73 [=================>............] - ETA: 4:54 - loss: 29.8784 - custom_mse: 466465.4375\n",
            "reduce_confident_loss =  0.74589771\n",
            "\n",
            "reduce_mean_cls_loss =  1.0071758\n",
            "\n",
            "regression loss =  14.2064781\n",
            "47/73 [==================>...........] - ETA: 4:41 - loss: 29.5822 - custom_mse: 494590.8125\n",
            "reduce_confident_loss =  0.709681451\n",
            "\n",
            "reduce_mean_cls_loss =  0.690117836\n",
            "\n",
            "regression loss =  16.7314301\n",
            "48/73 [==================>...........] - ETA: 4:30 - loss: 29.3437 - custom_mse: 505170.5938\n",
            "reduce_confident_loss =  0.68315804\n",
            "\n",
            "reduce_mean_cls_loss =  0.433176577\n",
            "\n",
            "regression loss =  14.1663723\n",
            "49/73 [===================>..........] - ETA: 4:18 - loss: 29.0567 - custom_mse: 510730.3125\n",
            "reduce_confident_loss =  0.673455775\n",
            "\n",
            "reduce_mean_cls_loss =  0.359541178\n",
            "\n",
            "regression loss =  7.78969288\n",
            "50/73 [===================>..........] - ETA: 4:06 - loss: 28.6520 - custom_mse: 513107.0312\n",
            "reduce_confident_loss =  0.713600218\n",
            "\n",
            "reduce_mean_cls_loss =  0.373042077\n",
            "\n",
            "regression loss =  2.84798527\n",
            "51/73 [===================>..........] - ETA: 3:54 - loss: 28.1674 - custom_mse: 514609.6875\n",
            "reduce_confident_loss =  0.770080686\n",
            "\n",
            "reduce_mean_cls_loss =  0.334052593\n",
            "\n",
            "regression loss =  4.37605762\n",
            "52/73 [====================>.........] - ETA: 3:42 - loss: 27.7311 - custom_mse: 515072.1562\n",
            "reduce_confident_loss =  0.74080652\n",
            "\n",
            "reduce_mean_cls_loss =  0.538860381\n",
            "\n",
            "regression loss =  23.2159767\n",
            "53/73 [====================>.........] - ETA: 3:31 - loss: 27.6700 - custom_mse: 527884.3125\n",
            "reduce_confident_loss =  0.767176509\n",
            "\n",
            "reduce_mean_cls_loss =  0.803974211\n",
            "\n",
            "regression loss =  64.5237122\n",
            "54/73 [=====================>........] - ETA: 3:19 - loss: 28.3816 - custom_mse: 529986.7500\n",
            "reduce_confident_loss =  0.778886735\n",
            "\n",
            "reduce_mean_cls_loss =  0.807161331\n",
            "\n",
            "regression loss =  59.4939804\n",
            "55/73 [=====================>........] - ETA: 3:08 - loss: 28.9761 - custom_mse: 531766.8125\n",
            "reduce_confident_loss =  0.77712\n",
            "\n",
            "reduce_mean_cls_loss =  0.745310724\n",
            "\n",
            "regression loss =  59.1855812\n",
            "56/73 [======================>.......] - ETA: 2:56 - loss: 29.5428 - custom_mse: 533525.3125\n",
            "reduce_confident_loss =  0.744925\n",
            "\n",
            "reduce_mean_cls_loss =  0.786015689\n",
            "\n",
            "regression loss =  23.9887333\n",
            "57/73 [======================>.......] - ETA: 2:45 - loss: 29.4722 - custom_mse: 536843.5625\n",
            "reduce_confident_loss =  0.713660836\n",
            "\n",
            "reduce_mean_cls_loss =  0.325605541\n",
            "\n",
            "regression loss =  6.54620028\n",
            "58/73 [======================>.......] - ETA: 2:34 - loss: 29.0948 - custom_mse: 542383.1875\n",
            "reduce_confident_loss =  0.704993665\n",
            "\n",
            "reduce_mean_cls_loss =  0.426209837\n",
            "\n",
            "regression loss =  19.3020897\n",
            "59/73 [=======================>......] - ETA: 2:23 - loss: 28.9480 - custom_mse: 558355.3750\n",
            "reduce_confident_loss =  0.695911884\n",
            "\n",
            "reduce_mean_cls_loss =  0.683086276\n",
            "\n",
            "regression loss =  22.1101151\n",
            "60/73 [=======================>......] - ETA: 2:12 - loss: 28.8570 - custom_mse: 569236.4375\n",
            "reduce_confident_loss =  0.701428473\n",
            "\n",
            "reduce_mean_cls_loss =  0.424386531\n",
            "\n",
            "regression loss =  4.73427963\n",
            "61/73 [========================>.....] - ETA: 2:01 - loss: 28.4800 - custom_mse: 578581.5000\n",
            "reduce_confident_loss =  0.71385324\n",
            "\n",
            "reduce_mean_cls_loss =  0.472608536\n",
            "\n",
            "regression loss =  44.8964577\n",
            "62/73 [========================>.....] - ETA: 1:50 - loss: 28.7640 - custom_mse: 594485.5625\n",
            "reduce_confident_loss =  0.668597102\n",
            "\n",
            "reduce_mean_cls_loss =  0.733220935\n",
            "\n",
            "regression loss =  65.5252075\n",
            "63/73 [========================>.....] - ETA: 1:40 - loss: 29.3697 - custom_mse: 610367.0000\n",
            "reduce_confident_loss =  0.699298441\n",
            "\n",
            "reduce_mean_cls_loss =  0.587307572\n",
            "\n",
            "regression loss =  17.9706173\n",
            "64/73 [=========================>....] - ETA: 1:29 - loss: 29.2117 - custom_mse: 619458.7500\n",
            "reduce_confident_loss =  0.732438147\n",
            "\n",
            "reduce_mean_cls_loss =  0.432699531\n",
            "\n",
            "regression loss =  4.24674034\n",
            "65/73 [=========================>....] - ETA: 1:19 - loss: 28.8456 - custom_mse: 628179.4375\n",
            "reduce_confident_loss =  0.719183266\n",
            "\n",
            "reduce_mean_cls_loss =  0.379530489\n",
            "\n",
            "regression loss =  3.70705366\n",
            "66/73 [==========================>...] - ETA: 1:09 - loss: 28.4813 - custom_mse: 637573.1250\n",
            "reduce_confident_loss =  0.704212\n",
            "\n",
            "reduce_mean_cls_loss =  0.453879505\n",
            "\n",
            "regression loss =  4.07147408\n",
            "67/73 [==========================>...] - ETA: 59s - loss: 28.1343 - custom_mse: 649644.1875 \n",
            "reduce_confident_loss =  0.697849452\n",
            "\n",
            "reduce_mean_cls_loss =  0.35579735\n",
            "\n",
            "regression loss =  3.94414163\n",
            "68/73 [==========================>...] - ETA: 48s - loss: 27.7940 - custom_mse: 659391.5625\n",
            "reduce_confident_loss =  0.710512459\n",
            "\n",
            "reduce_mean_cls_loss =  0.323729575\n",
            "\n",
            "regression loss =  2.96991777\n",
            "69/73 [===========================>..] - ETA: 39s - loss: 27.4493 - custom_mse: 669647.5000\n",
            "reduce_confident_loss =  0.711541176\n",
            "\n",
            "reduce_mean_cls_loss =  0.413604349\n",
            "\n",
            "regression loss =  5.02742958\n",
            "70/73 [===========================>..] - ETA: 29s - loss: 27.1450 - custom_mse: 678474.0000\n",
            "reduce_confident_loss =  0.698244452\n",
            "\n",
            "reduce_mean_cls_loss =  0.323429644\n",
            "\n",
            "regression loss =  6.24385071\n",
            "71/73 [============================>.] - ETA: 19s - loss: 26.8650 - custom_mse: 681144.6875\n",
            "reduce_confident_loss =  0.6948964\n",
            "\n",
            "reduce_mean_cls_loss =  0.321539968\n",
            "\n",
            "regression loss =  9.26059055\n",
            "72/73 [============================>.] - ETA: 9s - loss: 26.6346 - custom_mse: 684024.6875 \n",
            "reduce_confident_loss =  0.694295406\n",
            "\n",
            "reduce_mean_cls_loss =  0.336230427\n",
            "\n",
            "regression loss =  5.11861515\n",
            "73/73 [==============================] - ETA: 0s - loss: 26.4934 - custom_mse: 687325.0625\n",
            "reduce_confident_loss =  0.700783193\n",
            "\n",
            "reduce_mean_cls_loss =  0.316208363\n",
            "\n",
            "regression loss =  1.73548985\n",
            "\n",
            "reduce_confident_loss =  0.7075\n",
            "\n",
            "reduce_mean_cls_loss =  0.317150176\n",
            "\n",
            "regression loss =  7.00725222\n",
            "\n",
            "reduce_confident_loss =  0.730262935\n",
            "\n",
            "reduce_mean_cls_loss =  0.317300111\n",
            "\n",
            "regression loss =  5.23407078\n",
            "\n",
            "reduce_confident_loss =  0.706508577\n",
            "\n",
            "reduce_mean_cls_loss =  0.319474727\n",
            "\n",
            "regression loss =  10.982213\n",
            "\n",
            "reduce_confident_loss =  0.689197719\n",
            "\n",
            "reduce_mean_cls_loss =  0.504619896\n",
            "\n",
            "regression loss =  44.540081\n",
            "\n",
            "reduce_confident_loss =  0.683343351\n",
            "\n",
            "reduce_mean_cls_loss =  0.479858547\n",
            "\n",
            "regression loss =  30.6493988\n",
            "\n",
            "reduce_confident_loss =  0.698332131\n",
            "\n",
            "reduce_mean_cls_loss =  0.322841614\n",
            "\n",
            "regression loss =  12.3933649\n",
            "\n",
            "reduce_confident_loss =  0.700043619\n",
            "\n",
            "reduce_mean_cls_loss =  0.314658314\n",
            "\n",
            "regression loss =  15.6120367\n",
            "\n",
            "reduce_confident_loss =  0.698566377\n",
            "\n",
            "reduce_mean_cls_loss =  0.314254135\n",
            "\n",
            "regression loss =  11.144907\n",
            "\n",
            "reduce_confident_loss =  0.707722\n",
            "\n",
            "reduce_mean_cls_loss =  0.314622432\n",
            "\n",
            "regression loss =  7.02078629\n",
            "\n",
            "reduce_confident_loss =  0.718039\n",
            "\n",
            "reduce_mean_cls_loss =  0.315416902\n",
            "\n",
            "regression loss =  8.33398914\n",
            "\n",
            "reduce_confident_loss =  0.802378774\n",
            "\n",
            "reduce_mean_cls_loss =  0.31848827\n",
            "\n",
            "regression loss =  5.17476749\n",
            "\n",
            "reduce_confident_loss =  0.710023046\n",
            "\n",
            "reduce_mean_cls_loss =  0.353856176\n",
            "\n",
            "regression loss =  7.43936062\n",
            "\n",
            "reduce_confident_loss =  0.707588434\n",
            "\n",
            "reduce_mean_cls_loss =  0.699391067\n",
            "\n",
            "regression loss =  62.0892906\n",
            "73/73 [==============================] - 832s 11s/step - loss: 26.4934 - custom_mse: 687325.0625 - val_loss: 15.9813 - val_custom_mse: 85488.5312\n",
            "Epoch 12/15\n",
            "\n",
            "reduce_confident_loss =  0.695399821\n",
            "\n",
            "reduce_mean_cls_loss =  1.02022982\n",
            "\n",
            "regression loss =  94.5271606\n",
            " 1/73 [..............................] - ETA: 32:14 - loss: 96.2428 - custom_mse: 15951.1455\n",
            "reduce_confident_loss =  0.694135368\n",
            "\n",
            "reduce_mean_cls_loss =  0.65819037\n",
            "\n",
            "regression loss =  44.9401207\n",
            " 2/73 [..............................] - ETA: 11:51 - loss: 71.2676 - custom_mse: 21267.0625\n",
            "reduce_confident_loss =  0.696810842\n",
            "\n",
            "reduce_mean_cls_loss =  0.48890087\n",
            "\n",
            "regression loss =  27.7200127\n",
            " 3/73 [>.............................] - ETA: 11:12 - loss: 57.1470 - custom_mse: 33574.3750\n",
            "reduce_confident_loss =  0.690186\n",
            "\n",
            "reduce_mean_cls_loss =  0.493331224\n",
            "\n",
            "regression loss =  22.6231174\n",
            " 4/73 [>.............................] - ETA: 10:54 - loss: 48.8119 - custom_mse: 46095.9570\n",
            "reduce_confident_loss =  0.69217217\n",
            "\n",
            "reduce_mean_cls_loss =  0.476639\n",
            "\n",
            "regression loss =  21.2920647\n",
            " 5/73 [=>............................] - ETA: 10:37 - loss: 43.5417 - custom_mse: 57269.2695\n",
            "reduce_confident_loss =  0.687232435\n",
            "\n",
            "reduce_mean_cls_loss =  0.484894603\n",
            "\n",
            "regression loss =  21.5654602\n",
            " 6/73 [=>............................] - ETA: 10:23 - loss: 40.0743 - custom_mse: 68731.3125\n",
            "reduce_confident_loss =  0.68896991\n",
            "\n",
            "reduce_mean_cls_loss =  0.502489686\n",
            "\n",
            "regression loss =  35.0044327\n",
            " 7/73 [=>............................] - ETA: 10:12 - loss: 39.5203 - custom_mse: 79945.4844\n",
            "reduce_confident_loss =  0.668685\n",
            "\n",
            "reduce_mean_cls_loss =  0.414298922\n",
            "\n",
            "regression loss =  16.9693775\n",
            " 8/73 [==>...........................] - ETA: 10:19 - loss: 36.8368 - custom_mse: 81355.1016\n",
            "reduce_confident_loss =  0.704452574\n",
            "\n",
            "reduce_mean_cls_loss =  0.337808818\n",
            "\n",
            "regression loss =  8.39242172\n",
            " 9/73 [==>...........................] - ETA: 10:04 - loss: 33.7921 - custom_mse: 84348.4531\n",
            "reduce_confident_loss =  0.705141246\n",
            "\n",
            "reduce_mean_cls_loss =  0.360934466\n",
            "\n",
            "regression loss =  8.37223339\n",
            "10/73 [===>..........................] - ETA: 9:50 - loss: 31.3567 - custom_mse: 88445.8594 \n",
            "reduce_confident_loss =  0.70844996\n",
            "\n",
            "reduce_mean_cls_loss =  0.407838732\n",
            "\n",
            "regression loss =  21.8041134\n",
            "11/73 [===>..........................] - ETA: 9:40 - loss: 30.5898 - custom_mse: 93032.3203\n",
            "reduce_confident_loss =  0.704842448\n",
            "\n",
            "reduce_mean_cls_loss =  0.474261791\n",
            "\n",
            "regression loss =  21.5010777\n",
            "12/73 [===>..........................] - ETA: 9:28 - loss: 29.9307 - custom_mse: 105679.4844\n",
            "reduce_confident_loss =  0.697350562\n",
            "\n",
            "reduce_mean_cls_loss =  0.382434309\n",
            "\n",
            "regression loss =  6.31302166\n",
            "13/73 [====>.........................] - ETA: 9:16 - loss: 28.1970 - custom_mse: 115630.8047\n",
            "reduce_confident_loss =  0.695186138\n",
            "\n",
            "reduce_mean_cls_loss =  0.342082262\n",
            "\n",
            "regression loss =  6.50341797\n",
            "14/73 [====>.........................] - ETA: 9:04 - loss: 26.7215 - custom_mse: 124377.1562\n",
            "reduce_confident_loss =  0.694499\n",
            "\n",
            "reduce_mean_cls_loss =  0.333299935\n",
            "\n",
            "regression loss =  2.66937375\n",
            "15/73 [=====>........................] - ETA: 8:53 - loss: 25.1866 - custom_mse: 131559.0312\n",
            "reduce_confident_loss =  0.693277299\n",
            "\n",
            "reduce_mean_cls_loss =  0.362406343\n",
            "\n",
            "regression loss =  3.64496827\n",
            "16/73 [=====>........................] - ETA: 8:42 - loss: 23.9062 - custom_mse: 139483.5469\n",
            "reduce_confident_loss =  0.694953322\n",
            "\n",
            "reduce_mean_cls_loss =  0.395311564\n",
            "\n",
            "regression loss =  16.8017807\n",
            "17/73 [=====>........................] - ETA: 8:31 - loss: 23.5524 - custom_mse: 150016.5312\n",
            "reduce_confident_loss =  0.692240536\n",
            "\n",
            "reduce_mean_cls_loss =  0.578846931\n",
            "\n",
            "regression loss =  38.9787674\n",
            "18/73 [======>.......................] - ETA: 8:21 - loss: 24.4801 - custom_mse: 157194.2812\n",
            "reduce_confident_loss =  0.692952633\n",
            "\n",
            "reduce_mean_cls_loss =  0.596159935\n",
            "\n",
            "regression loss =  39.4625473\n",
            "19/73 [======>.......................] - ETA: 10:18 - loss: 25.3365 - custom_mse: 163448.4375\n",
            "reduce_confident_loss =  0.662864566\n",
            "\n",
            "reduce_mean_cls_loss =  0.49676314\n",
            "\n",
            "regression loss =  18.0580521\n",
            "20/73 [=======>......................] - ETA: 10:00 - loss: 25.0305 - custom_mse: 180326.3906\n",
            "reduce_confident_loss =  0.659340322\n",
            "\n",
            "reduce_mean_cls_loss =  0.522647798\n",
            "\n",
            "regression loss =  16.9122124\n",
            "21/73 [=======>......................] - ETA: 9:42 - loss: 24.7002 - custom_mse: 192484.6875 \n",
            "reduce_confident_loss =  0.654812157\n",
            "\n",
            "reduce_mean_cls_loss =  0.493306607\n",
            "\n",
            "regression loss =  10.7713919\n",
            "22/73 [========>.....................] - ETA: 9:26 - loss: 24.1193 - custom_mse: 209010.8281\n",
            "reduce_confident_loss =  0.742426097\n",
            "\n",
            "reduce_mean_cls_loss =  0.468700081\n",
            "\n",
            "regression loss =  16.8556194\n",
            "23/73 [========>.....................] - ETA: 9:12 - loss: 23.8561 - custom_mse: 213016.9688\n",
            "reduce_confident_loss =  0.729875922\n",
            "\n",
            "reduce_mean_cls_loss =  0.320834577\n",
            "\n",
            "regression loss =  12.0626879\n",
            "24/73 [========>.....................] - ETA: 9:00 - loss: 23.4085 - custom_mse: 214177.9844\n",
            "reduce_confident_loss =  0.712012\n",
            "\n",
            "reduce_mean_cls_loss =  0.314123899\n",
            "\n",
            "regression loss =  11.4109564\n",
            "25/73 [=========>....................] - ETA: 9:02 - loss: 22.9697 - custom_mse: 216614.5469\n",
            "reduce_confident_loss =  0.696437776\n",
            "\n",
            "reduce_mean_cls_loss =  0.414222389\n",
            "\n",
            "regression loss =  23.6528568\n",
            "26/73 [=========>....................] - ETA: 8:44 - loss: 23.0386 - custom_mse: 225082.1562\n",
            "reduce_confident_loss =  0.69479239\n",
            "\n",
            "reduce_mean_cls_loss =  0.404251128\n",
            "\n",
            "regression loss =  15.1160479\n",
            "27/73 [==========>...................] - ETA: 8:28 - loss: 22.7859 - custom_mse: 228109.3594\n",
            "reduce_confident_loss =  0.756599545\n",
            "\n",
            "reduce_mean_cls_loss =  0.316705376\n",
            "\n",
            "regression loss =  5.88516808\n",
            "28/73 [==========>...................] - ETA: 8:12 - loss: 22.2207 - custom_mse: 228160.5312\n",
            "reduce_confident_loss =  0.732030034\n",
            "\n",
            "reduce_mean_cls_loss =  0.328813344\n",
            "\n",
            "regression loss =  6.40945721\n",
            "29/73 [==========>...................] - ETA: 7:57 - loss: 21.7120 - custom_mse: 228223.1250\n",
            "reduce_confident_loss =  0.74789989\n",
            "\n",
            "reduce_mean_cls_loss =  0.325678408\n",
            "\n",
            "regression loss =  8.78701115\n",
            "30/73 [===========>..................] - ETA: 7:43 - loss: 21.3170 - custom_mse: 228305.7812\n",
            "reduce_confident_loss =  0.631671786\n",
            "\n",
            "reduce_mean_cls_loss =  0.37715283\n",
            "\n",
            "regression loss =  9.86969566\n",
            "31/73 [===========>..................] - ETA: 7:28 - loss: 20.9803 - custom_mse: 228432.6875\n",
            "reduce_confident_loss =  0.62155807\n",
            "\n",
            "reduce_mean_cls_loss =  0.371316761\n",
            "\n",
            "regression loss =  12.3616371\n",
            "32/73 [============>.................] - ETA: 7:15 - loss: 20.7419 - custom_mse: 228707.6094\n",
            "reduce_confident_loss =  0.694764674\n",
            "\n",
            "reduce_mean_cls_loss =  0.323902518\n",
            "\n",
            "regression loss =  7.83879137\n",
            "33/73 [============>.................] - ETA: 7:01 - loss: 20.3818 - custom_mse: 231770.6719\n",
            "reduce_confident_loss =  0.694823623\n",
            "\n",
            "reduce_mean_cls_loss =  0.329007804\n",
            "\n",
            "regression loss =  6.40604448\n",
            "34/73 [============>.................] - ETA: 6:48 - loss: 20.0009 - custom_mse: 236327.2656\n",
            "reduce_confident_loss =  0.698351443\n",
            "\n",
            "reduce_mean_cls_loss =  0.347047269\n",
            "\n",
            "regression loss =  7.95180941\n",
            "35/73 [=============>................] - ETA: 6:35 - loss: 19.6865 - custom_mse: 241581.6094\n",
            "reduce_confident_loss =  0.695717216\n",
            "\n",
            "reduce_mean_cls_loss =  0.330690205\n",
            "\n",
            "regression loss =  34.1510124\n",
            "36/73 [=============>................] - ETA: 6:28 - loss: 20.1168 - custom_mse: 245069.0156\n",
            "reduce_confident_loss =  0.701259255\n",
            "\n",
            "reduce_mean_cls_loss =  1.00359905\n",
            "\n",
            "regression loss =  52.5298615\n",
            "37/73 [==============>...............] - ETA: 6:15 - loss: 21.0389 - custom_mse: 255045.7188\n",
            "reduce_confident_loss =  0.718867838\n",
            "\n",
            "reduce_mean_cls_loss =  0.800332725\n",
            "\n",
            "regression loss =  37.8089561\n",
            "38/73 [==============>...............] - ETA: 6:08 - loss: 21.5202 - custom_mse: 262889.8125\n",
            "reduce_confident_loss =  0.695138156\n",
            "\n",
            "reduce_mean_cls_loss =  1.11135733\n",
            "\n",
            "regression loss =  73.0585785\n",
            "39/73 [===============>..............] - ETA: 5:55 - loss: 22.8880 - custom_mse: 263841.1562\n",
            "reduce_confident_loss =  0.691130638\n",
            "\n",
            "reduce_mean_cls_loss =  1.16749513\n",
            "\n",
            "regression loss =  90.440506\n",
            "40/73 [===============>..............] - ETA: 5:43 - loss: 24.6233 - custom_mse: 265060.5938\n",
            "reduce_confident_loss =  0.743406653\n",
            "\n",
            "reduce_mean_cls_loss =  0.951342463\n",
            "\n",
            "regression loss =  87.3896484\n",
            "41/73 [===============>..............] - ETA: 5:34 - loss: 26.1955 - custom_mse: 328493.5312\n",
            "reduce_confident_loss =  0.700754702\n",
            "\n",
            "reduce_mean_cls_loss =  1.20463169\n",
            "\n",
            "regression loss =  121.326653\n",
            "42/73 [================>.............] - ETA: 5:22 - loss: 28.5059 - custom_mse: 393240.0625"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x241PrXyFmFC",
        "outputId": "4dc923bd-ef9f-413d-983a-deb8a7c54c88"
      },
      "source": [
        "# Test with nan values in validation = 0. I forgot to change it\n",
        "history_2 = run_experiment(vit_resnet_backbone_model,history_= 'use_generator')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mKết quả truyền trực tuyến bị cắt bớt đến 5000 dòng cuối.\u001b[0m\n",
            "\n",
            "reduce_mean_cls_loss =  0.620021284\n",
            "\n",
            "regression loss =  7.74074173\n",
            "21/41 [==============>...............] - ETA: 9:11 - loss: 12.1025 - custom_mse: 262.5145\n",
            "reduce_confident_loss =  0.794042885\n",
            "\n",
            "reduce_mean_cls_loss =  0.698834121\n",
            "\n",
            "regression loss =  8.18835926\n",
            "22/41 [===============>..............] - ETA: 8:42 - loss: 11.9925 - custom_mse: 283.5906\n",
            "reduce_confident_loss =  0.676486909\n",
            "\n",
            "reduce_mean_cls_loss =  0.689739227\n",
            "\n",
            "regression loss =  8.12380219\n",
            "23/41 [===============>..............] - ETA: 8:14 - loss: 11.8837 - custom_mse: 121.5944\n",
            "reduce_confident_loss =  0.938977778\n",
            "\n",
            "reduce_mean_cls_loss =  0.693138182\n",
            "\n",
            "regression loss =  6.0613184\n",
            "24/41 [================>.............] - ETA: 7:46 - loss: 11.7091 - custom_mse: 64.1711 \n",
            "reduce_confident_loss =  0.937685132\n",
            "\n",
            "reduce_mean_cls_loss =  0.693146527\n",
            "\n",
            "regression loss =  5.64422226\n",
            "25/41 [=================>............] - ETA: 7:18 - loss: 11.5317 - custom_mse: 85.8029\n",
            "reduce_confident_loss =  0.909175694\n",
            "\n",
            "reduce_mean_cls_loss =  0.693146169\n",
            "\n",
            "regression loss =  9.53713226\n",
            "26/41 [==================>...........] - ETA: 6:50 - loss: 11.5166 - custom_mse: 258.3891\n",
            "reduce_confident_loss =  0.825778067\n",
            "\n",
            "reduce_mean_cls_loss =  0.681570351\n",
            "\n",
            "regression loss =  6.05377769\n",
            "27/41 [==================>...........] - ETA: 6:22 - loss: 11.3701 - custom_mse: 315.8016\n",
            "reduce_confident_loss =  0.824409366\n",
            "\n",
            "reduce_mean_cls_loss =  0.682981372\n",
            "\n",
            "regression loss =  5.2235055\n",
            "28/41 [===================>..........] - ETA: 5:54 - loss: 11.2044 - custom_mse: 225.2491\n",
            "reduce_confident_loss =  0.801036358\n",
            "\n",
            "reduce_mean_cls_loss =  0.672821581\n",
            "\n",
            "regression loss =  5.76869965\n",
            "29/41 [====================>.........] - ETA: 5:27 - loss: 11.0678 - custom_mse: 317.8102\n",
            "reduce_confident_loss =  0.685874641\n",
            "\n",
            "reduce_mean_cls_loss =  0.688941658\n",
            "\n",
            "regression loss =  6.25566626\n",
            "30/41 [====================>.........] - ETA: 4:59 - loss: 10.9532 - custom_mse: 335.3578\n",
            "reduce_confident_loss =  0.683768332\n",
            "\n",
            "reduce_mean_cls_loss =  0.679185033\n",
            "\n",
            "regression loss =  7.69454098\n",
            "31/41 [=====================>........] - ETA: 4:32 - loss: 10.8921 - custom_mse: 272.7123\n",
            "reduce_confident_loss =  0.67448616\n",
            "\n",
            "reduce_mean_cls_loss =  0.828660846\n",
            "\n",
            "regression loss =  8.61652\n",
            "32/41 [======================>.......] - ETA: 4:04 - loss: 10.8680 - custom_mse: 233.9834\n",
            "reduce_confident_loss =  0.607966781\n",
            "\n",
            "reduce_mean_cls_loss =  0.806893706\n",
            "\n",
            "regression loss =  9.01400661\n",
            "33/41 [=======================>......] - ETA: 3:37 - loss: 10.8546 - custom_mse: 175.0828\n",
            "reduce_confident_loss =  0.691498876\n",
            "\n",
            "reduce_mean_cls_loss =  0.671777725\n",
            "\n",
            "regression loss =  8.67246151\n",
            "34/41 [=======================>......] - ETA: 3:10 - loss: 10.8306 - custom_mse: 305.5309\n",
            "reduce_confident_loss =  0.845993698\n",
            "\n",
            "reduce_mean_cls_loss =  0.694569468\n",
            "\n",
            "regression loss =  12.6929455\n",
            "35/41 [========================>.....] - ETA: 2:43 - loss: 10.9278 - custom_mse: 86.9751 \n",
            "reduce_confident_loss =  0.879727781\n",
            "\n",
            "reduce_mean_cls_loss =  0.711610198\n",
            "\n",
            "regression loss =  13.9760752\n",
            "36/41 [=========================>....] - ETA: 2:17 - loss: 11.0567 - custom_mse: 431.9554\n",
            "reduce_confident_loss =  0.805183113\n",
            "\n",
            "reduce_mean_cls_loss =  0.747144282\n",
            "\n",
            "regression loss =  7.16191101\n",
            "37/41 [==========================>...] - ETA: 1:49 - loss: 10.9934 - custom_mse: 390.5675\n",
            "reduce_confident_loss =  0.877347112\n",
            "\n",
            "reduce_mean_cls_loss =  0.702360034\n",
            "\n",
            "regression loss =  4.58107519\n",
            "38/41 [==========================>...] - ETA: 1:22 - loss: 10.8662 - custom_mse: 149.3500\n",
            "reduce_confident_loss =  0.807013333\n",
            "\n",
            "reduce_mean_cls_loss =  0.708330452\n",
            "\n",
            "regression loss =  13.2642794\n",
            "39/41 [===========================>..] - ETA: 54s - loss: 10.9665 - custom_mse: 291.0935 \n",
            "reduce_confident_loss =  0.806738436\n",
            "\n",
            "reduce_mean_cls_loss =  0.723419964\n",
            "\n",
            "regression loss =  14.3340912\n",
            "40/41 [============================>.] - ETA: 27s - loss: 11.0890 - custom_mse: 102.7774indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 1\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 2\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 2\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 1\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 2\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.08011109e-05 0.0362951159 0.033365041 ... 0.318026572 0.405194342 0.999879718]\n",
            " [4.4929493e-07 0.01310727 0.0197274983 ... 0.137326121 0.008469522 0.999848127]\n",
            " [4.98348e-08 0.0125028193 0.225611717 ... 0.00709140301 0.0224793255 0.99987793]\n",
            " [1.15635906e-07 0.42558676 0.0121254027 ... 0.158072114 0.221331626 0.999997377]\n",
            " [8.77810109e-08 0.0115680397 0.00245291 ... 0.601857305 0.397985846 0.999998271]]\n",
            "\n",
            "reduce_confident_loss =  0.791585088\n",
            "\n",
            "reduce_mean_cls_loss =  0.717871964\n",
            "\n",
            "regression loss =  14.3312674\n",
            "41/41 [==============================] - ETA: 0s - loss: 11.1258 - custom_mse: 79.3429  \n",
            "reduce_confident_loss =  0.682973444\n",
            "\n",
            "reduce_mean_cls_loss =  0.70565635\n",
            "\n",
            "regression loss =  12.1579342\n",
            "\n",
            "reduce_confident_loss =  0.683820307\n",
            "\n",
            "reduce_mean_cls_loss =  0.705002129\n",
            "\n",
            "regression loss =  8.23073483\n",
            "\n",
            "reduce_confident_loss =  0.90154618\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149865\n",
            "\n",
            "regression loss =  18.6771126\n",
            "\n",
            "reduce_confident_loss =  0.786912918\n",
            "\n",
            "reduce_mean_cls_loss =  0.701089561\n",
            "\n",
            "regression loss =  18.807045\n",
            "\n",
            "reduce_confident_loss =  0.899690211\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149567\n",
            "\n",
            "regression loss =  18.9918079\n",
            "\n",
            "reduce_confident_loss =  0.901455581\n",
            "\n",
            "reduce_mean_cls_loss =  0.69314605\n",
            "\n",
            "regression loss =  14.3812904\n",
            "\n",
            "reduce_confident_loss =  0.788871408\n",
            "\n",
            "reduce_mean_cls_loss =  0.701324761\n",
            "\n",
            "regression loss =  13.2709274\n",
            "\n",
            "reduce_confident_loss =  0.90159142\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149686\n",
            "\n",
            "regression loss =  21.5406265\n",
            "\n",
            "reduce_confident_loss =  0.901026249\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149626\n",
            "\n",
            "regression loss =  26.5299988\n",
            "\n",
            "reduce_confident_loss =  0.900854647\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149686\n",
            "\n",
            "regression loss =  20.9802036\n",
            "\n",
            "reduce_confident_loss =  0.900586128\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149686\n",
            "\n",
            "regression loss =  21.5371265\n",
            "\n",
            "reduce_confident_loss =  0.773405731\n",
            "\n",
            "reduce_mean_cls_loss =  0.702114582\n",
            "\n",
            "regression loss =  14.4124012\n",
            "\n",
            "reduce_confident_loss =  0.698383\n",
            "\n",
            "reduce_mean_cls_loss =  0.694545209\n",
            "\n",
            "regression loss =  12.7520065\n",
            "\n",
            "reduce_confident_loss =  0.898714721\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149567\n",
            "\n",
            "regression loss =  13.086606\n",
            "\n",
            "reduce_confident_loss =  0.899677396\n",
            "\n",
            "reduce_mean_cls_loss =  0.693144679\n",
            "\n",
            "regression loss =  18.1577568\n",
            "41/41 [==============================] - 1269s 31s/step - loss: 11.1258 - custom_mse: 79.3429 - val_loss: 18.4097 - val_custom_mse: 168.7872\n",
            "Epoch 4/10\n",
            "\n",
            "reduce_confident_loss =  0.841188\n",
            "\n",
            "reduce_mean_cls_loss =  0.705782413\n",
            "\n",
            "regression loss =  19.5145264\n",
            " 1/41 [..............................] - ETA: 18:38 - loss: 21.0615 - custom_mse: 468.4472\n",
            "reduce_confident_loss =  0.668686688\n",
            "\n",
            "reduce_mean_cls_loss =  0.683081269\n",
            "\n",
            "regression loss =  12.4615574\n",
            " 2/41 [>.............................] - ETA: 15:47 - loss: 17.4374 - custom_mse: 507.1157\n",
            "reduce_confident_loss =  0.920945466\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147242\n",
            "\n",
            "regression loss =  10.8439035\n",
            " 3/41 [=>............................] - ETA: 15:11 - loss: 15.7776 - custom_mse: 515.9222\n",
            "reduce_confident_loss =  0.731304765\n",
            "\n",
            "reduce_mean_cls_loss =  0.719217956\n",
            "\n",
            "regression loss =  9.89325809\n",
            " 4/41 [=>............................] - ETA: 14:43 - loss: 14.6692 - custom_mse: 276.9424\n",
            "reduce_confident_loss =  0.716802478\n",
            "\n",
            "reduce_mean_cls_loss =  0.695457339\n",
            "\n",
            "regression loss =  12.3296604\n",
            " 5/41 [==>...........................] - ETA: 14:19 - loss: 14.4837 - custom_mse: 163.7041\n",
            "reduce_confident_loss =  0.903415084\n",
            "\n",
            "reduce_mean_cls_loss =  0.693146825\n",
            "\n",
            "regression loss =  15.2646332\n",
            " 6/41 [===>..........................] - ETA: 14:11 - loss: 14.8800 - custom_mse: 87.9444 Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "\n",
            "reduce_confident_loss =  0.909871936\n",
            "\n",
            "reduce_mean_cls_loss =  0.693145275\n",
            "\n",
            "regression loss =  13.6353054\n",
            " 7/41 [====>.........................] - ETA: 14:41 - loss: 14.9311 - custom_mse: 134.5798\n",
            "reduce_confident_loss =  0.891237438\n",
            "\n",
            "reduce_mean_cls_loss =  0.693148553\n",
            "\n",
            "regression loss =  15.7709417\n",
            " 8/41 [====>.........................] - ETA: 14:17 - loss: 15.2342 - custom_mse: 331.0312\n",
            "reduce_confident_loss =  0.650337636\n",
            "\n",
            "reduce_mean_cls_loss =  0.739647031\n",
            "\n",
            "regression loss =  9.647089\n",
            " 9/41 [=====>........................] - ETA: 13:52 - loss: 14.7678 - custom_mse: 203.4150\n",
            "reduce_confident_loss =  0.872117043\n",
            "\n",
            "reduce_mean_cls_loss =  0.693154871\n",
            "\n",
            "regression loss =  11.8984146\n",
            "10/41 [======>.......................] - ETA: 13:27 - loss: 14.6374 - custom_mse: 239.8502\n",
            "reduce_confident_loss =  0.880150497\n",
            "\n",
            "reduce_mean_cls_loss =  0.693177521\n",
            "\n",
            "regression loss =  8.52984619\n",
            "11/41 [=======>......................] - ETA: 13:03 - loss: 14.2252 - custom_mse: 182.8324\n",
            "reduce_confident_loss =  0.882344604\n",
            "\n",
            "reduce_mean_cls_loss =  0.69315052\n",
            "\n",
            "regression loss =  5.93576288\n",
            "12/41 [=======>......................] - ETA: 12:39 - loss: 13.6657 - custom_mse: 134.3050\n",
            "reduce_confident_loss =  0.878722191\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147719\n",
            "\n",
            "regression loss =  3.20645452\n",
            "13/41 [========>.....................] - ETA: 12:14 - loss: 12.9821 - custom_mse: 126.5712\n",
            "reduce_confident_loss =  0.903742254\n",
            "\n",
            "reduce_mean_cls_loss =  0.693150103\n",
            "\n",
            "regression loss =  3.94085789\n",
            "14/41 [=========>....................] - ETA: 11:48 - loss: 12.4503 - custom_mse: 96.1391 \n",
            "reduce_confident_loss =  0.877425551\n",
            "\n",
            "reduce_mean_cls_loss =  0.69755137\n",
            "\n",
            "regression loss =  12.9907198\n",
            "15/41 [=========>....................] - ETA: 11:23 - loss: 12.5914 - custom_mse: 259.8044\n",
            "reduce_confident_loss =  0.779030681\n",
            "\n",
            "reduce_mean_cls_loss =  0.711031079\n",
            "\n",
            "regression loss =  14.0161219\n",
            "16/41 [==========>...................] - ETA: 10:57 - loss: 12.7735 - custom_mse: 249.3353\n",
            "reduce_confident_loss =  0.720480263\n",
            "\n",
            "reduce_mean_cls_loss =  0.717249036\n",
            "\n",
            "regression loss =  11.6234388\n",
            "17/41 [===========>..................] - ETA: 11:11 - loss: 12.7905 - custom_mse: 324.7729\n",
            "reduce_confident_loss =  0.878843188\n",
            "\n",
            "reduce_mean_cls_loss =  0.693145454\n",
            "\n",
            "regression loss =  10.3724794\n",
            "18/41 [============>.................] - ETA: 10:41 - loss: 12.7435 - custom_mse: 380.0417\n",
            "reduce_confident_loss =  0.740332782\n",
            "\n",
            "reduce_mean_cls_loss =  0.651201308\n",
            "\n",
            "regression loss =  8.33476162\n",
            "19/41 [============>.................] - ETA: 10:11 - loss: 12.5847 - custom_mse: 358.7900\n",
            "reduce_confident_loss =  0.78260082\n",
            "\n",
            "reduce_mean_cls_loss =  0.670888901\n",
            "\n",
            "regression loss =  12.4538698\n",
            "20/41 [=============>................] - ETA: 9:42 - loss: 12.6508 - custom_mse: 251.6024 \n",
            "reduce_confident_loss =  0.621072352\n",
            "\n",
            "reduce_mean_cls_loss =  0.606811047\n",
            "\n",
            "regression loss =  9.56267262\n",
            "21/41 [==============>...............] - ETA: 9:13 - loss: 12.5622 - custom_mse: 291.2404\n",
            "reduce_confident_loss =  0.71936065\n",
            "\n",
            "reduce_mean_cls_loss =  0.711658061\n",
            "\n",
            "regression loss =  7.72938919\n",
            "22/41 [===============>..............] - ETA: 8:45 - loss: 12.4076 - custom_mse: 282.5023\n",
            "reduce_confident_loss =  0.637432277\n",
            "\n",
            "reduce_mean_cls_loss =  0.681612074\n",
            "\n",
            "regression loss =  10.3173084\n",
            "23/41 [===============>..............] - ETA: 8:16 - loss: 12.3740 - custom_mse: 119.1467\n",
            "reduce_confident_loss =  0.870025098\n",
            "\n",
            "reduce_mean_cls_loss =  0.693138659\n",
            "\n",
            "regression loss =  8.25554657\n",
            "24/41 [================>.............] - ETA: 7:48 - loss: 12.2676 - custom_mse: 35.5234 \n",
            "reduce_confident_loss =  0.891008675\n",
            "\n",
            "reduce_mean_cls_loss =  0.69309485\n",
            "\n",
            "regression loss =  6.14387846\n",
            "25/41 [=================>............] - ETA: 7:20 - loss: 12.0860 - custom_mse: 20.1313\n",
            "reduce_confident_loss =  0.844920039\n",
            "\n",
            "reduce_mean_cls_loss =  0.693142712\n",
            "\n",
            "regression loss =  10.6643314\n",
            "26/41 [==================>...........] - ETA: 6:52 - loss: 12.0905 - custom_mse: 190.9980\n",
            "reduce_confident_loss =  0.731699824\n",
            "\n",
            "reduce_mean_cls_loss =  0.652165413\n",
            "\n",
            "regression loss =  7.94435549\n",
            "27/41 [==================>...........] - ETA: 6:24 - loss: 11.9882 - custom_mse: 245.6103\n",
            "reduce_confident_loss =  0.684867084\n",
            "\n",
            "reduce_mean_cls_loss =  0.682593465\n",
            "\n",
            "regression loss =  8.14643288\n",
            "28/41 [===================>..........] - ETA: 5:56 - loss: 11.8998 - custom_mse: 159.8399\n",
            "reduce_confident_loss =  0.709064662\n",
            "\n",
            "reduce_mean_cls_loss =  0.670284927\n",
            "\n",
            "regression loss =  10.2853994\n",
            "29/41 [====================>.........] - ETA: 5:29 - loss: 11.8917 - custom_mse: 260.5769\n",
            "reduce_confident_loss =  0.643599\n",
            "\n",
            "reduce_mean_cls_loss =  0.677587867\n",
            "\n",
            "regression loss =  9.8338871\n",
            "30/41 [====================>.........] - ETA: 5:01 - loss: 11.8671 - custom_mse: 316.8690\n",
            "reduce_confident_loss =  0.66593045\n",
            "\n",
            "reduce_mean_cls_loss =  0.687100291\n",
            "\n",
            "regression loss =  8.71897507\n",
            "31/41 [=====================>........] - ETA: 4:33 - loss: 11.8092 - custom_mse: 279.9709\n",
            "reduce_confident_loss =  0.662560165\n",
            "\n",
            "reduce_mean_cls_loss =  0.776084602\n",
            "\n",
            "regression loss =  8.04837704\n",
            "32/41 [======================>.......] - ETA: 4:06 - loss: 11.7367 - custom_mse: 301.4338\n",
            "reduce_confident_loss =  0.634633064\n",
            "\n",
            "reduce_mean_cls_loss =  0.733563125\n",
            "\n",
            "regression loss =  10.147974\n",
            "33/41 [=======================>......] - ETA: 3:38 - loss: 11.7300 - custom_mse: 172.5996\n",
            "reduce_confident_loss =  0.646156788\n",
            "\n",
            "reduce_mean_cls_loss =  0.685155809\n",
            "\n",
            "regression loss =  9.65637\n",
            "34/41 [=======================>......] - ETA: 3:11 - loss: 11.7081 - custom_mse: 283.6146\n",
            "reduce_confident_loss =  0.804019272\n",
            "\n",
            "reduce_mean_cls_loss =  0.691809773\n",
            "\n",
            "regression loss =  10.4524565\n",
            "35/41 [========================>.....] - ETA: 2:44 - loss: 11.7150 - custom_mse: 90.8837 \n",
            "reduce_confident_loss =  0.815326154\n",
            "\n",
            "reduce_mean_cls_loss =  0.703522384\n",
            "\n",
            "regression loss =  12.4623022\n",
            "36/41 [=========================>....] - ETA: 2:17 - loss: 11.7780 - custom_mse: 397.6632\n",
            "reduce_confident_loss =  0.74463588\n",
            "\n",
            "reduce_mean_cls_loss =  0.718944252\n",
            "\n",
            "regression loss =  12.4224272\n",
            "37/41 [==========================>...] - ETA: 1:50 - loss: 11.8349 - custom_mse: 367.4103\n",
            "reduce_confident_loss =  0.858938336\n",
            "\n",
            "reduce_mean_cls_loss =  0.693932533\n",
            "\n",
            "regression loss =  7.65466452\n",
            "38/41 [==========================>...] - ETA: 1:22 - loss: 11.7658 - custom_mse: 71.8774 \n",
            "reduce_confident_loss =  0.707971215\n",
            "\n",
            "reduce_mean_cls_loss =  0.704984367\n",
            "\n",
            "regression loss =  9.98187447\n",
            "39/41 [===========================>..] - ETA: 54s - loss: 11.7563 - custom_mse: 219.3672\n",
            "reduce_confident_loss =  0.707625\n",
            "\n",
            "reduce_mean_cls_loss =  0.739130557\n",
            "\n",
            "regression loss =  10.773881\n",
            "40/41 [============================>.] - ETA: 27s - loss: 11.7679 - custom_mse: 90.6394 indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 4\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.83958382e-05 0.231625438 9.94491e-06 ... 0.00987416506 0.271327019 0.999998]\n",
            " [4.52667251e-07 0.0262576342 1.65430374e-05 ... 0.0244325101 0.0165808201 0.999990225]\n",
            " [4.91374578e-07 0.106522709 1.25865688e-06 ... 0.0159739554 0.0319986045 0.999989688]\n",
            " [1.5908252e-05 0.0847464502 0.00023624301 ... 0.501609504 0.196951747 0.999980569]\n",
            " [3.27117561e-07 0.0466060936 6.94015034e-06 ... 0.0338687 0.611433864 0.999987185]]\n",
            "\n",
            "reduce_confident_loss =  0.758695483\n",
            "\n",
            "reduce_mean_cls_loss =  0.722921789\n",
            "\n",
            "regression loss =  12.2261581\n",
            "41/41 [==============================] - ETA: 0s - loss: 11.7829 - custom_mse: 90.6314 \n",
            "reduce_confident_loss =  0.686502457\n",
            "\n",
            "reduce_mean_cls_loss =  0.69846046\n",
            "\n",
            "regression loss =  13.6647701\n",
            "\n",
            "reduce_confident_loss =  0.687266231\n",
            "\n",
            "reduce_mean_cls_loss =  0.698047638\n",
            "\n",
            "regression loss =  11.3779669\n",
            "\n",
            "reduce_confident_loss =  0.849403501\n",
            "\n",
            "reduce_mean_cls_loss =  0.69315064\n",
            "\n",
            "regression loss =  17.1812096\n",
            "\n",
            "reduce_confident_loss =  0.716785729\n",
            "\n",
            "reduce_mean_cls_loss =  0.699116647\n",
            "\n",
            "regression loss =  15.0982428\n",
            "\n",
            "reduce_confident_loss =  0.846608579\n",
            "\n",
            "reduce_mean_cls_loss =  0.69315\n",
            "\n",
            "regression loss =  22.0370789\n",
            "\n",
            "reduce_confident_loss =  0.848866761\n",
            "\n",
            "reduce_mean_cls_loss =  0.693145871\n",
            "\n",
            "regression loss =  15.8705339\n",
            "\n",
            "reduce_confident_loss =  0.718835\n",
            "\n",
            "reduce_mean_cls_loss =  0.698936462\n",
            "\n",
            "regression loss =  18.2827339\n",
            "\n",
            "reduce_confident_loss =  0.849845767\n",
            "\n",
            "reduce_mean_cls_loss =  0.693150222\n",
            "\n",
            "regression loss =  24.575182\n",
            "\n",
            "reduce_confident_loss =  0.848648489\n",
            "\n",
            "reduce_mean_cls_loss =  0.693150222\n",
            "\n",
            "regression loss =  16.4766903\n",
            "\n",
            "reduce_confident_loss =  0.848375678\n",
            "\n",
            "reduce_mean_cls_loss =  0.693150103\n",
            "\n",
            "regression loss =  16.8382988\n",
            "\n",
            "reduce_confident_loss =  0.848480523\n",
            "\n",
            "reduce_mean_cls_loss =  0.693150222\n",
            "\n",
            "regression loss =  17.8078461\n",
            "\n",
            "reduce_confident_loss =  0.712962031\n",
            "\n",
            "reduce_mean_cls_loss =  0.699493408\n",
            "\n",
            "regression loss =  17.9595375\n",
            "\n",
            "reduce_confident_loss =  0.693810344\n",
            "\n",
            "reduce_mean_cls_loss =  0.690356255\n",
            "\n",
            "regression loss =  12.5450106\n",
            "\n",
            "reduce_confident_loss =  0.84577322\n",
            "\n",
            "reduce_mean_cls_loss =  0.69315\n",
            "\n",
            "regression loss =  12.9531937\n",
            "\n",
            "reduce_confident_loss =  0.84583056\n",
            "\n",
            "reduce_mean_cls_loss =  0.6931445\n",
            "\n",
            "regression loss =  22.4486351\n",
            "41/41 [==============================] - 1276s 31s/step - loss: 11.7829 - custom_mse: 90.6314 - val_loss: 18.3994 - val_custom_mse: 273.8244\n",
            "Epoch 5/10\n",
            "\n",
            "reduce_confident_loss =  0.775705397\n",
            "\n",
            "reduce_mean_cls_loss =  0.703943372\n",
            "\n",
            "regression loss =  21.665329\n",
            " 1/41 [..............................] - ETA: 18:15 - loss: 23.1450 - custom_mse: 517.1333\n",
            "reduce_confident_loss =  0.670092165\n",
            "\n",
            "reduce_mean_cls_loss =  0.672516048\n",
            "\n",
            "regression loss =  11.9497309\n",
            " 2/41 [>.............................] - ETA: 15:49 - loss: 18.2187 - custom_mse: 554.4850\n",
            "reduce_confident_loss =  0.839851916\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147302\n",
            "\n",
            "regression loss =  10.1422825\n",
            " 3/41 [=>............................] - ETA: 15:15 - loss: 16.0375 - custom_mse: 618.0663\n",
            "reduce_confident_loss =  0.694799304\n",
            "\n",
            "reduce_mean_cls_loss =  0.705823064\n",
            "\n",
            "regression loss =  8.98702717\n",
            " 4/41 [=>............................] - ETA: 14:50 - loss: 14.6251 - custom_mse: 301.6625\n",
            "reduce_confident_loss =  0.686907887\n",
            "\n",
            "reduce_mean_cls_loss =  0.694374382\n",
            "\n",
            "regression loss =  14.5415249\n",
            " 5/41 [==>...........................] - ETA: 14:25 - loss: 14.8846 - custom_mse: 258.0461\n",
            "reduce_confident_loss =  0.86329633\n",
            "\n",
            "reduce_mean_cls_loss =  0.69314605\n",
            "\n",
            "regression loss =  16.7255192\n",
            " 6/41 [===>..........................] - ETA: 14:14 - loss: 15.4508 - custom_mse: 161.7826Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "\n",
            "reduce_confident_loss =  0.887626946\n",
            "\n",
            "reduce_mean_cls_loss =  0.693143666\n",
            "\n",
            "regression loss =  12.182971\n",
            " 7/41 [====>.........................] - ETA: 14:42 - loss: 15.2098 - custom_mse: 147.8934\n",
            "reduce_confident_loss =  0.888510227\n",
            "\n",
            "reduce_mean_cls_loss =  0.693159103\n",
            "\n",
            "regression loss =  11.8019514\n",
            " 8/41 [====>.........................] - ETA: 14:18 - loss: 14.9815 - custom_mse: 222.0278\n",
            "reduce_confident_loss =  0.633646309\n",
            "\n",
            "reduce_mean_cls_loss =  0.690348089\n",
            "\n",
            "regression loss =  9.80742168\n",
            " 9/41 [=====>........................] - ETA: 13:55 - loss: 14.5538 - custom_mse: 274.1870\n",
            "reduce_confident_loss =  0.886570156\n",
            "\n",
            "reduce_mean_cls_loss =  0.693148077\n",
            "\n",
            "regression loss =  8.57516384\n",
            "10/41 [======>.......................] - ETA: 13:31 - loss: 14.1139 - custom_mse: 76.2114 \n",
            "reduce_confident_loss =  0.873279691\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147182\n",
            "\n",
            "regression loss =  5.88063622\n",
            "11/41 [=======>......................] - ETA: 13:08 - loss: 13.5078 - custom_mse: 51.3671\n",
            "reduce_confident_loss =  0.887525678\n",
            "\n",
            "reduce_mean_cls_loss =  0.693148196\n",
            "\n",
            "regression loss =  5.48065758\n",
            "12/41 [=======>......................] - ETA: 12:42 - loss: 12.9706 - custom_mse: 39.3725\n",
            "reduce_confident_loss =  0.883464217\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147361\n",
            "\n",
            "regression loss =  5.5869236\n",
            "13/41 [========>.....................] - ETA: 12:17 - loss: 12.5239 - custom_mse: 34.6086\n",
            "reduce_confident_loss =  0.876371384\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147957\n",
            "\n",
            "regression loss =  4.82431316\n",
            "14/41 [=========>....................] - ETA: 11:51 - loss: 12.0860 - custom_mse: 39.2322\n",
            "reduce_confident_loss =  0.863727868\n",
            "\n",
            "reduce_mean_cls_loss =  0.693486631\n",
            "\n",
            "regression loss =  11.9320879\n",
            "15/41 [=========>....................] - ETA: 11:25 - loss: 12.1796 - custom_mse: 251.6309\n",
            "reduce_confident_loss =  0.778445303\n",
            "\n",
            "reduce_mean_cls_loss =  0.755335152\n",
            "\n",
            "regression loss =  11.6157169\n",
            "16/41 [==========>...................] - ETA: 10:59 - loss: 12.2402 - custom_mse: 232.7869\n",
            "reduce_confident_loss =  0.721866071\n",
            "\n",
            "reduce_mean_cls_loss =  0.730039954\n",
            "\n",
            "regression loss =  15.5126457\n",
            "17/41 [===========>..................] - ETA: 11:12 - loss: 12.5181 - custom_mse: 292.1329\n",
            "reduce_confident_loss =  0.91442287\n",
            "\n",
            "reduce_mean_cls_loss =  0.693146706\n",
            "\n",
            "regression loss =  15.3334446\n",
            "18/41 [============>.................] - ETA: 10:42 - loss: 12.7638 - custom_mse: 322.2144\n",
            "reduce_confident_loss =  0.739607155\n",
            "\n",
            "reduce_mean_cls_loss =  0.636136889\n",
            "\n",
            "regression loss =  7.78189945\n",
            "19/41 [============>.................] - ETA: 10:13 - loss: 12.5740 - custom_mse: 237.9516\n",
            "reduce_confident_loss =  0.75689888\n",
            "\n",
            "reduce_mean_cls_loss =  0.6689502\n",
            "\n",
            "regression loss =  12.7225046\n",
            "20/41 [=============>................] - ETA: 9:44 - loss: 12.6527 - custom_mse: 243.3042 \n",
            "reduce_confident_loss =  0.649586797\n",
            "\n",
            "reduce_mean_cls_loss =  0.598288059\n",
            "\n",
            "regression loss =  8.24102783\n",
            "21/41 [==============>...............] - ETA: 9:15 - loss: 12.5021 - custom_mse: 248.4224\n",
            "reduce_confident_loss =  0.728796542\n",
            "\n",
            "reduce_mean_cls_loss =  0.717298806\n",
            "\n",
            "regression loss =  10.6182718\n",
            "22/41 [===============>..............] - ETA: 8:47 - loss: 12.4822 - custom_mse: 275.2864\n",
            "reduce_confident_loss =  0.673414767\n",
            "\n",
            "reduce_mean_cls_loss =  0.686311305\n",
            "\n",
            "regression loss =  9.43983555\n",
            "23/41 [===============>..............] - ETA: 8:18 - loss: 12.4090 - custom_mse: 98.5066 \n",
            "reduce_confident_loss =  0.897963345\n",
            "\n",
            "reduce_mean_cls_loss =  0.693145096\n",
            "\n",
            "regression loss =  8.02728176\n",
            "24/41 [================>.............] - ETA: 7:50 - loss: 12.2927 - custom_mse: 31.5714\n",
            "reduce_confident_loss =  0.895599961\n",
            "\n",
            "reduce_mean_cls_loss =  0.693146646\n",
            "\n",
            "regression loss =  7.984447\n",
            "25/41 [=================>............] - ETA: 7:21 - loss: 12.1840 - custom_mse: 35.9737\n",
            "reduce_confident_loss =  0.851480961\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147123\n",
            "\n",
            "regression loss =  12.167099\n",
            "26/41 [==================>...........] - ETA: 6:53 - loss: 12.2427 - custom_mse: 223.2937\n",
            "reduce_confident_loss =  0.748024404\n",
            "\n",
            "reduce_mean_cls_loss =  0.689680755\n",
            "\n",
            "regression loss =  7.51890945\n",
            "27/41 [==================>...........] - ETA: 6:25 - loss: 12.1210 - custom_mse: 297.5498\n",
            "reduce_confident_loss =  0.726058304\n",
            "\n",
            "reduce_mean_cls_loss =  0.652333558\n",
            "\n",
            "regression loss =  3.68528438\n",
            "28/41 [===================>..........] - ETA: 5:57 - loss: 11.8690 - custom_mse: 207.3011\n",
            "reduce_confident_loss =  0.735965073\n",
            "\n",
            "reduce_mean_cls_loss =  0.665550888\n",
            "\n",
            "regression loss =  5.1869626\n",
            "29/41 [====================>.........] - ETA: 5:29 - loss: 11.6869 - custom_mse: 285.7336\n",
            "reduce_confident_loss =  0.658081055\n",
            "\n",
            "reduce_mean_cls_loss =  0.658438802\n",
            "\n",
            "regression loss =  6.25153971\n",
            "30/41 [====================>.........] - ETA: 5:02 - loss: 11.5496 - custom_mse: 303.1011\n",
            "reduce_confident_loss =  0.662410796\n",
            "\n",
            "reduce_mean_cls_loss =  0.681798518\n",
            "\n",
            "regression loss =  6.27260637\n",
            "31/41 [=====================>........] - ETA: 4:34 - loss: 11.4227 - custom_mse: 265.1805\n",
            "reduce_confident_loss =  0.630701482\n",
            "\n",
            "reduce_mean_cls_loss =  0.79021889\n",
            "\n",
            "regression loss =  6.00425\n",
            "32/41 [======================>.......] - ETA: 4:07 - loss: 11.2978 - custom_mse: 249.9184\n",
            "reduce_confident_loss =  0.58871156\n",
            "\n",
            "reduce_mean_cls_loss =  0.776078105\n",
            "\n",
            "regression loss =  8.04496\n",
            "33/41 [=======================>......] - ETA: 3:39 - loss: 11.2406 - custom_mse: 165.1776\n",
            "reduce_confident_loss =  0.642764449\n",
            "\n",
            "reduce_mean_cls_loss =  0.655051112\n",
            "\n",
            "regression loss =  6.93610525\n",
            "34/41 [=======================>......] - ETA: 3:11 - loss: 11.1522 - custom_mse: 278.0779\n",
            "reduce_confident_loss =  0.807776451\n",
            "\n",
            "reduce_mean_cls_loss =  0.687944412\n",
            "\n",
            "regression loss =  6.70272827\n",
            "35/41 [========================>.....] - ETA: 2:45 - loss: 11.0678 - custom_mse: 52.0871 \n",
            "reduce_confident_loss =  0.800821364\n",
            "\n",
            "reduce_mean_cls_loss =  0.727284849\n",
            "\n",
            "regression loss =  13.2036572\n",
            "36/41 [=========================>....] - ETA: 2:18 - loss: 11.1695 - custom_mse: 423.2751\n",
            "reduce_confident_loss =  0.722318232\n",
            "\n",
            "reduce_mean_cls_loss =  0.77798456\n",
            "\n",
            "regression loss =  11.0050478\n",
            "37/41 [==========================>...] - ETA: 1:50 - loss: 11.2056 - custom_mse: 427.7300\n",
            "reduce_confident_loss =  0.800990462\n",
            "\n",
            "reduce_mean_cls_loss =  0.701030612\n",
            "\n",
            "regression loss =  7.64825869\n",
            "38/41 [==========================>...] - ETA: 1:22 - loss: 11.1516 - custom_mse: 80.4663 \n",
            "reduce_confident_loss =  0.663908303\n",
            "\n",
            "reduce_mean_cls_loss =  0.698546\n",
            "\n",
            "regression loss =  11.9613504\n",
            "39/41 [===========================>..] - ETA: 55s - loss: 11.2073 - custom_mse: 225.5443\n",
            "reduce_confident_loss =  0.705923378\n",
            "\n",
            "reduce_mean_cls_loss =  0.715423465\n",
            "\n",
            "regression loss =  13.2700424\n",
            "40/41 [============================>.] - ETA: 27s - loss: 11.2944 - custom_mse: 101.4076indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 3\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[5.76486264e-06 0.391795129 6.44708598e-06 ... 0.00225016475 0.0473392 0.999999404]\n",
            " [0.000160366297 0.0400238931 0.00036585331 ... 0.0113180578 0.0751214 0.999999762]\n",
            " [6.31199555e-06 0.151525587 1.74592042e-05 ... 0.00387811661 0.0201675892 0.99994266]\n",
            " [4.24078394e-07 0.000104259489 1.20644404e-06 ... 0.012748301 0.00693327188 0.999998093]\n",
            " [6.74963516e-08 0.0112133324 5.15293868e-06 ... 0.0105594397 0.0868605673 0.999999762]]\n",
            "\n",
            "reduce_confident_loss =  0.72686547\n",
            "\n",
            "reduce_mean_cls_loss =  0.731410563\n",
            "\n",
            "regression loss =  12.1687\n",
            "41/41 [==============================] - ETA: 0s - loss: 11.3124 - custom_mse: 85.5527  \n",
            "reduce_confident_loss =  0.700531125\n",
            "\n",
            "reduce_mean_cls_loss =  0.717199266\n",
            "\n",
            "regression loss =  12.6997423\n",
            "\n",
            "reduce_confident_loss =  0.700616956\n",
            "\n",
            "reduce_mean_cls_loss =  0.718264043\n",
            "\n",
            "regression loss =  8.58451366\n",
            "\n",
            "reduce_confident_loss =  0.839004695\n",
            "\n",
            "reduce_mean_cls_loss =  0.693528056\n",
            "\n",
            "regression loss =  16.7331734\n",
            "\n",
            "reduce_confident_loss =  0.729192436\n",
            "\n",
            "reduce_mean_cls_loss =  0.711545527\n",
            "\n",
            "regression loss =  12.8744431\n",
            "\n",
            "reduce_confident_loss =  0.833462238\n",
            "\n",
            "reduce_mean_cls_loss =  0.693451464\n",
            "\n",
            "regression loss =  18.7838173\n",
            "\n",
            "reduce_confident_loss =  0.838264763\n",
            "\n",
            "reduce_mean_cls_loss =  0.693013191\n",
            "\n",
            "regression loss =  11.9085493\n",
            "\n",
            "reduce_confident_loss =  0.732340336\n",
            "\n",
            "reduce_mean_cls_loss =  0.711956441\n",
            "\n",
            "regression loss =  13.7624426\n",
            "\n",
            "reduce_confident_loss =  0.837462425\n",
            "\n",
            "reduce_mean_cls_loss =  0.693523228\n",
            "\n",
            "regression loss =  19.8647614\n",
            "\n",
            "reduce_confident_loss =  0.835829735\n",
            "\n",
            "reduce_mean_cls_loss =  0.693547547\n",
            "\n",
            "regression loss =  14.762495\n",
            "\n",
            "reduce_confident_loss =  0.836417496\n",
            "\n",
            "reduce_mean_cls_loss =  0.693526685\n",
            "\n",
            "regression loss =  15.520443\n",
            "\n",
            "reduce_confident_loss =  0.83412087\n",
            "\n",
            "reduce_mean_cls_loss =  0.693522036\n",
            "\n",
            "regression loss =  18.3779793\n",
            "\n",
            "reduce_confident_loss =  0.724312\n",
            "\n",
            "reduce_mean_cls_loss =  0.716856\n",
            "\n",
            "regression loss =  16.6431065\n",
            "\n",
            "reduce_confident_loss =  0.706618786\n",
            "\n",
            "reduce_mean_cls_loss =  0.691153705\n",
            "\n",
            "regression loss =  12.6504822\n",
            "\n",
            "reduce_confident_loss =  0.832007408\n",
            "\n",
            "reduce_mean_cls_loss =  0.693518639\n",
            "\n",
            "regression loss =  10.5176973\n",
            "\n",
            "reduce_confident_loss =  0.831597745\n",
            "\n",
            "reduce_mean_cls_loss =  0.692829967\n",
            "\n",
            "regression loss =  17.183033\n",
            "41/41 [==============================] - 1277s 31s/step - loss: 11.3124 - custom_mse: 85.5527 - val_loss: 16.1701 - val_custom_mse: 152.0140\n",
            "Epoch 6/10\n",
            "\n",
            "reduce_confident_loss =  0.734972537\n",
            "\n",
            "reduce_mean_cls_loss =  0.702967763\n",
            "\n",
            "regression loss =  19.4017639\n",
            " 1/41 [..............................] - ETA: 17:37 - loss: 20.8397 - custom_mse: 571.9086\n",
            "reduce_confident_loss =  0.643100798\n",
            "\n",
            "reduce_mean_cls_loss =  0.646111\n",
            "\n",
            "regression loss =  10.095993\n",
            " 2/41 [>.............................] - ETA: 15:37 - loss: 16.1125 - custom_mse: 631.4590\n",
            "reduce_confident_loss =  0.83780849\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147242\n",
            "\n",
            "regression loss =  7.63917351\n",
            " 3/41 [=>............................] - ETA: 15:08 - loss: 13.7983 - custom_mse: 726.0886\n",
            "reduce_confident_loss =  0.666215062\n",
            "\n",
            "reduce_mean_cls_loss =  0.706369698\n",
            "\n",
            "regression loss =  7.70813513\n",
            " 4/41 [=>............................] - ETA: 14:41 - loss: 12.6189 - custom_mse: 289.6745\n",
            "reduce_confident_loss =  0.635847092\n",
            "\n",
            "reduce_mean_cls_loss =  0.698990047\n",
            "\n",
            "regression loss =  11.107132\n",
            " 5/41 [==>...........................] - ETA: 14:21 - loss: 12.5835 - custom_mse: 241.5435\n",
            "reduce_confident_loss =  0.797819316\n",
            "\n",
            "reduce_mean_cls_loss =  0.693084538\n",
            "\n",
            "regression loss =  9.02684402\n",
            " 6/41 [===>..........................] - ETA: 14:09 - loss: 12.2392 - custom_mse: 99.5225 Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "\n",
            "reduce_confident_loss =  0.787854075\n",
            "\n",
            "reduce_mean_cls_loss =  0.693114281\n",
            "\n",
            "regression loss =  15.2815218\n",
            " 7/41 [====>.........................] - ETA: 14:35 - loss: 12.8854 - custom_mse: 170.1925\n",
            "reduce_confident_loss =  0.871519923\n",
            "\n",
            "reduce_mean_cls_loss =  0.693156242\n",
            "\n",
            "regression loss =  14.0092354\n",
            " 8/41 [====>.........................] - ETA: 14:13 - loss: 13.2215 - custom_mse: 250.4877\n",
            "reduce_confident_loss =  0.617563128\n",
            "\n",
            "reduce_mean_cls_loss =  0.684484184\n",
            "\n",
            "regression loss =  8.96388435\n",
            " 9/41 [=====>........................] - ETA: 13:49 - loss: 12.8931 - custom_mse: 230.8206\n",
            "reduce_confident_loss =  0.85936451\n",
            "\n",
            "reduce_mean_cls_loss =  0.693167031\n",
            "\n",
            "regression loss =  10.5179195\n",
            "10/41 [======>.......................] - ETA: 13:24 - loss: 12.8108 - custom_mse: 160.2886\n",
            "reduce_confident_loss =  0.836791396\n",
            "\n",
            "reduce_mean_cls_loss =  0.693157732\n",
            "\n",
            "regression loss =  6.49361753\n",
            "11/41 [=======>......................] - ETA: 12:59 - loss: 12.3756 - custom_mse: 114.4738\n",
            "reduce_confident_loss =  0.866028666\n",
            "\n",
            "reduce_mean_cls_loss =  0.693158686\n",
            "\n",
            "regression loss =  4.31201839\n",
            "12/41 [=======>......................] - ETA: 12:33 - loss: 11.8336 - custom_mse: 100.1625\n",
            "reduce_confident_loss =  0.841118395\n",
            "\n",
            "reduce_mean_cls_loss =  0.693304896\n",
            "\n",
            "regression loss =  3.06550574\n",
            "13/41 [========>.....................] - ETA: 12:08 - loss: 11.2772 - custom_mse: 96.0978 \n",
            "reduce_confident_loss =  0.826986372\n",
            "\n",
            "reduce_mean_cls_loss =  0.69324863\n",
            "\n",
            "regression loss =  4.09998417\n",
            "14/41 [=========>....................] - ETA: 11:42 - loss: 10.8731 - custom_mse: 85.7534\n",
            "reduce_confident_loss =  0.835931599\n",
            "\n",
            "reduce_mean_cls_loss =  0.693904698\n",
            "\n",
            "regression loss =  12.3701\n",
            "15/41 [=========>....................] - ETA: 11:17 - loss: 11.0749 - custom_mse: 271.3829\n",
            "reduce_confident_loss =  0.726281464\n",
            "\n",
            "reduce_mean_cls_loss =  0.779976428\n",
            "\n",
            "regression loss =  12.6677094\n",
            "16/41 [==========>...................] - ETA: 10:52 - loss: 11.2686 - custom_mse: 213.4497\n",
            "reduce_confident_loss =  0.656062543\n",
            "\n",
            "reduce_mean_cls_loss =  0.70961082\n",
            "\n",
            "regression loss =  13.277215\n",
            "17/41 [===========>..................] - ETA: 11:04 - loss: 11.4671 - custom_mse: 351.0753\n",
            "reduce_confident_loss =  0.810139477\n",
            "\n",
            "reduce_mean_cls_loss =  0.693145\n",
            "\n",
            "regression loss =  15.2860889\n",
            "18/41 [============>.................] - ETA: 10:34 - loss: 11.7627 - custom_mse: 456.5225\n",
            "reduce_confident_loss =  0.61857456\n",
            "\n",
            "reduce_mean_cls_loss =  0.645536065\n",
            "\n",
            "regression loss =  7.88533878\n",
            "19/41 [============>.................] - ETA: 10:05 - loss: 11.6252 - custom_mse: 324.3199\n",
            "reduce_confident_loss =  0.695136845\n",
            "\n",
            "reduce_mean_cls_loss =  0.674634278\n",
            "\n",
            "regression loss =  12.7910967\n",
            "20/41 [=============>................] - ETA: 9:37 - loss: 11.7520 - custom_mse: 235.3082 \n",
            "reduce_confident_loss =  0.594828248\n",
            "\n",
            "reduce_mean_cls_loss =  0.642262101\n",
            "\n",
            "regression loss =  8.54219723\n",
            "21/41 [==============>...............] - ETA: 9:08 - loss: 11.6580 - custom_mse: 319.7372\n",
            "reduce_confident_loss =  0.668962777\n",
            "\n",
            "reduce_mean_cls_loss =  0.708512187\n",
            "\n",
            "regression loss =  7.55566931\n",
            "22/41 [===============>..............] - ETA: 8:40 - loss: 11.5342 - custom_mse: 274.9737\n",
            "reduce_confident_loss =  0.583211243\n",
            "\n",
            "reduce_mean_cls_loss =  0.685111463\n",
            "\n",
            "regression loss =  9.47104931\n",
            "23/41 [===============>..............] - ETA: 8:12 - loss: 11.4996 - custom_mse: 106.1850\n",
            "reduce_confident_loss =  0.802369773\n",
            "\n",
            "reduce_mean_cls_loss =  0.693116\n",
            "\n",
            "regression loss =  4.09362888\n",
            "24/41 [================>.............] - ETA: 7:43 - loss: 11.2534 - custom_mse: 11.6465 \n",
            "reduce_confident_loss =  0.802566826\n",
            "\n",
            "reduce_mean_cls_loss =  0.693116\n",
            "\n",
            "regression loss =  3.12235093\n",
            "25/41 [=================>............] - ETA: 7:15 - loss: 10.9879 - custom_mse: 5.9279 \n",
            "reduce_confident_loss =  0.761422575\n",
            "\n",
            "reduce_mean_cls_loss =  0.693118751\n",
            "\n",
            "regression loss =  8.59974289\n",
            "26/41 [==================>...........] - ETA: 6:48 - loss: 10.9520 - custom_mse: 179.4015\n",
            "reduce_confident_loss =  0.64980191\n",
            "\n",
            "reduce_mean_cls_loss =  0.67304647\n",
            "\n",
            "regression loss =  7.40589333\n",
            "27/41 [==================>...........] - ETA: 6:20 - loss: 10.8697 - custom_mse: 224.6078\n",
            "reduce_confident_loss =  0.64041239\n",
            "\n",
            "reduce_mean_cls_loss =  0.636910498\n",
            "\n",
            "regression loss =  6.80135441\n",
            "28/41 [===================>..........] - ETA: 5:52 - loss: 10.7700 - custom_mse: 139.2865\n",
            "reduce_confident_loss =  0.636241853\n",
            "\n",
            "reduce_mean_cls_loss =  0.617464483\n",
            "\n",
            "regression loss =  7.81879425\n",
            "29/41 [====================>.........] - ETA: 5:25 - loss: 10.7115 - custom_mse: 219.8820\n",
            "reduce_confident_loss =  0.605088115\n",
            "\n",
            "reduce_mean_cls_loss =  0.662094533\n",
            "\n",
            "regression loss =  8.03254414\n",
            "30/41 [====================>.........] - ETA: 4:58 - loss: 10.6644 - custom_mse: 280.7775\n",
            "reduce_confident_loss =  0.609240949\n",
            "\n",
            "reduce_mean_cls_loss =  0.638125896\n",
            "\n",
            "regression loss =  6.58799362\n",
            "31/41 [=====================>........] - ETA: 4:30 - loss: 10.5732 - custom_mse: 220.2814\n",
            "reduce_confident_loss =  0.603512645\n",
            "\n",
            "reduce_mean_cls_loss =  0.765131295\n",
            "\n",
            "regression loss =  5.88523865\n",
            "32/41 [======================>.......] - ETA: 4:03 - loss: 10.4694 - custom_mse: 183.6864\n",
            "reduce_confident_loss =  0.583954\n",
            "\n",
            "reduce_mean_cls_loss =  0.735452294\n",
            "\n",
            "regression loss =  7.56468916\n",
            "33/41 [=======================>......] - ETA: 3:36 - loss: 10.4214 - custom_mse: 125.8958\n",
            "reduce_confident_loss =  0.619138718\n",
            "\n",
            "reduce_mean_cls_loss =  0.623879313\n",
            "\n",
            "regression loss =  8.13479614\n",
            "34/41 [=======================>......] - ETA: 3:09 - loss: 10.3907 - custom_mse: 237.4190\n",
            "reduce_confident_loss =  0.739261\n",
            "\n",
            "reduce_mean_cls_loss =  0.691079259\n",
            "\n",
            "regression loss =  8.13036346\n",
            "35/41 [========================>.....] - ETA: 2:42 - loss: 10.3670 - custom_mse: 72.6287 \n",
            "reduce_confident_loss =  0.751216412\n",
            "\n",
            "reduce_mean_cls_loss =  0.700529\n",
            "\n",
            "regression loss =  12.0188704\n",
            "36/41 [=========================>....] - ETA: 2:16 - loss: 10.4532 - custom_mse: 386.7938\n",
            "reduce_confident_loss =  0.633921623\n",
            "\n",
            "reduce_mean_cls_loss =  0.727053106\n",
            "\n",
            "regression loss =  10.5355186\n",
            "37/41 [==========================>...] - ETA: 1:48 - loss: 10.4922 - custom_mse: 318.3274\n",
            "reduce_confident_loss =  0.726557791\n",
            "\n",
            "reduce_mean_cls_loss =  0.69731015\n",
            "\n",
            "regression loss =  6.52257109\n",
            "38/41 [==========================>...] - ETA: 1:21 - loss: 10.4252 - custom_mse: 65.5240 \n",
            "reduce_confident_loss =  0.632136881\n",
            "\n",
            "reduce_mean_cls_loss =  0.705360115\n",
            "\n",
            "regression loss =  7.33141375\n",
            "39/41 [===========================>..] - ETA: 54s - loss: 10.3802 - custom_mse: 248.3298\n",
            "reduce_confident_loss =  0.635030448\n",
            "\n",
            "reduce_mean_cls_loss =  0.745866239\n",
            "\n",
            "regression loss =  11.4551706\n",
            "40/41 [============================>.] - ETA: 27s - loss: 10.4416 - custom_mse: 135.8544indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[6.4051339e-05 0.418666184 7.863463e-06 ... 0.0324506462 0.105994314 0.999999762]\n",
            " [4.73955515e-06 0.349796861 1.27421454e-05 ... 0.00061711669 0.0311900675 0.999999762]\n",
            " [7.09553206e-07 0.000550925732 1.93693049e-05 ... 0.0662825704 0.069293648 0.999994397]\n",
            " [2.07010926e-05 0.033005774 2.78033758e-06 ... 0.00876256824 0.0297403932 0.999999583]\n",
            " [8.98752405e-05 0.00734815 0.000105024708 ... 0.0225412548 0.0177327096 0.999862194]]\n",
            "\n",
            "reduce_confident_loss =  0.65350616\n",
            "\n",
            "reduce_mean_cls_loss =  0.733373106\n",
            "\n",
            "regression loss =  11.1147766\n",
            "41/41 [==============================] - ETA: 0s - loss: 10.4575 - custom_mse: 120.1412 \n",
            "reduce_confident_loss =  0.687614262\n",
            "\n",
            "reduce_mean_cls_loss =  0.70733124\n",
            "\n",
            "regression loss =  13.7595005\n",
            "\n",
            "reduce_confident_loss =  0.683858037\n",
            "\n",
            "reduce_mean_cls_loss =  0.705848515\n",
            "\n",
            "regression loss =  10.7998295\n",
            "\n",
            "reduce_confident_loss =  0.812273\n",
            "\n",
            "reduce_mean_cls_loss =  0.693306267\n",
            "\n",
            "regression loss =  16.6858\n",
            "\n",
            "reduce_confident_loss =  0.709172249\n",
            "\n",
            "reduce_mean_cls_loss =  0.703316689\n",
            "\n",
            "regression loss =  12.3953791\n",
            "\n",
            "reduce_confident_loss =  0.813255429\n",
            "\n",
            "reduce_mean_cls_loss =  0.693283498\n",
            "\n",
            "regression loss =  16.705246\n",
            "\n",
            "reduce_confident_loss =  0.819565237\n",
            "\n",
            "reduce_mean_cls_loss =  0.693090498\n",
            "\n",
            "regression loss =  13.9671345\n",
            "\n",
            "reduce_confident_loss =  0.715087354\n",
            "\n",
            "reduce_mean_cls_loss =  0.70304805\n",
            "\n",
            "regression loss =  16.295023\n",
            "\n",
            "reduce_confident_loss =  0.819831312\n",
            "\n",
            "reduce_mean_cls_loss =  0.693293631\n",
            "\n",
            "regression loss =  13.3616219\n",
            "\n",
            "reduce_confident_loss =  0.818499625\n",
            "\n",
            "reduce_mean_cls_loss =  0.693294346\n",
            "\n",
            "regression loss =  12.6888762\n",
            "\n",
            "reduce_confident_loss =  0.818031609\n",
            "\n",
            "reduce_mean_cls_loss =  0.693289459\n",
            "\n",
            "regression loss =  13.2781267\n",
            "\n",
            "reduce_confident_loss =  0.807216\n",
            "\n",
            "reduce_mean_cls_loss =  0.693290174\n",
            "\n",
            "regression loss =  14.9946032\n",
            "\n",
            "reduce_confident_loss =  0.704059184\n",
            "\n",
            "reduce_mean_cls_loss =  0.706270218\n",
            "\n",
            "regression loss =  18.0394459\n",
            "\n",
            "reduce_confident_loss =  0.691282868\n",
            "\n",
            "reduce_mean_cls_loss =  0.692230761\n",
            "\n",
            "regression loss =  13.2956181\n",
            "\n",
            "reduce_confident_loss =  0.809723437\n",
            "\n",
            "reduce_mean_cls_loss =  0.693344116\n",
            "\n",
            "regression loss =  9.56529236\n",
            "\n",
            "reduce_confident_loss =  0.805536926\n",
            "\n",
            "reduce_mean_cls_loss =  0.692977786\n",
            "\n",
            "regression loss =  19.7739124\n",
            "41/41 [==============================] - 1261s 31s/step - loss: 10.4575 - custom_mse: 120.1412 - val_loss: 15.7464 - val_custom_mse: 189.5187\n",
            "Epoch 7/10\n",
            "\n",
            "reduce_confident_loss =  0.706203043\n",
            "\n",
            "reduce_mean_cls_loss =  0.70619452\n",
            "\n",
            "regression loss =  18.7333488\n",
            " 1/41 [..............................] - ETA: 17:35 - loss: 20.1457 - custom_mse: 517.1101\n",
            "reduce_confident_loss =  0.661412776\n",
            "\n",
            "reduce_mean_cls_loss =  0.672495306\n",
            "\n",
            "regression loss =  9.91282272\n",
            " 2/41 [>.............................] - ETA: 15:37 - loss: 15.6962 - custom_mse: 601.3452\n",
            "reduce_confident_loss =  0.789940178\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147242\n",
            "\n",
            "regression loss =  8.98872566\n",
            " 3/41 [=>............................] - ETA: 15:07 - loss: 13.9548 - custom_mse: 667.5199\n",
            "reduce_confident_loss =  0.659855425\n",
            "\n",
            "reduce_mean_cls_loss =  0.700290263\n",
            "\n",
            "regression loss =  9.03791523\n",
            " 4/41 [=>............................] - ETA: 14:41 - loss: 13.0656 - custom_mse: 310.0981\n",
            "reduce_confident_loss =  0.620566189\n",
            "\n",
            "reduce_mean_cls_loss =  0.693384469\n",
            "\n",
            "regression loss =  15.9976797\n",
            " 5/41 [==>...........................] - ETA: 14:17 - loss: 13.9148 - custom_mse: 246.3020\n",
            "reduce_confident_loss =  0.753912508\n",
            "\n",
            "reduce_mean_cls_loss =  0.693144917\n",
            "\n",
            "regression loss =  12.812541\n",
            " 6/41 [===>..........................] - ETA: 14:05 - loss: 13.9723 - custom_mse: 120.0341Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "\n",
            "reduce_confident_loss =  0.76730454\n",
            "\n",
            "reduce_mean_cls_loss =  0.693143249\n",
            "\n",
            "regression loss =  12.73526\n",
            " 7/41 [====>.........................] - ETA: 14:32 - loss: 14.0042 - custom_mse: 142.6533\n",
            "reduce_confident_loss =  0.818025\n",
            "\n",
            "reduce_mean_cls_loss =  0.69314754\n",
            "\n",
            "regression loss =  6.63520384\n",
            " 8/41 [====>.........................] - ETA: 14:12 - loss: 13.2720 - custom_mse: 213.8414\n",
            "reduce_confident_loss =  0.634762347\n",
            "\n",
            "reduce_mean_cls_loss =  0.689768612\n",
            "\n",
            "regression loss =  9.26177788\n",
            " 9/41 [=====>........................] - ETA: 13:48 - loss: 12.9736 - custom_mse: 257.9161\n",
            "reduce_confident_loss =  0.856826127\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149865\n",
            "\n",
            "regression loss =  5.6170826\n",
            "10/41 [======>.......................] - ETA: 13:23 - loss: 12.3929 - custom_mse: 97.3421 \n",
            "reduce_confident_loss =  0.845384777\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147361\n",
            "\n",
            "regression loss =  6.87706423\n",
            "11/41 [=======>......................] - ETA: 12:58 - loss: 12.0313 - custom_mse: 54.3037\n",
            "reduce_confident_loss =  0.859751046\n",
            "\n",
            "reduce_mean_cls_loss =  0.69314748\n",
            "\n",
            "regression loss =  7.23871\n",
            "12/41 [=======>......................] - ETA: 12:33 - loss: 11.7614 - custom_mse: 53.9588\n",
            "reduce_confident_loss =  0.854044795\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147361\n",
            "\n",
            "regression loss =  6.73951721\n",
            "13/41 [========>.....................] - ETA: 12:07 - loss: 11.4941 - custom_mse: 67.5855\n",
            "reduce_confident_loss =  0.834593296\n",
            "\n",
            "reduce_mean_cls_loss =  0.693166077\n",
            "\n",
            "regression loss =  5.53556585\n",
            "14/41 [=========>....................] - ETA: 11:42 - loss: 11.1776 - custom_mse: 83.0372\n",
            "reduce_confident_loss =  0.799782574\n",
            "\n",
            "reduce_mean_cls_loss =  0.698768198\n",
            "\n",
            "regression loss =  10.5218916\n",
            "15/41 [=========>....................] - ETA: 11:17 - loss: 11.2338 - custom_mse: 220.9243\n",
            "reduce_confident_loss =  0.706457257\n",
            "\n",
            "reduce_mean_cls_loss =  0.711929858\n",
            "\n",
            "regression loss =  10.839076\n",
            "16/41 [==========>...................] - ETA: 10:51 - loss: 11.2978 - custom_mse: 204.9649\n",
            "reduce_confident_loss =  0.63336575\n",
            "\n",
            "reduce_mean_cls_loss =  0.767479897\n",
            "\n",
            "regression loss =  12.5211058\n",
            "17/41 [===========>..................] - ETA: 11:05 - loss: 11.4521 - custom_mse: 252.3363\n",
            "reduce_confident_loss =  0.806231797\n",
            "\n",
            "reduce_mean_cls_loss =  0.6931234\n",
            "\n",
            "regression loss =  14.7887135\n",
            "18/41 [============>.................] - ETA: 10:36 - loss: 11.7208 - custom_mse: 313.6010\n",
            "reduce_confident_loss =  0.64800328\n",
            "\n",
            "reduce_mean_cls_loss =  0.589438\n",
            "\n",
            "regression loss =  6.95599318\n",
            "19/41 [============>.................] - ETA: 10:07 - loss: 11.5351 - custom_mse: 232.5455\n",
            "reduce_confident_loss =  0.676221192\n",
            "\n",
            "reduce_mean_cls_loss =  0.631139159\n",
            "\n",
            "regression loss =  11.7284708\n",
            "20/41 [=============>................] - ETA: 9:38 - loss: 11.6102 - custom_mse: 258.6368 \n",
            "reduce_confident_loss =  0.591451287\n",
            "\n",
            "reduce_mean_cls_loss =  0.589084\n",
            "\n",
            "regression loss =  7.96768522\n",
            "21/41 [==============>...............] - ETA: 9:09 - loss: 11.4929 - custom_mse: 248.2538\n",
            "reduce_confident_loss =  0.654193\n",
            "\n",
            "reduce_mean_cls_loss =  0.724156201\n",
            "\n",
            "regression loss =  9.60733795\n",
            "22/41 [===============>..............] - ETA: 8:41 - loss: 11.4699 - custom_mse: 282.6297\n",
            "reduce_confident_loss =  0.579130709\n",
            "\n",
            "reduce_mean_cls_loss =  0.645958\n",
            "\n",
            "regression loss =  9.37773132\n",
            "23/41 [===============>..............] - ETA: 8:12 - loss: 11.4322 - custom_mse: 91.7054 \n",
            "reduce_confident_loss =  0.795865595\n",
            "\n",
            "reduce_mean_cls_loss =  0.6930933\n",
            "\n",
            "regression loss =  5.78598261\n",
            "24/41 [================>.............] - ETA: 7:44 - loss: 11.2590 - custom_mse: 19.1746\n",
            "reduce_confident_loss =  0.772444606\n",
            "\n",
            "reduce_mean_cls_loss =  0.693137\n",
            "\n",
            "regression loss =  5.38694859\n",
            "25/41 [=================>............] - ETA: 7:16 - loss: 11.0827 - custom_mse: 17.0614\n",
            "reduce_confident_loss =  0.746103227\n",
            "\n",
            "reduce_mean_cls_loss =  0.693135142\n",
            "\n",
            "regression loss =  9.4901123\n",
            "26/41 [==================>...........] - ETA: 6:48 - loss: 11.0768 - custom_mse: 222.0531\n",
            "reduce_confident_loss =  0.666651785\n",
            "\n",
            "reduce_mean_cls_loss =  0.669938445\n",
            "\n",
            "regression loss =  6.07515049\n",
            "27/41 [==================>...........] - ETA: 6:20 - loss: 10.9411 - custom_mse: 292.7356\n",
            "reduce_confident_loss =  0.632621944\n",
            "\n",
            "reduce_mean_cls_loss =  0.615709\n",
            "\n",
            "regression loss =  3.95857406\n",
            "28/41 [===================>..........] - ETA: 5:53 - loss: 10.7363 - custom_mse: 209.2052\n",
            "reduce_confident_loss =  0.627714455\n",
            "\n",
            "reduce_mean_cls_loss =  0.601137221\n",
            "\n",
            "regression loss =  4.86664438\n",
            "29/41 [====================>.........] - ETA: 5:25 - loss: 10.5762 - custom_mse: 275.6392\n",
            "reduce_confident_loss =  0.586533189\n",
            "\n",
            "reduce_mean_cls_loss =  0.628978848\n",
            "\n",
            "regression loss =  5.4521184\n",
            "30/41 [====================>.........] - ETA: 4:58 - loss: 10.4460 - custom_mse: 330.8482\n",
            "reduce_confident_loss =  0.607512653\n",
            "\n",
            "reduce_mean_cls_loss =  0.628536165\n",
            "\n",
            "regression loss =  5.17135668\n",
            "31/41 [=====================>........] - ETA: 4:31 - loss: 10.3157 - custom_mse: 256.7301\n",
            "reduce_confident_loss =  0.598054588\n",
            "\n",
            "reduce_mean_cls_loss =  0.772152364\n",
            "\n",
            "regression loss =  5.21808624\n",
            "32/41 [======================>.......] - ETA: 4:03 - loss: 10.1992 - custom_mse: 261.6420\n",
            "reduce_confident_loss =  0.561619282\n",
            "\n",
            "reduce_mean_cls_loss =  0.739034772\n",
            "\n",
            "regression loss =  6.76366472\n",
            "33/41 [=======================>......] - ETA: 3:36 - loss: 10.1345 - custom_mse: 176.4759\n",
            "reduce_confident_loss =  0.596793056\n",
            "\n",
            "reduce_mean_cls_loss =  0.635568559\n",
            "\n",
            "regression loss =  5.9555006\n",
            "34/41 [=======================>......] - ETA: 3:09 - loss: 10.0478 - custom_mse: 289.7469\n",
            "reduce_confident_loss =  0.718733728\n",
            "\n",
            "reduce_mean_cls_loss =  0.676168144\n",
            "\n",
            "regression loss =  6.24549723\n",
            "35/41 [========================>.....] - ETA: 2:43 - loss: 9.9791 - custom_mse: 46.4232  \n",
            "reduce_confident_loss =  0.710335195\n",
            "\n",
            "reduce_mean_cls_loss =  0.721519172\n",
            "\n",
            "regression loss =  11.4653845\n",
            "36/41 [=========================>....] - ETA: 2:16 - loss: 10.0601 - custom_mse: 349.0164\n",
            "reduce_confident_loss =  0.623941422\n",
            "\n",
            "reduce_mean_cls_loss =  0.732529938\n",
            "\n",
            "regression loss =  9.49872684\n",
            "37/41 [==========================>...] - ETA: 1:49 - loss: 10.0816 - custom_mse: 339.5539\n",
            "reduce_confident_loss =  0.716911852\n",
            "\n",
            "reduce_mean_cls_loss =  0.694214165\n",
            "\n",
            "regression loss =  4.1476531\n",
            "38/41 [==========================>...] - ETA: 1:21 - loss: 9.9626 - custom_mse: 60.2874  \n",
            "reduce_confident_loss =  0.638525963\n",
            "\n",
            "reduce_mean_cls_loss =  0.713011384\n",
            "\n",
            "regression loss =  8.73167\n",
            "39/41 [===========================>..] - ETA: 54s - loss: 9.9657 - custom_mse: 216.7073\n",
            "reduce_confident_loss =  0.624056\n",
            "\n",
            "reduce_mean_cls_loss =  0.702417254\n",
            "\n",
            "regression loss =  7.17578602\n",
            "40/41 [============================>.] - ETA: 27s - loss: 9.9291 - custom_mse: 101.9733indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 4\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[3.8145572e-05 0.185968667 0.000173807144 ... 0.00113287568 0.020211041 0.999988198]\n",
            " [1.128554e-06 0.0632222891 2.73014143e-06 ... 0.00432637334 0.00589177 1]\n",
            " [3.74602132e-05 0.0428937674 4.14570059e-05 ... 0.0397338867 0.0709354 0.999977231]\n",
            " [3.37151105e-05 0.0952443779 6.41776933e-06 ... 0.00816229 0.01245749 0.999988556]\n",
            " [2.24200812e-06 0.154449821 7.28289024e-06 ... 0.0161289275 0.0532453656 1]]\n",
            "\n",
            "reduce_confident_loss =  0.606312871\n",
            "\n",
            "reduce_mean_cls_loss =  0.725442\n",
            "\n",
            "regression loss =  7.37512541\n",
            "41/41 [==============================] - ETA: 0s - loss: 9.9196 - custom_mse: 82.2966  \n",
            "reduce_confident_loss =  0.693782389\n",
            "\n",
            "reduce_mean_cls_loss =  0.737185597\n",
            "\n",
            "regression loss =  13.8454924\n",
            "\n",
            "reduce_confident_loss =  0.697022319\n",
            "\n",
            "reduce_mean_cls_loss =  0.737633169\n",
            "\n",
            "regression loss =  8.45463848\n",
            "\n",
            "reduce_confident_loss =  0.802289665\n",
            "\n",
            "reduce_mean_cls_loss =  0.697365403\n",
            "\n",
            "regression loss =  18.3671169\n",
            "\n",
            "reduce_confident_loss =  0.718129098\n",
            "\n",
            "reduce_mean_cls_loss =  0.74229908\n",
            "\n",
            "regression loss =  14.7114429\n",
            "\n",
            "reduce_confident_loss =  0.815351546\n",
            "\n",
            "reduce_mean_cls_loss =  0.699581206\n",
            "\n",
            "regression loss =  13.6401062\n",
            "\n",
            "reduce_confident_loss =  0.818807065\n",
            "\n",
            "reduce_mean_cls_loss =  0.69082737\n",
            "\n",
            "regression loss =  9.89750099\n",
            "\n",
            "reduce_confident_loss =  0.720716596\n",
            "\n",
            "reduce_mean_cls_loss =  0.736452937\n",
            "\n",
            "regression loss =  9.72677612\n",
            "\n",
            "reduce_confident_loss =  0.818161845\n",
            "\n",
            "reduce_mean_cls_loss =  0.698347\n",
            "\n",
            "regression loss =  14.505537\n",
            "\n",
            "reduce_confident_loss =  0.819181442\n",
            "\n",
            "reduce_mean_cls_loss =  0.697902799\n",
            "\n",
            "regression loss =  15.9443569\n",
            "\n",
            "reduce_confident_loss =  0.81826508\n",
            "\n",
            "reduce_mean_cls_loss =  0.698290825\n",
            "\n",
            "regression loss =  17.388464\n",
            "\n",
            "reduce_confident_loss =  0.814243197\n",
            "\n",
            "reduce_mean_cls_loss =  0.699652\n",
            "\n",
            "regression loss =  17.306469\n",
            "\n",
            "reduce_confident_loss =  0.707650125\n",
            "\n",
            "reduce_mean_cls_loss =  0.750856519\n",
            "\n",
            "regression loss =  18.4749393\n",
            "\n",
            "reduce_confident_loss =  0.701538384\n",
            "\n",
            "reduce_mean_cls_loss =  0.68196708\n",
            "\n",
            "regression loss =  15.122488\n",
            "\n",
            "reduce_confident_loss =  0.810743749\n",
            "\n",
            "reduce_mean_cls_loss =  0.699599564\n",
            "\n",
            "regression loss =  9.80941391\n",
            "\n",
            "reduce_confident_loss =  0.805001676\n",
            "\n",
            "reduce_mean_cls_loss =  0.686857522\n",
            "\n",
            "regression loss =  17.7768822\n",
            "41/41 [==============================] - 1261s 31s/step - loss: 9.9196 - custom_mse: 82.2966 - val_loss: 15.7539 - val_custom_mse: 164.7995\n",
            "Epoch 8/10\n",
            "\n",
            "reduce_confident_loss =  0.669872344\n",
            "\n",
            "reduce_mean_cls_loss =  0.714921176\n",
            "\n",
            "regression loss =  17.474165\n",
            " 1/41 [..............................] - ETA: 17:41 - loss: 18.8590 - custom_mse: 502.4844\n",
            "reduce_confident_loss =  0.673395693\n",
            "\n",
            "reduce_mean_cls_loss =  0.616665184\n",
            "\n",
            "regression loss =  10.9553213\n",
            " 2/41 [>.............................] - ETA: 15:48 - loss: 15.5522 - custom_mse: 553.5389\n",
            "reduce_confident_loss =  0.746711791\n",
            "\n",
            "reduce_mean_cls_loss =  0.69314748\n",
            "\n",
            "regression loss =  8.97123\n",
            " 3/41 [=>............................] - ETA: 15:16 - loss: 13.8385 - custom_mse: 571.9194\n",
            "reduce_confident_loss =  0.626660466\n",
            "\n",
            "reduce_mean_cls_loss =  0.70578438\n",
            "\n",
            "regression loss =  7.48252439\n",
            " 4/41 [=>............................] - ETA: 14:47 - loss: 12.5826 - custom_mse: 280.8040\n",
            "reduce_confident_loss =  0.597657144\n",
            "\n",
            "reduce_mean_cls_loss =  0.71077019\n",
            "\n",
            "regression loss =  8.82875347\n",
            " 5/41 [==>...........................] - ETA: 14:23 - loss: 12.0935 - custom_mse: 167.5759\n",
            "reduce_confident_loss =  0.768845439\n",
            "\n",
            "reduce_mean_cls_loss =  0.689960599\n",
            "\n",
            "regression loss =  7.56335354\n",
            " 6/41 [===>..........................] - ETA: 14:11 - loss: 11.5816 - custom_mse: 47.6103 Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "\n",
            "reduce_confident_loss =  0.73649013\n",
            "\n",
            "reduce_mean_cls_loss =  0.693139136\n",
            "\n",
            "regression loss =  15.8344364\n",
            " 7/41 [====>.........................] - ETA: 14:36 - loss: 12.3934 - custom_mse: 145.2366\n",
            "reduce_confident_loss =  0.812997937\n",
            "\n",
            "reduce_mean_cls_loss =  0.693188667\n",
            "\n",
            "regression loss =  15.7469549\n",
            " 8/41 [====>.........................] - ETA: 14:12 - loss: 13.0009 - custom_mse: 292.5951\n",
            "reduce_confident_loss =  0.61146009\n",
            "\n",
            "reduce_mean_cls_loss =  0.653686941\n",
            "\n",
            "regression loss =  10.0653868\n",
            " 9/41 [=====>........................] - ETA: 13:51 - loss: 12.8153 - custom_mse: 203.2543\n",
            "reduce_confident_loss =  0.832403302\n",
            "\n",
            "reduce_mean_cls_loss =  0.693249583\n",
            "\n",
            "regression loss =  13.3323689\n",
            "10/41 [======>.......................] - ETA: 13:26 - loss: 13.0196 - custom_mse: 182.9963\n",
            "reduce_confident_loss =  0.803033352\n",
            "\n",
            "reduce_mean_cls_loss =  0.693155468\n",
            "\n",
            "regression loss =  9.50383854\n",
            "11/41 [=======>......................] - ETA: 13:01 - loss: 12.8360 - custom_mse: 120.1455\n",
            "reduce_confident_loss =  0.812310636\n",
            "\n",
            "reduce_mean_cls_loss =  0.693472743\n",
            "\n",
            "regression loss =  8.22691\n",
            "12/41 [=======>......................] - ETA: 12:35 - loss: 12.5774 - custom_mse: 110.8054\n",
            "reduce_confident_loss =  0.809185803\n",
            "\n",
            "reduce_mean_cls_loss =  0.693185508\n",
            "\n",
            "regression loss =  5.2390132\n",
            "13/41 [========>.....................] - ETA: 12:10 - loss: 12.1284 - custom_mse: 116.5787\n",
            "reduce_confident_loss =  0.797817111\n",
            "\n",
            "reduce_mean_cls_loss =  0.693579555\n",
            "\n",
            "regression loss =  2.69666243\n",
            "14/41 [=========>....................] - ETA: 11:45 - loss: 11.5613 - custom_mse: 103.7383\n",
            "reduce_confident_loss =  0.792550087\n",
            "\n",
            "reduce_mean_cls_loss =  0.705122352\n",
            "\n",
            "regression loss =  9.04676533\n",
            "15/41 [=========>....................] - ETA: 11:19 - loss: 11.4935 - custom_mse: 235.1243\n",
            "reduce_confident_loss =  0.657635152\n",
            "\n",
            "reduce_mean_cls_loss =  0.745441794\n",
            "\n",
            "regression loss =  10.3152084\n",
            "16/41 [==========>...................] - ETA: 10:54 - loss: 11.5075 - custom_mse: 220.3433\n",
            "reduce_confident_loss =  0.635430813\n",
            "\n",
            "reduce_mean_cls_loss =  0.710263669\n",
            "\n",
            "regression loss =  12.4227877\n",
            "17/41 [===========>..................] - ETA: 11:06 - loss: 11.6405 - custom_mse: 392.8237\n",
            "reduce_confident_loss =  0.763999403\n",
            "\n",
            "reduce_mean_cls_loss =  0.693087637\n",
            "\n",
            "regression loss =  18.425539\n",
            "18/41 [============>.................] - ETA: 10:36 - loss: 12.0984 - custom_mse: 495.7773\n",
            "reduce_confident_loss =  0.612948298\n",
            "\n",
            "reduce_mean_cls_loss =  0.666861\n",
            "\n",
            "regression loss =  9.0969286\n",
            "19/41 [============>.................] - ETA: 10:07 - loss: 12.0078 - custom_mse: 336.9769\n",
            "reduce_confident_loss =  0.680757463\n",
            "\n",
            "reduce_mean_cls_loss =  0.662667871\n",
            "\n",
            "regression loss =  10.6177492\n",
            "20/41 [=============>................] - ETA: 9:38 - loss: 12.0055 - custom_mse: 265.2248 \n",
            "reduce_confident_loss =  0.609395444\n",
            "\n",
            "reduce_mean_cls_loss =  0.638523161\n",
            "\n",
            "regression loss =  9.66911125\n",
            "21/41 [==============>...............] - ETA: 9:10 - loss: 11.9536 - custom_mse: 332.8331\n",
            "reduce_confident_loss =  0.62388885\n",
            "\n",
            "reduce_mean_cls_loss =  0.693983853\n",
            "\n",
            "regression loss =  7.18820238\n",
            "22/41 [===============>..............] - ETA: 8:41 - loss: 11.7969 - custom_mse: 310.5020\n",
            "reduce_confident_loss =  0.601567\n",
            "\n",
            "reduce_mean_cls_loss =  0.68152988\n",
            "\n",
            "regression loss =  10.4818211\n",
            "23/41 [===============>..............] - ETA: 8:13 - loss: 11.7955 - custom_mse: 106.9299\n",
            "reduce_confident_loss =  0.789268672\n",
            "\n",
            "reduce_mean_cls_loss =  0.69314605\n",
            "\n",
            "regression loss =  4.12744522\n",
            "24/41 [================>.............] - ETA: 7:45 - loss: 11.5378 - custom_mse: 11.1057 \n",
            "reduce_confident_loss =  0.797507584\n",
            "\n",
            "reduce_mean_cls_loss =  0.693146765\n",
            "\n",
            "regression loss =  3.78722596\n",
            "25/41 [=================>............] - ETA: 7:17 - loss: 11.2874 - custom_mse: 8.8311 \n",
            "reduce_confident_loss =  0.760373\n",
            "\n",
            "reduce_mean_cls_loss =  0.693145931\n",
            "\n",
            "regression loss =  4.53850174\n",
            "26/41 [==================>...........] - ETA: 6:49 - loss: 11.0837 - custom_mse: 180.2368\n",
            "reduce_confident_loss =  0.652183414\n",
            "\n",
            "reduce_mean_cls_loss =  0.689308465\n",
            "\n",
            "regression loss =  5.00707674\n",
            "27/41 [==================>...........] - ETA: 6:21 - loss: 10.9084 - custom_mse: 256.4831\n",
            "reduce_confident_loss =  0.644058883\n",
            "\n",
            "reduce_mean_cls_loss =  0.686294496\n",
            "\n",
            "regression loss =  5.82889\n",
            "28/41 [===================>..........] - ETA: 5:54 - loss: 10.7745 - custom_mse: 177.9327\n",
            "reduce_confident_loss =  0.662808299\n",
            "\n",
            "reduce_mean_cls_loss =  0.640838325\n",
            "\n",
            "regression loss =  8.20422363\n",
            "29/41 [====================>.........] - ETA: 5:26 - loss: 10.7308 - custom_mse: 253.9126\n",
            "reduce_confident_loss =  0.587715\n",
            "\n",
            "reduce_mean_cls_loss =  0.680153191\n",
            "\n",
            "regression loss =  8.00688648\n",
            "30/41 [====================>.........] - ETA: 4:59 - loss: 10.6823 - custom_mse: 312.3030\n",
            "reduce_confident_loss =  0.58578831\n",
            "\n",
            "reduce_mean_cls_loss =  0.655694664\n",
            "\n",
            "regression loss =  6.31940651\n",
            "31/41 [=====================>........] - ETA: 4:31 - loss: 10.5816 - custom_mse: 264.6110\n",
            "reduce_confident_loss =  0.590117037\n",
            "\n",
            "reduce_mean_cls_loss =  0.762619555\n",
            "\n",
            "regression loss =  5.67446232\n",
            "32/41 [======================>.......] - ETA: 4:04 - loss: 10.4705 - custom_mse: 170.7257\n",
            "reduce_confident_loss =  0.556927383\n",
            "\n",
            "reduce_mean_cls_loss =  0.773666501\n",
            "\n",
            "regression loss =  8.87721252\n",
            "33/41 [=======================>......] - ETA: 3:37 - loss: 10.4625 - custom_mse: 147.3057\n",
            "reduce_confident_loss =  0.597882926\n",
            "\n",
            "reduce_mean_cls_loss =  0.645220101\n",
            "\n",
            "regression loss =  8.3538723\n",
            "34/41 [=======================>......] - ETA: 3:09 - loss: 10.4371 - custom_mse: 266.6913\n",
            "reduce_confident_loss =  0.706023276\n",
            "\n",
            "reduce_mean_cls_loss =  0.679944396\n",
            "\n",
            "regression loss =  6.99817228\n",
            "35/41 [========================>.....] - ETA: 2:43 - loss: 10.3784 - custom_mse: 73.6867 \n",
            "reduce_confident_loss =  0.715156\n",
            "\n",
            "reduce_mean_cls_loss =  0.707662523\n",
            "\n",
            "regression loss =  11.6368647\n",
            "36/41 [=========================>....] - ETA: 2:16 - loss: 10.4529 - custom_mse: 322.1473\n",
            "reduce_confident_loss =  0.622918\n",
            "\n",
            "reduce_mean_cls_loss =  0.712092936\n",
            "\n",
            "regression loss =  10.2243395\n",
            "37/41 [==========================>...] - ETA: 1:49 - loss: 10.4828 - custom_mse: 250.1613\n",
            "reduce_confident_loss =  0.713466942\n",
            "\n",
            "reduce_mean_cls_loss =  0.693724513\n",
            "\n",
            "regression loss =  7.59517527\n",
            "38/41 [==========================>...] - ETA: 1:22 - loss: 10.4438 - custom_mse: 69.3305 \n",
            "reduce_confident_loss =  0.635006\n",
            "\n",
            "reduce_mean_cls_loss =  0.703534663\n",
            "\n",
            "regression loss =  5.20447254\n",
            "39/41 [===========================>..] - ETA: 54s - loss: 10.3438 - custom_mse: 223.6799\n",
            "reduce_confident_loss =  0.631700218\n",
            "\n",
            "reduce_mean_cls_loss =  0.724314213\n",
            "\n",
            "regression loss =  7.33955908\n",
            "40/41 [============================>.] - ETA: 27s - loss: 10.3026 - custom_mse: 126.3680indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 3\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[1.31558613e-06 0.191891879 0.000241994858 ... 0.0362701416 0.0763256252 0.999997675]\n",
            " [5.47787795e-06 8.05366e-05 9.22897e-07 ... 0.00184926391 0.00145509839 0.999865174]\n",
            " [6.47188426e-05 0.00856557488 5.87827844e-06 ... 0.00642237067 0.01627478 0.999986291]\n",
            " [7.19934633e-06 0.0014283061 8.03477305e-05 ... 0.00294360518 0.0057926476 0.999989808]\n",
            " [8.94709083e-05 0.0137238503 7.08333209e-06 ... 0.00633579493 0.0131816864 0.999997616]]\n",
            "\n",
            "reduce_confident_loss =  0.606317639\n",
            "\n",
            "reduce_mean_cls_loss =  0.708177209\n",
            "\n",
            "regression loss =  8.93259239\n",
            "41/41 [==============================] - ETA: 0s - loss: 10.3022 - custom_mse: 109.0828 \n",
            "reduce_confident_loss =  0.602903426\n",
            "\n",
            "reduce_mean_cls_loss =  0.704888\n",
            "\n",
            "regression loss =  13.5287724\n",
            "\n",
            "reduce_confident_loss =  0.604848564\n",
            "\n",
            "reduce_mean_cls_loss =  0.705112159\n",
            "\n",
            "regression loss =  10.1969948\n",
            "\n",
            "reduce_confident_loss =  0.726540148\n",
            "\n",
            "reduce_mean_cls_loss =  0.693176925\n",
            "\n",
            "regression loss =  16.2122746\n",
            "\n",
            "reduce_confident_loss =  0.621263504\n",
            "\n",
            "reduce_mean_cls_loss =  0.706967473\n",
            "\n",
            "regression loss =  11.862195\n",
            "\n",
            "reduce_confident_loss =  0.734105408\n",
            "\n",
            "reduce_mean_cls_loss =  0.693243682\n",
            "\n",
            "regression loss =  15.7358971\n",
            "\n",
            "reduce_confident_loss =  0.734302402\n",
            "\n",
            "reduce_mean_cls_loss =  0.693107665\n",
            "\n",
            "regression loss =  11.909\n",
            "\n",
            "reduce_confident_loss =  0.624941468\n",
            "\n",
            "reduce_mean_cls_loss =  0.705817103\n",
            "\n",
            "regression loss =  15.1591206\n",
            "\n",
            "reduce_confident_loss =  0.731705308\n",
            "\n",
            "reduce_mean_cls_loss =  0.693238378\n",
            "\n",
            "regression loss =  13.3958683\n",
            "\n",
            "reduce_confident_loss =  0.731485724\n",
            "\n",
            "reduce_mean_cls_loss =  0.693237841\n",
            "\n",
            "regression loss =  12.5972805\n",
            "\n",
            "reduce_confident_loss =  0.732033372\n",
            "\n",
            "reduce_mean_cls_loss =  0.693240762\n",
            "\n",
            "regression loss =  12.9245281\n",
            "\n",
            "reduce_confident_loss =  0.729676187\n",
            "\n",
            "reduce_mean_cls_loss =  0.693225861\n",
            "\n",
            "regression loss =  14.7040462\n",
            "\n",
            "reduce_confident_loss =  0.618207276\n",
            "\n",
            "reduce_mean_cls_loss =  0.707948685\n",
            "\n",
            "regression loss =  16.8922749\n",
            "\n",
            "reduce_confident_loss =  0.612358689\n",
            "\n",
            "reduce_mean_cls_loss =  0.687979281\n",
            "\n",
            "regression loss =  11.8900661\n",
            "\n",
            "reduce_confident_loss =  0.731353343\n",
            "\n",
            "reduce_mean_cls_loss =  0.693242192\n",
            "\n",
            "regression loss =  9.84675217\n",
            "\n",
            "reduce_confident_loss =  0.728423476\n",
            "\n",
            "reduce_mean_cls_loss =  0.693047583\n",
            "\n",
            "regression loss =  19.2582378\n",
            "41/41 [==============================] - 1266s 31s/step - loss: 10.3022 - custom_mse: 109.0828 - val_loss: 15.0281 - val_custom_mse: 170.6103\n",
            "Epoch 9/10\n",
            "\n",
            "reduce_confident_loss =  0.675277829\n",
            "\n",
            "reduce_mean_cls_loss =  0.705260396\n",
            "\n",
            "regression loss =  17.1854267\n",
            " 1/41 [..............................] - ETA: 17:33 - loss: 18.5660 - custom_mse: 438.6463\n",
            "reduce_confident_loss =  0.617641866\n",
            "\n",
            "reduce_mean_cls_loss =  0.665329754\n",
            "\n",
            "regression loss =  10.0667429\n",
            " 2/41 [>.............................] - ETA: 15:45 - loss: 14.9578 - custom_mse: 521.0162\n",
            "reduce_confident_loss =  0.725904047\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147242\n",
            "\n",
            "regression loss =  8.36162186\n",
            " 3/41 [=>............................] - ETA: 15:09 - loss: 13.2321 - custom_mse: 527.6312\n",
            "reduce_confident_loss =  0.59537518\n",
            "\n",
            "reduce_mean_cls_loss =  0.711969316\n",
            "\n",
            "regression loss =  6.98687077\n",
            " 4/41 [=>............................] - ETA: 14:50 - loss: 11.9976 - custom_mse: 300.5587\n",
            "reduce_confident_loss =  0.590479434\n",
            "\n",
            "reduce_mean_cls_loss =  0.701245964\n",
            "\n",
            "regression loss =  10.5699377\n",
            " 5/41 [==>...........................] - ETA: 14:25 - loss: 11.9704 - custom_mse: 164.8942\n",
            "reduce_confident_loss =  0.755800784\n",
            "\n",
            "reduce_mean_cls_loss =  0.693103\n",
            "\n",
            "regression loss =  7.24247932\n",
            " 6/41 [===>..........................] - ETA: 14:12 - loss: 11.4239 - custom_mse: 47.0465 Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "\n",
            "reduce_confident_loss =  0.722964287\n",
            "\n",
            "reduce_mean_cls_loss =  0.693134\n",
            "\n",
            "regression loss =  11.6858377\n",
            " 7/41 [====>.........................] - ETA: 14:36 - loss: 11.6637 - custom_mse: 103.1275\n",
            "reduce_confident_loss =  0.77881068\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147659\n",
            "\n",
            "regression loss =  5.49896\n",
            " 8/41 [====>.........................] - ETA: 14:14 - loss: 11.0771 - custom_mse: 231.5311\n",
            "reduce_confident_loss =  0.599639118\n",
            "\n",
            "reduce_mean_cls_loss =  0.686516643\n",
            "\n",
            "regression loss =  7.04862165\n",
            " 9/41 [=====>........................] - ETA: 13:50 - loss: 10.7724 - custom_mse: 269.7841\n",
            "reduce_confident_loss =  0.832480848\n",
            "\n",
            "reduce_mean_cls_loss =  0.693148613\n",
            "\n",
            "regression loss =  4.06355381\n",
            "10/41 [======>.......................] - ETA: 13:25 - loss: 10.2540 - custom_mse: 77.5655 \n",
            "reduce_confident_loss =  0.800030589\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147361\n",
            "\n",
            "regression loss =  5.4464345\n",
            "11/41 [=======>......................] - ETA: 13:00 - loss: 9.9527 - custom_mse: 57.7182 \n",
            "reduce_confident_loss =  0.802484632\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147361\n",
            "\n",
            "regression loss =  5.94074821\n",
            "12/41 [=======>......................] - ETA: 12:35 - loss: 9.7430 - custom_mse: 55.5272\n",
            "reduce_confident_loss =  0.774963319\n",
            "\n",
            "reduce_mean_cls_loss =  0.693152308\n",
            "\n",
            "regression loss =  5.00875\n",
            "13/41 [========>.....................] - ETA: 12:10 - loss: 9.4918 - custom_mse: 67.4736\n",
            "reduce_confident_loss =  0.787051857\n",
            "\n",
            "reduce_mean_cls_loss =  0.693193734\n",
            "\n",
            "regression loss =  4.1538949\n",
            "14/41 [=========>....................] - ETA: 11:45 - loss: 9.2162 - custom_mse: 70.1939\n",
            "reduce_confident_loss =  0.753983915\n",
            "\n",
            "reduce_mean_cls_loss =  0.717906773\n",
            "\n",
            "regression loss =  8.59172058\n",
            "15/41 [=========>....................] - ETA: 11:20 - loss: 9.2727 - custom_mse: 218.4994\n",
            "reduce_confident_loss =  0.65914005\n",
            "\n",
            "reduce_mean_cls_loss =  0.782938957\n",
            "\n",
            "regression loss =  10.6316299\n",
            "16/41 [==========>...................] - ETA: 10:54 - loss: 9.4478 - custom_mse: 208.1132\n",
            "reduce_confident_loss =  0.635737062\n",
            "\n",
            "reduce_mean_cls_loss =  0.748214245\n",
            "\n",
            "regression loss =  9.7208643\n",
            "17/41 [===========>..................] - ETA: 11:06 - loss: 9.5453 - custom_mse: 291.2472\n",
            "reduce_confident_loss =  0.757702172\n",
            "\n",
            "reduce_mean_cls_loss =  0.692145348\n",
            "\n",
            "regression loss =  10.0598516\n",
            "18/41 [============>.................] - ETA: 10:36 - loss: 9.6544 - custom_mse: 309.0031\n",
            "reduce_confident_loss =  0.644937\n",
            "\n",
            "reduce_mean_cls_loss =  0.58761847\n",
            "\n",
            "regression loss =  6.64009571\n",
            "19/41 [============>.................] - ETA: 10:07 - loss: 9.5606 - custom_mse: 241.1935\n",
            "reduce_confident_loss =  0.664921105\n",
            "\n",
            "reduce_mean_cls_loss =  0.632683277\n",
            "\n",
            "regression loss =  9.02481174\n",
            "20/41 [=============>................] - ETA: 9:38 - loss: 9.5987 - custom_mse: 284.4155 \n",
            "reduce_confident_loss =  0.584114194\n",
            "\n",
            "reduce_mean_cls_loss =  0.626985133\n",
            "\n",
            "regression loss =  7.7128\n",
            "21/41 [==============>...............] - ETA: 9:10 - loss: 9.5666 - custom_mse: 279.6822\n",
            "reduce_confident_loss =  0.608602524\n",
            "\n",
            "reduce_mean_cls_loss =  0.741602778\n",
            "\n",
            "regression loss =  9.63181782\n",
            "22/41 [===============>..............] - ETA: 8:41 - loss: 9.6309 - custom_mse: 284.5793\n",
            "reduce_confident_loss =  0.591489911\n",
            "\n",
            "reduce_mean_cls_loss =  0.65057373\n",
            "\n",
            "regression loss =  8.1185627\n",
            "23/41 [===============>..............] - ETA: 8:13 - loss: 9.6192 - custom_mse: 100.4849\n",
            "reduce_confident_loss =  0.758944094\n",
            "\n",
            "reduce_mean_cls_loss =  0.693125427\n",
            "\n",
            "regression loss =  4.93181229\n",
            "24/41 [================>.............] - ETA: 7:45 - loss: 9.4844 - custom_mse: 13.8947 \n",
            "reduce_confident_loss =  0.76050216\n",
            "\n",
            "reduce_mean_cls_loss =  0.692166\n",
            "\n",
            "regression loss =  3.45575476\n",
            "25/41 [=================>............] - ETA: 7:17 - loss: 9.3013 - custom_mse: 8.8411 \n",
            "reduce_confident_loss =  0.735887885\n",
            "\n",
            "reduce_mean_cls_loss =  0.69312638\n",
            "\n",
            "regression loss =  7.44725466\n",
            "26/41 [==================>...........] - ETA: 6:49 - loss: 9.2850 - custom_mse: 212.2455\n",
            "reduce_confident_loss =  0.628236294\n",
            "\n",
            "reduce_mean_cls_loss =  0.676770747\n",
            "\n",
            "regression loss =  5.6961937\n",
            "27/41 [==================>...........] - ETA: 6:21 - loss: 9.2004 - custom_mse: 312.8104\n",
            "reduce_confident_loss =  0.634960473\n",
            "\n",
            "reduce_mean_cls_loss =  0.605586171\n",
            "\n",
            "regression loss =  5.03068066\n",
            "28/41 [===================>..........] - ETA: 5:53 - loss: 9.0958 - custom_mse: 234.2618\n",
            "reduce_confident_loss =  0.620215893\n",
            "\n",
            "reduce_mean_cls_loss =  0.614127\n",
            "\n",
            "regression loss =  5.68021488\n",
            "29/41 [====================>.........] - ETA: 5:26 - loss: 9.0206 - custom_mse: 319.7316\n",
            "reduce_confident_loss =  0.599039853\n",
            "\n",
            "reduce_mean_cls_loss =  0.644081533\n",
            "\n",
            "regression loss =  6.88400459\n",
            "30/41 [====================>.........] - ETA: 4:58 - loss: 8.9908 - custom_mse: 359.6237\n",
            "reduce_confident_loss =  0.596283138\n",
            "\n",
            "reduce_mean_cls_loss =  0.600081146\n",
            "\n",
            "regression loss =  5.93327761\n",
            "31/41 [=====================>........] - ETA: 4:31 - loss: 8.9307 - custom_mse: 291.2771\n",
            "reduce_confident_loss =  0.589915812\n",
            "\n",
            "reduce_mean_cls_loss =  0.748682618\n",
            "\n",
            "regression loss =  5.8923192\n",
            "32/41 [======================>.......] - ETA: 4:03 - loss: 8.8776 - custom_mse: 283.9695\n",
            "reduce_confident_loss =  0.567370772\n",
            "\n",
            "reduce_mean_cls_loss =  0.716921806\n",
            "\n",
            "regression loss =  7.66332054\n",
            "33/41 [=======================>......] - ETA: 3:36 - loss: 8.8798 - custom_mse: 193.1965\n",
            "reduce_confident_loss =  0.612466216\n",
            "\n",
            "reduce_mean_cls_loss =  0.621294\n",
            "\n",
            "regression loss =  5.70901442\n",
            "34/41 [=======================>......] - ETA: 3:09 - loss: 8.8228 - custom_mse: 298.3353\n",
            "reduce_confident_loss =  0.694687963\n",
            "\n",
            "reduce_mean_cls_loss =  0.67496711\n",
            "\n",
            "regression loss =  7.20366764\n",
            "35/41 [========================>.....] - ETA: 2:43 - loss: 8.8157 - custom_mse: 53.1440 \n",
            "reduce_confident_loss =  0.694918454\n",
            "\n",
            "reduce_mean_cls_loss =  0.71216625\n",
            "\n",
            "regression loss =  10.7250481\n",
            "36/41 [=========================>....] - ETA: 2:16 - loss: 8.9078 - custom_mse: 347.3662\n",
            "reduce_confident_loss =  0.638686717\n",
            "\n",
            "reduce_mean_cls_loss =  0.747823536\n",
            "\n",
            "regression loss =  6.49400282\n",
            "37/41 [==========================>...] - ETA: 1:49 - loss: 8.8800 - custom_mse: 345.0568\n",
            "reduce_confident_loss =  0.723282516\n",
            "\n",
            "reduce_mean_cls_loss =  0.709227622\n",
            "\n",
            "regression loss =  4.36978674\n",
            "38/41 [==========================>...] - ETA: 1:21 - loss: 8.7990 - custom_mse: 61.8609 \n",
            "reduce_confident_loss =  0.672209084\n",
            "\n",
            "reduce_mean_cls_loss =  0.700945854\n",
            "\n",
            "regression loss =  8.15241\n",
            "39/41 [===========================>..] - ETA: 54s - loss: 8.8177 - custom_mse: 224.5062\n",
            "reduce_confident_loss =  0.613183677\n",
            "\n",
            "reduce_mean_cls_loss =  0.715985298\n",
            "\n",
            "regression loss =  11.9745159\n",
            "40/41 [============================>.] - ETA: 27s - loss: 8.9298 - custom_mse: 123.2438indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 2\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 1\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.000524193048 0.0156159699 0.00014102459 ... 0.0632348657 0.0526700616 0.999350429]\n",
            " [0.00132730603 0.00564184785 3.77222023e-05 ... 0.0148212612 0.0056245029 0.999961615]\n",
            " [0.00187772512 0.224871695 1.80417519e-05 ... 0.0170478225 0.00734227896 0.999984622]\n",
            " [0.00655844808 0.293359965 0.000199645758 ... 0.00760799646 0.00510138273 0.999983072]\n",
            " [3.32268e-05 0.0826568902 0.000114344592 ... 0.00359329581 0.0448468328 0.999880433]]\n",
            "\n",
            "reduce_confident_loss =  0.60461545\n",
            "\n",
            "reduce_mean_cls_loss =  0.732295871\n",
            "\n",
            "regression loss =  9.37812328\n",
            "41/41 [==============================] - ETA: 0s - loss: 8.9436 - custom_mse: 102.4566 \n",
            "reduce_confident_loss =  0.588031471\n",
            "\n",
            "reduce_mean_cls_loss =  0.754743159\n",
            "\n",
            "regression loss =  11.9865971\n",
            "\n",
            "reduce_confident_loss =  0.594220042\n",
            "\n",
            "reduce_mean_cls_loss =  0.735056877\n",
            "\n",
            "regression loss =  9.267416\n",
            "\n",
            "reduce_confident_loss =  0.722338378\n",
            "\n",
            "reduce_mean_cls_loss =  0.693161547\n",
            "\n",
            "regression loss =  15.827651\n",
            "\n",
            "reduce_confident_loss =  0.611803472\n",
            "\n",
            "reduce_mean_cls_loss =  0.76052922\n",
            "\n",
            "regression loss =  12.4291611\n",
            "\n",
            "reduce_confident_loss =  0.724693239\n",
            "\n",
            "reduce_mean_cls_loss =  0.693998039\n",
            "\n",
            "regression loss =  13.913373\n",
            "\n",
            "reduce_confident_loss =  0.73077935\n",
            "\n",
            "reduce_mean_cls_loss =  0.692475\n",
            "\n",
            "regression loss =  9.24064\n",
            "\n",
            "reduce_confident_loss =  0.616665661\n",
            "\n",
            "reduce_mean_cls_loss =  0.794451714\n",
            "\n",
            "regression loss =  10.5678215\n",
            "\n",
            "reduce_confident_loss =  0.726974845\n",
            "\n",
            "reduce_mean_cls_loss =  0.694297731\n",
            "\n",
            "regression loss =  13.2685957\n",
            "\n",
            "reduce_confident_loss =  0.724830806\n",
            "\n",
            "reduce_mean_cls_loss =  0.694343567\n",
            "\n",
            "regression loss =  12.4854908\n",
            "\n",
            "reduce_confident_loss =  0.72619468\n",
            "\n",
            "reduce_mean_cls_loss =  0.694178164\n",
            "\n",
            "regression loss =  13.7541981\n",
            "\n",
            "reduce_confident_loss =  0.72291863\n",
            "\n",
            "reduce_mean_cls_loss =  0.693862498\n",
            "\n",
            "regression loss =  15.6473274\n",
            "\n",
            "reduce_confident_loss =  0.612168968\n",
            "\n",
            "reduce_mean_cls_loss =  0.777671218\n",
            "\n",
            "regression loss =  17.5945091\n",
            "\n",
            "reduce_confident_loss =  0.608517349\n",
            "\n",
            "reduce_mean_cls_loss =  0.661836088\n",
            "\n",
            "regression loss =  11.9123325\n",
            "\n",
            "reduce_confident_loss =  0.727557302\n",
            "\n",
            "reduce_mean_cls_loss =  0.694015622\n",
            "\n",
            "regression loss =  10.2219782\n",
            "\n",
            "reduce_confident_loss =  0.72292614\n",
            "\n",
            "reduce_mean_cls_loss =  0.692181528\n",
            "\n",
            "regression loss =  16.527565\n",
            "41/41 [==============================] - 1263s 31s/step - loss: 8.9436 - custom_mse: 102.4566 - val_loss: 14.3082 - val_custom_mse: 147.2243\n",
            "Epoch 10/10\n",
            "\n",
            "reduce_confident_loss =  0.671810687\n",
            "\n",
            "reduce_mean_cls_loss =  0.730207622\n",
            "\n",
            "regression loss =  16.3115406\n",
            " 1/41 [..............................] - ETA: 17:36 - loss: 17.7136 - custom_mse: 432.6048\n",
            "reduce_confident_loss =  0.664600492\n",
            "\n",
            "reduce_mean_cls_loss =  0.623556614\n",
            "\n",
            "regression loss =  9.00809479\n",
            " 2/41 [>.............................] - ETA: 15:35 - loss: 14.0049 - custom_mse: 541.1487\n",
            "reduce_confident_loss =  0.722888\n",
            "\n",
            "reduce_mean_cls_loss =  0.693147957\n",
            "\n",
            "regression loss =  8.38785267\n",
            " 3/41 [=>............................] - ETA: 15:06 - loss: 12.6046 - custom_mse: 607.8109\n",
            "reduce_confident_loss =  0.588531\n",
            "\n",
            "reduce_mean_cls_loss =  0.738886416\n",
            "\n",
            "regression loss =  6.99848461\n",
            " 4/41 [=>............................] - ETA: 14:39 - loss: 11.5349 - custom_mse: 268.4187\n",
            "reduce_confident_loss =  0.588792384\n",
            "\n",
            "reduce_mean_cls_loss =  0.707816422\n",
            "\n",
            "regression loss =  8.99887848\n",
            " 5/41 [==>...........................] - ETA: 14:15 - loss: 11.2870 - custom_mse: 134.1812\n",
            "reduce_confident_loss =  0.738217711\n",
            "\n",
            "reduce_mean_cls_loss =  0.69047451\n",
            "\n",
            "regression loss =  6.74357748\n",
            " 6/41 [===>..........................] - ETA: 14:04 - loss: 10.7679 - custom_mse: 38.0041 Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "Can not convert <class 'str'> to int\n",
            "\n",
            "reduce_confident_loss =  0.731496334\n",
            "\n",
            "reduce_mean_cls_loss =  0.691622138\n",
            "\n",
            "regression loss =  11.1438828\n",
            " 7/41 [====>.........................] - ETA: 14:34 - loss: 11.0249 - custom_mse: 105.8299\n",
            "reduce_confident_loss =  0.799739957\n",
            "\n",
            "reduce_mean_cls_loss =  0.693210661\n",
            "\n",
            "regression loss =  12.3953676\n",
            " 8/41 [====>.........................] - ETA: 14:11 - loss: 11.3828 - custom_mse: 287.3240\n",
            "reduce_confident_loss =  0.609984\n",
            "\n",
            "reduce_mean_cls_loss =  0.67806828\n",
            "\n",
            "regression loss =  9.24838734\n",
            " 9/41 [=====>........................] - ETA: 13:48 - loss: 11.2888 - custom_mse: 195.3269\n",
            "reduce_confident_loss =  0.804795682\n",
            "\n",
            "reduce_mean_cls_loss =  0.693152308\n",
            "\n",
            "regression loss =  9.43512\n",
            "10/41 [======>.......................] - ETA: 13:24 - loss: 11.2532 - custom_mse: 173.0192\n",
            "reduce_confident_loss =  0.800897241\n",
            "\n",
            "reduce_mean_cls_loss =  0.693183064\n",
            "\n",
            "regression loss =  7.34693718\n",
            "11/41 [=======>......................] - ETA: 12:59 - loss: 11.0339 - custom_mse: 133.9993\n",
            "reduce_confident_loss =  0.785527766\n",
            "\n",
            "reduce_mean_cls_loss =  0.693170428\n",
            "\n",
            "regression loss =  5.68823814\n",
            "12/41 [=======>......................] - ETA: 12:33 - loss: 10.7117 - custom_mse: 117.0915\n",
            "reduce_confident_loss =  0.774885833\n",
            "\n",
            "reduce_mean_cls_loss =  0.693170846\n",
            "\n",
            "regression loss =  3.84574556\n",
            "13/41 [========>.....................] - ETA: 12:08 - loss: 10.2965 - custom_mse: 124.9440\n",
            "reduce_confident_loss =  0.777965546\n",
            "\n",
            "reduce_mean_cls_loss =  0.6935094\n",
            "\n",
            "regression loss =  3.6700058\n",
            "14/41 [=========>....................] - ETA: 11:42 - loss: 9.9282 - custom_mse: 141.2716 \n",
            "reduce_confident_loss =  0.752913356\n",
            "\n",
            "reduce_mean_cls_loss =  0.704327881\n",
            "\n",
            "regression loss =  6.79876232\n",
            "15/41 [=========>....................] - ETA: 11:17 - loss: 9.8168 - custom_mse: 201.5667\n",
            "reduce_confident_loss =  0.629635811\n",
            "\n",
            "reduce_mean_cls_loss =  0.766459107\n",
            "\n",
            "regression loss =  8.93732071\n",
            "16/41 [==========>...................] - ETA: 10:52 - loss: 9.8491 - custom_mse: 199.9521\n",
            "reduce_confident_loss =  0.630709\n",
            "\n",
            "reduce_mean_cls_loss =  0.749525964\n",
            "\n",
            "regression loss =  7.34512329\n",
            "17/41 [===========>..................] - ETA: 11:05 - loss: 9.7830 - custom_mse: 316.0399\n",
            "reduce_confident_loss =  0.738554299\n",
            "\n",
            "reduce_mean_cls_loss =  0.689444423\n",
            "\n",
            "regression loss =  12.2892923\n",
            "18/41 [============>.................] - ETA: 10:35 - loss: 10.0015 - custom_mse: 368.3370\n",
            "reduce_confident_loss =  0.611621678\n",
            "\n",
            "reduce_mean_cls_loss =  0.588621914\n",
            "\n",
            "regression loss =  8.6096487\n",
            "19/41 [============>.................] - ETA: 10:07 - loss: 9.9914 - custom_mse: 271.2008 \n",
            "reduce_confident_loss =  0.655102372\n",
            "\n",
            "reduce_mean_cls_loss =  0.666467667\n",
            "\n",
            "regression loss =  11.7275162\n",
            "20/41 [=============>................] - ETA: 9:38 - loss: 10.1443 - custom_mse: 252.6882\n",
            "reduce_confident_loss =  0.59633857\n",
            "\n",
            "reduce_mean_cls_loss =  0.634730518\n",
            "\n",
            "regression loss =  8.11304188\n",
            "21/41 [==============>...............] - ETA: 9:09 - loss: 10.1062 - custom_mse: 325.8578\n",
            "reduce_confident_loss =  0.59805882\n",
            "\n",
            "reduce_mean_cls_loss =  0.693653643\n",
            "\n",
            "regression loss =  6.98534393\n",
            "22/41 [===============>..............] - ETA: 8:41 - loss: 10.0231 - custom_mse: 283.6302\n",
            "reduce_confident_loss =  0.590643227\n",
            "\n",
            "reduce_mean_cls_loss =  0.685845494\n",
            "\n",
            "regression loss =  8.66927052\n",
            "23/41 [===============>..............] - ETA: 8:13 - loss: 10.0197 - custom_mse: 96.5326 \n",
            "reduce_confident_loss =  0.742650449\n",
            "\n",
            "reduce_mean_cls_loss =  0.693144441\n",
            "\n",
            "regression loss =  2.66395\n",
            "24/41 [================>.............] - ETA: 7:44 - loss: 9.7730 - custom_mse: 4.7548  \n",
            "reduce_confident_loss =  0.736195445\n",
            "\n",
            "reduce_mean_cls_loss =  0.693145752\n",
            "\n",
            "regression loss =  3.42956543\n",
            "25/41 [=================>............] - ETA: 7:16 - loss: 9.5765 - custom_mse: 7.4651\n",
            "reduce_confident_loss =  0.714588583\n",
            "\n",
            "reduce_mean_cls_loss =  0.693146288\n",
            "\n",
            "regression loss =  3.77309036\n",
            "26/41 [==================>...........] - ETA: 6:49 - loss: 9.4074 - custom_mse: 177.9872\n",
            "reduce_confident_loss =  0.628781796\n",
            "\n",
            "reduce_mean_cls_loss =  0.68890208\n",
            "\n",
            "regression loss =  4.60969877\n",
            "27/41 [==================>...........] - ETA: 6:21 - loss: 9.2785 - custom_mse: 255.2867\n",
            "reduce_confident_loss =  0.626340568\n",
            "\n",
            "reduce_mean_cls_loss =  0.653626502\n",
            "\n",
            "regression loss =  6.13411474\n",
            "28/41 [===================>..........] - ETA: 5:53 - loss: 9.2119 - custom_mse: 197.4138\n",
            "reduce_confident_loss =  0.621851087\n",
            "\n",
            "reduce_mean_cls_loss =  0.660544097\n",
            "\n",
            "regression loss =  6.43109465\n",
            "29/41 [====================>.........] - ETA: 5:26 - loss: 9.1603 - custom_mse: 297.0348\n",
            "reduce_confident_loss =  0.581501842\n",
            "\n",
            "reduce_mean_cls_loss =  0.66342169\n",
            "\n",
            "regression loss =  5.68150425\n",
            "30/41 [====================>.........] - ETA: 4:58 - loss: 9.0858 - custom_mse: 363.7563\n",
            "reduce_confident_loss =  0.594192803\n",
            "\n",
            "reduce_mean_cls_loss =  0.625534654\n",
            "\n",
            "regression loss =  5.17495108\n",
            "31/41 [=====================>........] - ETA: 4:31 - loss: 8.9990 - custom_mse: 296.8487\n",
            "reduce_confident_loss =  0.5950948\n",
            "\n",
            "reduce_mean_cls_loss =  0.723102689\n",
            "\n",
            "regression loss =  4.65342093\n",
            "32/41 [======================>.......] - ETA: 4:04 - loss: 8.9044 - custom_mse: 230.6952\n",
            "reduce_confident_loss =  0.579851091\n",
            "\n",
            "reduce_mean_cls_loss =  0.74885124\n",
            "\n",
            "regression loss =  8.57880878\n",
            "33/41 [=======================>......] - ETA: 3:36 - loss: 8.9348 - custom_mse: 181.4185\n",
            "reduce_confident_loss =  0.609048426\n",
            "\n",
            "reduce_mean_cls_loss =  0.647099674\n",
            "\n",
            "regression loss =  6.83271933\n",
            "34/41 [=======================>......] - ETA: 3:09 - loss: 8.9099 - custom_mse: 294.8814\n",
            "reduce_confident_loss =  0.688092351\n",
            "\n",
            "reduce_mean_cls_loss =  0.687743187\n",
            "\n",
            "regression loss =  6.36856556\n",
            "35/41 [========================>.....] - ETA: 2:43 - loss: 8.8766 - custom_mse: 68.7704 \n",
            "reduce_confident_loss =  0.668922842\n",
            "\n",
            "reduce_mean_cls_loss =  0.699245095\n",
            "\n",
            "regression loss =  10.3119736\n",
            "36/41 [=========================>....] - ETA: 2:16 - loss: 8.9545 - custom_mse: 275.5217\n",
            "reduce_confident_loss =  0.610565066\n",
            "\n",
            "reduce_mean_cls_loss =  0.724695206\n",
            "\n",
            "regression loss =  9.10977745\n",
            "37/41 [==========================>...] - ETA: 1:49 - loss: 8.9948 - custom_mse: 189.5874\n",
            "reduce_confident_loss =  0.704136908\n",
            "\n",
            "reduce_mean_cls_loss =  0.699622571\n",
            "\n",
            "regression loss =  8.16811848\n",
            "38/41 [==========================>...] - ETA: 1:21 - loss: 9.0100 - custom_mse: 69.4760 \n",
            "reduce_confident_loss =  0.617722332\n",
            "\n",
            "reduce_mean_cls_loss =  0.701484561\n",
            "\n",
            "regression loss =  6.68892193\n",
            "39/41 [===========================>..] - ETA: 54s - loss: 8.9843 - custom_mse: 192.3338\n",
            "reduce_confident_loss =  0.604798734\n",
            "\n",
            "reduce_mean_cls_loss =  0.709182322\n",
            "\n",
            "regression loss =  7.47890568\n",
            "40/41 [============================>.] - ETA: 27s - loss: 8.9795 - custom_mse: 123.6949indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 1\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 0\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 2\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 6\n",
            "row = \n",
            " 0\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 5\n",
            "row = \n",
            " 5\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "indices from hungarian algorithm is larger than the object\n",
            "col = \n",
            " 1\n",
            "row = \n",
            " 6\n",
            "y true = \n",
            " [[0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]\n",
            " [0 0 nan ... nan nan nan]]\n",
            "y pred = \n",
            " [[0.0641212463 0.00636255741 0.000232994556 ... 0.315351516 0.0233571827 0.999544859]\n",
            " [0.0024009645 0.00254413486 1.30711778e-05 ... 0.0415830314 0.00564807653 0.998917162]\n",
            " [0.0584560335 0.0408785045 0.00084900856 ... 0.0382760465 0.00315767527 0.910350442]\n",
            " [0.0571287572 0.00916555524 0.00419080257 ... 0.505601823 0.149791986 0.99970448]\n",
            " [0.00463479757 0.0287235975 0.000105853374 ... 0.165802628 0.00615423918 0.999928594]]\n",
            "\n",
            "reduce_confident_loss =  0.617903888\n",
            "\n",
            "reduce_mean_cls_loss =  0.706379414\n",
            "\n",
            "regression loss =  6.98924589\n",
            "41/41 [==============================] - ETA: 0s - loss: 8.9743 - custom_mse: 96.0064  \n",
            "reduce_confident_loss =  0.58594048\n",
            "\n",
            "reduce_mean_cls_loss =  0.736159921\n",
            "\n",
            "regression loss =  12.9722452\n",
            "\n",
            "reduce_confident_loss =  0.585923731\n",
            "\n",
            "reduce_mean_cls_loss =  0.702400267\n",
            "\n",
            "regression loss =  9.74342\n",
            "\n",
            "reduce_confident_loss =  0.726240218\n",
            "\n",
            "reduce_mean_cls_loss =  0.693149209\n",
            "\n",
            "regression loss =  16.4085274\n",
            "\n",
            "reduce_confident_loss =  0.589606702\n",
            "\n",
            "reduce_mean_cls_loss =  0.707809508\n",
            "\n",
            "regression loss =  10.8894348\n",
            "\n",
            "reduce_confident_loss =  0.725021899\n",
            "\n",
            "reduce_mean_cls_loss =  0.693214834\n",
            "\n",
            "regression loss =  15.6857414\n",
            "\n",
            "reduce_confident_loss =  0.726319969\n",
            "\n",
            "reduce_mean_cls_loss =  0.693057954\n",
            "\n",
            "regression loss =  10.6505833\n",
            "\n",
            "reduce_confident_loss =  0.592257559\n",
            "\n",
            "reduce_mean_cls_loss =  0.731878936\n",
            "\n",
            "regression loss =  12.2998562\n",
            "\n",
            "reduce_confident_loss =  0.724184453\n",
            "\n",
            "reduce_mean_cls_loss =  0.693257928\n",
            "\n",
            "regression loss =  14.0350838\n",
            "\n",
            "reduce_confident_loss =  0.724003792\n",
            "\n",
            "reduce_mean_cls_loss =  0.693292737\n",
            "\n",
            "regression loss =  10.0350142\n",
            "\n",
            "reduce_confident_loss =  0.723849\n",
            "\n",
            "reduce_mean_cls_loss =  0.693253934\n",
            "\n",
            "regression loss =  9.81471443\n",
            "\n",
            "reduce_confident_loss =  0.724513054\n",
            "\n",
            "reduce_mean_cls_loss =  0.693164647\n",
            "\n",
            "regression loss =  15.991786\n",
            "\n",
            "reduce_confident_loss =  0.5923751\n",
            "\n",
            "reduce_mean_cls_loss =  0.710580826\n",
            "\n",
            "regression loss =  17.6953087\n",
            "\n",
            "reduce_confident_loss =  0.590027\n",
            "\n",
            "reduce_mean_cls_loss =  0.67968142\n",
            "\n",
            "regression loss =  11.9318466\n",
            "\n",
            "reduce_confident_loss =  0.7255916\n",
            "\n",
            "reduce_mean_cls_loss =  0.693195403\n",
            "\n",
            "regression loss =  12.9222918\n",
            "\n",
            "reduce_confident_loss =  0.723567247\n",
            "\n",
            "reduce_mean_cls_loss =  0.693107665\n",
            "\n",
            "regression loss =  16.391964\n",
            "41/41 [==============================] - 1262s 31s/step - loss: 8.9743 - custom_mse: 96.0064 - val_loss: 14.4802 - val_custom_mse: 168.3330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exBLthxxog61"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAB8_iPpnS5C",
        "outputId": "61f3e02a-4b6a-4565-9be6-9091fb2f3e95"
      },
      "source": [
        "vit_resnet_backbone_model.save(\"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_2\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dense_25_layer_call_fn while saving (showing 5 of 1000). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_2/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_2/assets\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZDTtUwp4D9F",
        "outputId": "11a9aeda-58a7-40dc-bef3-ce609e5d6179"
      },
      "source": [
        "vit_resnet_backbone_model.save(\"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_3.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NOY5Yg-pWtD"
      },
      "source": [
        "# Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5YjSKmEwC49"
      },
      "source": [
        "num_of_image = '006484'\n",
        "round = 'round1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1KOEjj6x1CV"
      },
      "source": [
        "def show_prediction(num_of_image,round):\n",
        "    # RGB image\n",
        "    test_predict_img_front_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/front_image/{round}/{round}_{num_of_image}.jpg'\n",
        "    test_predict_img_left_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/left_image/{round}/{round}_{num_of_image}.jpg'\n",
        "    test_predict_img_right_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/right_image/{round}/{round}_{num_of_image}.jpg'\n",
        "    test_predict_img_back_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/back_image/{round}/{round}_{num_of_image}.jpg'\n",
        "\n",
        "    test_predict_img_front = read_rgb_image(test_predict_img_path)\n",
        "    test_predict_img_left = read_rgb_image(test_predict_img_left_path)\n",
        "    test_predict_img_right = read_rgb_image(test_predict_img_right_path)\n",
        "    test_predict_img_back = read_rgb_image(test_predict_img_back_path)\n",
        "\n",
        "    # Depth image\n",
        "    test_predict_img_front_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/front_image/{round}/{round}_{num_of_image}.png'\n",
        "    test_predict_img_left_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/left_image/{round}/{round}_{num_of_image}.png'\n",
        "    test_predict_img_righ_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/right_image/{round}/{round}_{num_of_image}.png'\n",
        "    test_predict_img_back_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/back_image/{round}/{round}_{num_of_image}.png'\n",
        "\n",
        "    test_predict_img_depth_front = read_depth_image(test_predict_img_front_depth_path)\n",
        "    test_predict_img_depth_left = read_depth_image(test_predict_img_left_depth_path)\n",
        "    test_predict_img_depth_right = read_depth_image(test_predict_img_righ_depth_path)\n",
        "    test_predict_img_depth_back = read_depth_image(test_predict_img_back_depth_path)\n",
        "\n",
        "    # Concatenate rgb and depth\n",
        "    concatennate_rgb_depth_front = tf.concat([test_predict_img_front,test_predict_img_depth_front],axis=-1)\n",
        "    concatennate_rgb_depth_right = tf.concat([test_predict_img_right,test_predict_img_depth_left],axis=-1)\n",
        "    concatennate_rgb_depth_left = tf.concat([test_predict_img_left,test_predict_img_depth_left],axis=-1)\n",
        "    concatennate_rgb_depth_back = tf.concat([test_predict_img_back,test_predict_img_depth_back],axis=-1)\n",
        "\n",
        "    normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "    normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "    normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "    normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)\n",
        "\n",
        "\n",
        "    normalized_front_expand = tf.expand_dims(normalized_front,axis=0)\n",
        "    normalized_left_expand = tf.expand_dims(normalized_left,axis=0)\n",
        "    normalized_right_expand = tf.expand_dims(normalized_right,axis=0)\n",
        "    normalized_back_expand = tf.expand_dims(normalized_back,axis=0)\n",
        "\n",
        "    concat_all_sides = tf.concat([normalized_front_expand,normalized_left_expand,normalized_right_expand,normalized_back_expand],axis=0)    \n",
        "    concat_all_sides = tf.expand_dims(concat_all_sides,axis = 0)\n",
        "\n",
        "    # Run prediction\n",
        "    prediction = vit_resnet_backbone_model.predict(concat_all_sides)\n",
        "    plt.imshow(cv2.imread(test_predict_img_front_path))\n",
        "    tf.print(prediction.reshape((7,4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "qd0ao5Cmyj9k",
        "outputId": "d7c21ed4-a728-4731-b121-e83959b3a4b8"
      },
      "source": [
        "show_prediction(num_of_image,round)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "array([[-18.339245  ,  -7.079572  , -17.967066  ,  14.553135  ],\n",
            "       [ 17.6661    ,  -9.033151  ,  -4.478234  ,   1.3453833 ],\n",
            "       [ -7.056542  ,  15.691201  , -14.380486  ,  -1.5405031 ],\n",
            "       [ 21.184322  ,  14.703356  ,   1.0204788 ,  -4.4348283 ],\n",
            "       [ 23.613546  ,   5.1092587 ,  -5.324966  ,  -4.4120116 ],\n",
            "       [ 26.839258  , -11.163725  ,  -1.2937788 ,  -2.724493  ],\n",
            "       [-22.615032  ,  -0.12604015,  17.059353  ,  -9.124053  ]],\n",
            "      dtype=float32)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7pUTcAnqk7M"
      },
      "source": [
        "RGB images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSiH4ONVoDfG"
      },
      "source": [
        "test_predict_img_front_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/front_image/round3/round3_{num_of_image}.jpg'\n",
        "test_predict_img_left_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/left_image/round3/round3_{num_of_image}.jpg'\n",
        "test_predict_img_right_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/right_image/round3/round3_{num_of_image}.jpg'\n",
        "test_predict_img_back_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/rgb_image/back_image/round3/round3_{num_of_image}.jpg'\n",
        "\n",
        "test_predict_img_front = read_rgb_image(test_predict_img_path)\n",
        "test_predict_img_left = read_rgb_image(test_predict_img_left_path)\n",
        "test_predict_img_right = read_rgb_image(test_predict_img_right_path)\n",
        "test_predict_img_back = read_rgb_image(test_predict_img_back_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhbhtRtmqnBJ"
      },
      "source": [
        "Depth images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JVjOyq3qo2V"
      },
      "source": [
        "test_predict_img_front_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/front_image/round3/round3_{num_of_image}.png'\n",
        "test_predict_img_left_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/left_image/round3/round3_{num_of_image}.png'\n",
        "test_predict_img_righ_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/right_image/round3/round3_{num_of_image}.png'\n",
        "test_predict_img_back_depth_path = f'/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/carla_data_test/Town01/depth_image/back_image/round3/round3_{num_of_image}.png'\n",
        "\n",
        "test_predict_img_depth_front = read_depth_image(test_predict_img_front_depth_path)\n",
        "test_predict_img_depth_left = read_depth_image(test_predict_img_left_depth_path)\n",
        "test_predict_img_depth_right = read_depth_image(test_predict_img_righ_depth_path)\n",
        "test_predict_img_depth_back = read_depth_image(test_predict_img_back_depth_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6jVI6OzrlLb"
      },
      "source": [
        "Concatenate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtICm_N1rmli"
      },
      "source": [
        "# Concatenate rgb and depth\n",
        "concatennate_rgb_depth_front = tf.concat([test_predict_img_front,test_predict_img_depth_front],axis=-1)\n",
        "concatennate_rgb_depth_right = tf.concat([test_predict_img_right,test_predict_img_depth_left],axis=-1)\n",
        "concatennate_rgb_depth_left = tf.concat([test_predict_img_left,test_predict_img_depth_left],axis=-1)\n",
        "concatennate_rgb_depth_back = tf.concat([test_predict_img_back,test_predict_img_depth_back],axis=-1)\n",
        "\n",
        "normalized_front = normalize_on_rgb_depth(concatennate_rgb_depth_front)\n",
        "normalized_left = normalize_on_rgb_depth(concatennate_rgb_depth_left)\n",
        "normalized_right = normalize_on_rgb_depth(concatennate_rgb_depth_right)\n",
        "normalized_back = normalize_on_rgb_depth(concatennate_rgb_depth_back)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vin0zGbesRPd"
      },
      "source": [
        "normalized_front_expand = tf.expand_dims(normalized_front,axis=0)\n",
        "normalized_left_expand = tf.expand_dims(normalized_left,axis=0)\n",
        "normalized_right_expand = tf.expand_dims(normalized_right,axis=0)\n",
        "normalized_back_expand = tf.expand_dims(normalized_back,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnfT_nrbuiC9",
        "outputId": "e1060553-226d-4dbf-df9e-fde8b74a78f2"
      },
      "source": [
        "normalized_back_expand.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 200, 200, 4])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAuHuNhetx8e"
      },
      "source": [
        "concat_all_sides = tf.concat([normalized_front_expand,normalized_left_expand,normalized_right_expand,normalized_back_expand],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc6Y-kx1usVE"
      },
      "source": [
        "concat_all_sides = tf.expand_dims(concat_all_sides,axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ang8tTfNuPVM",
        "outputId": "586c32fa-dc89-425e-c6be-d6dbf696e3a5"
      },
      "source": [
        "concat_all_sides.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 4, 200, 200, 4])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cC_5YBdYTVs",
        "outputId": "c34790cd-4e9b-4fc4-d329-47686da0604e"
      },
      "source": [
        "vit_resnet_backbone_model.predict(concat_all_sides)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.26018267e+01, -6.47885036e+00, -1.45323305e+01,\n",
              "         1.15185947e+01,  1.71507587e+01, -5.59835243e+00,\n",
              "        -4.12346077e+00,  2.24878624e-01, -7.08493710e+00,\n",
              "         1.01386576e+01, -1.02705517e+01, -3.74427706e-01,\n",
              "         1.68265953e+01,  1.49047499e+01,  2.31250000e+00,\n",
              "        -1.77779830e+00,  1.73440380e+01,  4.46030903e+00,\n",
              "        -3.55648065e+00, -1.58945751e+00,  2.42900105e+01,\n",
              "        -9.68129921e+00,  2.07253844e-02, -2.28077722e+00,\n",
              "        -1.57672348e+01, -2.07410240e+00,  1.28771639e+01,\n",
              "        -7.75781918e+00]], dtype=float32)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBt7nEJL2ASp"
      },
      "source": [
        "# load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebKlm2BsI79"
      },
      "source": [
        "path_to_model = \"/content/drive/Shareddrives/Behavior vehicles prediction_BME/carla_dataset/save_model/vit_resnet_hungrian_bce_loss_2\"\n",
        "custom_objects = {'Patches':Patches,'PatchEncoder': PatchEncoder, # Still have a problem here\n",
        "                  'Hungarian_loss': hungarian_loss,\n",
        "                  'custom_MSE': custom_MSE,\n",
        "                  'hungarian_loss_fit':hungarian_loss_fit }\n",
        "reconstructed_model = tf.keras.models.load_model(path_to_model,custom_objects=custom_objects)                                                                                                                                                     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ytdeGoS2GJT",
        "outputId": "0ee3a501-937a-4ba5-fdbe-d7fc5904578a"
      },
      "source": [
        "reconstructed_model.predict(concat_all_sides)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.26018267e+01, -6.47885036e+00, -1.45323305e+01,\n",
              "         1.15185947e+01,  1.71507587e+01, -5.59835243e+00,\n",
              "        -4.12346077e+00,  2.24878624e-01, -7.08493710e+00,\n",
              "         1.01386576e+01, -1.02705517e+01, -3.74427706e-01,\n",
              "         1.68265953e+01,  1.49047499e+01,  2.31250000e+00,\n",
              "        -1.77779830e+00,  1.73440380e+01,  4.46030903e+00,\n",
              "        -3.55648065e+00, -1.58945751e+00,  2.42900105e+01,\n",
              "        -9.68129921e+00,  2.07253844e-02, -2.28077722e+00,\n",
              "        -1.57672348e+01, -2.07410240e+00,  1.28771639e+01,\n",
              "        -7.75781918e+00]], dtype=float32)"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3hj1_ubGMva"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}