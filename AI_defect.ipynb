{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_defect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailuu684/Convolution-net/blob/main/AI_defect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEbK_kk9fs-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262d21d5-114d-41d1-8ce3-0728b88e8160"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISomse_RKodS"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Import thu vien segmentation_models\n",
        "!pip install segmentation_models \n",
        "from segmentation_models.metrics import iou_score \n",
        "!pip install segmentation_models \n",
        "from segmentation_models import Unet\n",
        "!pip install segmentation_models \n",
        "import segmentation_models as sm\n",
        "sm.set_framework(\"tf.keras\")\n",
        "sm.framework()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZSiPXknmBmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b874f24a-e1c9-4a16-be4f-99c96ce38f67"
      },
      "source": [
        "\n",
        "# Dinh nghia bien\n",
        "data_path  = \"/content/gdrive/MyDrive/Img_train_AI_defect/dataset\"\n",
        "w, h = 512, 512\n",
        "batch_size = 16\n",
        "\n",
        "# Dataset va Dataloader\n",
        "\n",
        "BACKBONE = \"resnet34\"\n",
        "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
        "\n",
        "# Dung de tao toan bo du lieu va load theo batch\n",
        "class Dataset:\n",
        "    def __init__(self, image_path, mask_path, w, h):\n",
        "        # the paths of images\n",
        "        self.image_path = image_path\n",
        "        # the paths of segmentation images\n",
        "        self.mask_path = mask_path\n",
        "\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # read data\n",
        "        image = cv2.imread(self.image_path[i])\n",
        "        image = cv2.resize(image, (self.w, self.h), interpolation=cv2.INTER_AREA)\n",
        "        image = preprocess_input(image)\n",
        "\n",
        "        mask = cv2.imread(self.mask_path[i], cv2.IMREAD_UNCHANGED)\n",
        "        image_mask = cv2.resize(mask, (self.w, self.h), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        image_mask = [(image_mask == v) for v in [1]]\n",
        "        image_mask = np.stack(image_mask, axis=-1).astype('float')\n",
        "\n",
        "        return image, image_mask\n",
        "\n",
        "class Dataloader(tf.keras.utils.Sequence):\n",
        "    def __init__(self, dataset, batch_size,shape, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.shape = shape\n",
        "        self.indexes = np.arange(self.shape)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
        "\n",
        "        return tuple(batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)\n",
        "\n",
        "#  Load thong tin tu folder dataset de tao 2 bien image_path, mask_path\n",
        "def load_path(data_path):\n",
        "    # Get normal image and mask\n",
        "    classes = ['Class1', 'Class2', 'Class3', 'Class4', 'Class5', 'Class6']\n",
        "\n",
        "    # Lop qua cac thu muc khong loi\n",
        "    normal_image_path = []\n",
        "    normal_mask_path = []\n",
        "    for class_ in classes:\n",
        "        current_folder = os.path.join(data_path, class_)\n",
        "        for file in os.listdir(current_folder):\n",
        "            if file.endswith(\"png\") and (not file.startswith(\".\")):\n",
        "                image_path = os.path.join(current_folder, file)\n",
        "                mask_path = os.path.join(current_folder + \"_mask\", file)\n",
        "                normal_mask_path.append(mask_path)\n",
        "                normal_image_path.append(image_path)\n",
        "\n",
        "    # Get defect image and mask\n",
        "    defect_image_path = []\n",
        "    defect_mask_path = []\n",
        "    for class_ in classes:\n",
        "        class_ = class_ + \"_def\"\n",
        "        current_folder = os.path.join(data_path, class_)\n",
        "        for file in os.listdir(current_folder):\n",
        "            if file.endswith(\"png\") and (not file.startswith(\".\")):\n",
        "                image_path = os.path.join(current_folder, file)\n",
        "                mask_path = os.path.join(current_folder + \"_mask\", file)\n",
        "                defect_mask_path.append(mask_path)\n",
        "                defect_image_path.append(image_path)\n",
        "\n",
        "    # Normal:   normal_mask_path - chua toan bo duong dan cua mask\n",
        "    #            normal_image_path-  chua toan bo duong dan den image\n",
        "    # Defect:   defect_mask_path - chua toan bo duong dan cua mask\n",
        "    #            defect_image_path-  chua toan bo duong dan den image\n",
        "\n",
        "    idx = random.sample(range(len(normal_mask_path)), len(defect_mask_path))\n",
        "\n",
        "    normal_mask_path_new = []\n",
        "    normal_image_path_new = []\n",
        "\n",
        "    for id in idx:\n",
        "        normal_image_path_new.append(normal_image_path[id])\n",
        "        normal_mask_path_new.append(normal_mask_path[id])\n",
        "\n",
        "    image_path = normal_image_path_new + defect_image_path\n",
        "    mask_path = normal_mask_path_new + defect_mask_path\n",
        "\n",
        "    return image_path, mask_path\n",
        "\n",
        "# Thu hien load va train model\n",
        "\n",
        "# Load duong dan vao 2 bien\n",
        "image_path, mask_path = load_path(data_path)\n",
        "\n",
        "# Chia du lieu train, test\n",
        "image_train, image_test, mask_train, mask_test = train_test_split(image_path, mask_path, test_size=0.2)\n",
        "\n",
        "# Tao dataset va dataloader\n",
        "train_dataset = Dataset(image_train, mask_train, w, h)\n",
        "test_dataset = Dataset(image_test, mask_test, w, h)\n",
        "\n",
        "train_loader = Dataloader(train_dataset, batch_size, shape=len(image_train), shuffle=True)\n",
        "test_loader = Dataloader(test_dataset, batch_size, shape=len(image_test), shuffle=True)\n",
        "\n",
        "# Khoi tao model\n",
        "opt = tf.keras.optimizers.Adam(0.001)\n",
        "model = Unet(BACKBONE, encoder_weights=\"imagenet\", classes=1, activation=\"sigmoid\", input_shape=(512, 512, 3), encoder_freeze = True)\n",
        "loss1 = sm.losses.categorical_focal_dice_loss\n",
        "model.compile(optimizer=opt, loss=loss1, metrics=[iou_score])\n",
        "\n",
        "# Train model\n",
        "#is_train = False\n",
        "#if is_train:\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"/content/gdrive/MyDrive/Img_train_AI_defect/checkpoint.hdf5\"\n",
        "callback = ModelCheckpoint(filepath, monitor='val_iou_score', verbose=1, save_best_only=True,mode='max')\n",
        "\n",
        "model.fit_generator(train_loader, validation_data=test_loader, epochs=50, callbacks=[callback])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90/90 [==============================] - 150s 2s/step - loss: 0.0600 - iou_score: 0.8907 - val_loss: 0.3417 - val_iou_score: 0.5324\n",
            "\n",
            "Epoch 00046: val_iou_score did not improve from 0.54560\n",
            "Epoch 47/50\n",
            "90/90 [==============================] - 150s 2s/step - loss: 0.0573 - iou_score: 0.8956 - val_loss: 0.3349 - val_iou_score: 0.5388\n",
            "\n",
            "Epoch 00047: val_iou_score did not improve from 0.54560\n",
            "Epoch 48/50\n",
            "90/90 [==============================] - 150s 2s/step - loss: 0.0567 - iou_score: 0.8966 - val_loss: 0.3529 - val_iou_score: 0.5197\n",
            "\n",
            "Epoch 00048: val_iou_score did not improve from 0.54560\n",
            "Epoch 49/50\n",
            "90/90 [==============================] - 150s 2s/step - loss: 0.0571 - iou_score: 0.8958 - val_loss: 0.3591 - val_iou_score: 0.5113\n",
            "\n",
            "Epoch 00049: val_iou_score did not improve from 0.54560\n",
            "Epoch 50/50\n",
            "90/90 [==============================] - 150s 2s/step - loss: 0.0585 - iou_score: 0.8934 - val_loss: 0.3329 - val_iou_score: 0.5399\n",
            "\n",
            "Epoch 00050: val_iou_score did not improve from 0.54560\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ee9a07ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}